Start testing on iteration 499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.493s + 0.001s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.381s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.372s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.370s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.368s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.370s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.370s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.369s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.369s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.372s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.609s + 0.001s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.428s + 0.003s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.409s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.407s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.402s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.398s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.402s + 0.002s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.402s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.400s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.401s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.400s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.401s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.397s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.546s + 0.002s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.396s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.395s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.399s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.393s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.388s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.389s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.389s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.390s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.389s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.389s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.389s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.387s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.522s + 0.001s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.392s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.381s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.385s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.384s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.381s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.380s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.382s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.381s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.378s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.379s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.378s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 60.463s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [47 10 36  9 14 13 28 18 23 11 58 39 61 48 66 67 25 64  3 40 12 24 19 32
 44 63 53 55 35 59 51 62 21 27  5  6 15 49 31 22 26  8 20 29 57 37 46  1
 54 43 56 52 41  2 30 68 33 17  7 60  0 16 42 50 65 45 38 34  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3593
INFO voc_eval.py: 171: [ 3 26 12 22  6 19 10 27 24  8 30  1 17  2 28 15 11 23 25 29 16  0  9  4
 21 18  5 13 20  7 14]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6143
INFO voc_eval.py: 171: [164  95 240 293 124  69  75 275 174 183  39  59  77 277 232  56 247 132
 154 146  58 350 234 221 353 137 138 352  99 358 131 245 158 113 114 343
 207 190 320 117 112  74   8 231  88  80  24 319   3 172 287 136 186 185
  79  84 289 253 206  76   4 111 324 153  50 344 194 284 171 334 237 351
 338  86  67 208 115 165  68   2  57 119 102 191 187 201  25 147  29  19
 258 305 130 150 333 322 175 244 288 196 312  51  78 259  81 345 152 281
 116 163 203 249  21 286 166  46 235 264 179 337 252  37 198  73 169 107
 335 233  36 297  35 269 270  31 354 120 101 314 268 204 193  20 168 159
  72 300 331  83 224 192 346 274  93  43 267 197 105  16 218 230  64   0
 278  10  30 103 173 170 290 149 260 212   1 339 109 226  27 241 143  26
 127 257 304 126  28 325  42 317 246 271 213  49   7 161 188  44  34 321
 202 238 251 135 248 121 118 292 301 313  22  71 160 242 162 205 327 222
 348  15 326 342 347 329 316 182  89 250 176   6  45 310  54 125 214 285
 181  53 200 279 243 296 323 336 180 273 254 303 134 359  85 142  47  98
 110  66 178 145 255 100  55 299 291 148 266  48 361  40 123 330  38  62
 294 211 302 282  96 227 141 228 217 315   5 209 265 309  17  12 177  90
 220 261 151 280 223  14 306 210 139 133 340  41 362 184   9 229 128 262
 167  13 157  87 239 328 219  82 189 360 318 311  97 276 104 122 195  65
 298 106 225  63 156 256 216 215  18  23 363  92 108 272 129  61  52 332
 236  94  11  91 349 263 155 308 295  33  60 357 341 140 199  70 144  32
 283 355 307 356]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4304
INFO voc_eval.py: 171: [103  86  87  85  77   7  73   1  72   0  88 101  60  50  70  68  95  97
  25  91 123  30  23  81  40  71  22  89  29  37 121  33 106  14 139  51
  52  27 133 140 127  75  17 108  64  34  98  39   6 102 105  79  56  82
  49  15 120  31   5  74  45  26 136   2   8 137  58 100 118  69  42  10
 119  55 135  99  65 109  18 124  32  47 128 130  53  61  78 107  44 134
  21  38  93  67  36   9 126  41  80  63 110 112  54  16  66  62 114   4
  90  19 115 131 104  94  13 117  48 122  46  59 138  24  12 113 132 116
 125   3  96  43 111  20  35  92  83 129  76  11  57  28  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4873
INFO voc_eval.py: 171: [ 89  42  90   1  69  41  73  93  46  92  40  79   3  78  52  85  91  43
   4 122  44  38  64 109  39  77  81  65  48  16  11  47  72  80  98 123
  27  76  95  36  94  56  17  18  24 102 100   2  87 105  30  54 108  55
 106   6 110  63  35 112  15  83 120  10   7  49  26  74  21  34   9  88
  32  82  53  29 101 103  97  28  96  68  75  86 119   0  25 116  57  31
  84  61  71   5  66  20  70  37 117  12  60  19  23 107 113  67 111  58
  99  62  45 114 115   8  13  33  50 104  59  14  51  22 118 121]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4681
INFO voc_eval.py: 171: [12 11 29  5  3  4  2 20 21 13  1 23 24  6 14 15 10  8 28  0 26 16 27 19
 17 25 18  9  7 22]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.7591
INFO voc_eval.py: 171: [123 187  51  21 171 170   7  58 106  35 122 115 113  49  65  89   0 159
 174 178 125  25 155  80 169 172  95   5   9  56  85  57  34  52 154  99
  68  10 107  74  20  81 136 142  19 109  78 153 164  71 161 186  53  76
 127 139  29 160 146 130  66  87 147  72  43  23   2   4 118  22 183 167
 177  88 131 111 184  90  28 137 129 100  31  77 110  82 116 126 162  62
  30   6  91  67  47  45 120  55 163  41 104 168 176 190  79 119  64 145
  63  83  69  42  32   8 144 151 103  60  37 124 158 140  54 185 101  27
 132 135 188 134 180 189  12  73  40  16  84 182  18 128  44 105 152  39
  92  59  36  11 143 149  15 165  38   3 133  13 112  98 156  75 114  50
 102  24 148  86 108  17 175 138  46  26 173 157  48  93  96  70   1  94
 121 181  14 117 150  33 179 141  61  97 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6386
INFO voc_eval.py: 171: [61  5 31 65 55 44 33 49 50 28 17 68 71 54 24  2 51 66 25 16 27 69 23 34
 35  4 39 56 37 26  9 29 57 11 15  3 46 62 41 42 43 13 10 20 19 47 53 45
 30 60 36 32 64 38 58  8 63 18 14  1 12 22  0 52  7 59 40 48 70  6 21 67]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.2354
INFO voc_eval.py: 171: [ 28  62 106  57  89 113 107 303 227 257 287  45 300  77 249  58  55 167
 288  17 313 149 246 332 196 229  27  37 104 184  18 198 166 150 179 114
  14 293  59 169 330 256 323 121  47 183  87 177 262 244 173   8  34  61
 239 108 236 237 250 292   9  66 294 261 181 280 217  91  92 190  43  10
 278 272  63 216 120  38   6 209 119  69 204 248  88 222  11 314 195 154
 136 220 218  29  79 315   2  72 122 202 144  32 191  36 163 127 241 270
  93 316 115  96 301 135 212 235 277 146 312 268 327 266 110 299  48 233
  51 230 252 264  83 105 254 333 224 221 193  21 161 273 197 226  31 116
   7  97 238 282  53 159  35  42 138 285  26 205 142 130 251  16 157 171
 170 131  94  73  68   5 125 321 284  80 203 147 207 103 187  19  64 255
  98  81 168  30 325  56 311 228 194 329  71 269 176 298 331  54 156  65
 117 320 153 289 111 174 210 260 132 253 137 305 291 140 102 258  12 223
  15 162 141 134 263  85  20 200 247 286 189 155  13 100 129 201 211 318
 133  41 160 123 302  95  52 164 308   1 328  75  40 172 145 158 231 151
 219 297   3 118 225 175 215 240 185 309 128 182  84 143 265  78 267 319
 259  44 290 276 148 109 152  60  76  99 279 112 324 165 232  74 206  67
 192 334  82 234 310 178 188  23 245 271 139  22  24 322 208 307 296 243
 304  39  49  33 283  46 180 126  50 274 186  86 306 242 295 213   4 101
  70  90  25 275 214 326 317 281 199 124   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5835
INFO voc_eval.py: 171: [ 7 44 33 20 22 17 16 21  3 18 79 24 42 47  4  2  1 41 64 92 78 95 15 23
 50 49 38 84 35 61 62 69 46 32 29 60 27 13 72 77 63 81 28 59 40 31 43 70
 10 30 71 48 87 90 80 66 89 82 67 83 68 58 37 26 73 45 14 52 74  0 19 65
 93 54 75 94  9 25  5 11 39  6 57 76 88 55 12 53 86  8 91 36 34 56 85 51]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.7858
INFO voc_eval.py: 171: [ 87 179 181 177 212  50  19  55 128 196 124  12   8 188  98 148  96 205
  92 154 193 165 169 173  51 200 167 117  86  76 132  85 147 176 140  21
  61 162  58  69 121  89  41  16 145  74  66  73 155 186 184  99 101 115
   2 161  18 110 170 153 207  67 134  31  14 133  39 183  91  26  24  90
  65 151 210 100  38 129  46  36 118  10 122  63 208 102  28 138 215  56
  33 144  44  13  82 211 111  54 168  57  45  70 201 120  81  60 152 198
  88 195  40  43  75  15 209  68  95 194 107  34  62 105 185 191 104  94
 180   3 125  93  20 199 136  11 150 202 126 109 139 217  42  79  17 203
  97 112 189  77  59 135 106 146  78 206 187  83  25 160 113  23 182 130
   6 131  22  27 172  84 157 190 114 127 137 156 142 175  32 119 108   1
  48 123  52 149 159   5 213 204  47 178 197 116   4  35 141 192 216  80
  72   0 214 174  71  64 103  53  37   7 143 166  29 164 171   9 163  49
 158  30]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5634
INFO voc_eval.py: 171: [106  94  23  79 126 142  66  21  41  60 101  83  34   8  73  24  12  47
 118   4 109  46  85 108  69  20  48  72 135  57 116  77   9 110 129 104
 128  93  62   3  39  76  54 137  88 115 123  22  64 132  80 112  25  17
  42  28   7  84 130 139  32 120 124 122 107 102  86  15  82 134  33 136
  92   5 133 121 119  55  70  18   0  26  89  45 111  19  51 131  43   6
  31 140 127  56  38  61  63  99  27  81  87  96  16  67  98 143  10  78
 100  68   1  71  37  30 117 103 105 113  58  49  11  50  13  14   2  29
  53  52  36  75  59  40  95 144 125  35  65  91  90  74 141 138  44 114
  97]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6284
INFO voc_eval.py: 171: [ 21 121  60 103  22 105  37 135  99  89   6 137 127  63  76  81 113 128
  64 138  88  23  98   2 110  55 129 132  66  47  42 134 119  45   4  58
 102  84 126  56  73  87  93  35 120  97  80 141  72  92  59  41  11  62
 124 136  30  48 104   0  71 139  38  51  77 142  25 122  29  85  65   5
 114  24  10  67  36  70  33 115  54  18  26  68  90 145  52  17  82 148
 112  19  46  44   1  40 108 116 101  86  34  28   9  43 131  27  94 123
  31 118  49 100  57  39  69 130 144  79  91   8 125  95  20  83  15   3
  74  16 143 107 133 111  50  14  61  75 106 117  32  13 146  96  53 140
  12   7 149 109  78 147]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7291
INFO voc_eval.py: 171: [11 28  3 14  2 19  7 15 20  5 23 30  6  9 17 21 10 25 12 24 18 29  1  0
 22 13  8 16 26  4 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8060
INFO voc_eval.py: 171: [1107  789  788 ...  951  902  382]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7620
INFO voc_eval.py: 171: [ 15 131  43  17  11  91  47  19  92 144   2   1   8 109 137 117  26  41
  23  59  97 138  89  76 130  54  31  28  72  84 101  24  51 105  14 139
  21  36 129  35  52  46 140  18  12 110  80 116  33 100 124  58   9  16
  13  10  85  77 119  61  50 125 115  32  86 132  68  87 121 114  71 135
 120 108 134 136  83 126  78 102  27  90 112  56  94  34 123   7 146 113
  73  81  22  20  82  88 107   0 128   3  79 111 141  44 122  38  99  63
   5  42  95  93 145  69 118 106  64 142  39  66  53 133 104  37  96  70
  30  74 127  62   4  55   6 103 143  57  67  98  75  45  49  60  40  29
  65  48  25]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.6204
INFO voc_eval.py: 171: [53 58 60 63 73 52  3 87 91  4 25 64 31 90 55 89 13 17 65 14 66 74 26 29
 22 23 45 70 41 34 32 57 76 54 99  8 69 42 37 83 15 35 56 98 48 10 50 75
 97 77 24 92 51 62 39 86 85 79 40 82 68 96 61  9 43  0 16 21 72 93  5 84
 19 27 78 59 47 11 71 38 94 18 12 30 44 80 88 33 49  6 81 20  2 28 36 46
 95 67  1  7]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5796
INFO voc_eval.py: 171: [ 16  37  63   0 108  38 103  91   5  44  60   2  39  57  82  27  50 110
  51  31  90  84  69  10  52  26  18  19 104  21  55  72  93  97  34  25
  95  81   1  17  87  36  49   9  78 100 106  58   7  45   4  13  54  70
  15  98  71 105  74  22  30  77  62  20  94 107   6  24 102  68  29  12
  46  47   8 109  42  32  56  79  92  83  61 101   3  75  43  64  11  28
  88  59  65  35  48  85  99  96  40  66  73  67  23  86  53  89  80  76
  33  14  41]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.4315
INFO voc_eval.py: 171: [22 12 29 31 13 11  4  3  1 17 10 16  2 25 19 14  8 15 30  5 24 21  7  9
 18 23 28  0 26 27 32 20  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5570
INFO voc_eval.py: 171: [ 1 18  3 34  9 14 27 24  8 22 29 20  4 16 26 21 12 35  6 15  2 17 13 11
 25 30 33 23 10  5  0 28 32 31 19  7]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5276
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5784
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.359
INFO cross_voc_dataset_evaluator.py: 134: 0.614
INFO cross_voc_dataset_evaluator.py: 134: 0.430
INFO cross_voc_dataset_evaluator.py: 134: 0.487
INFO cross_voc_dataset_evaluator.py: 134: 0.468
INFO cross_voc_dataset_evaluator.py: 134: 0.759
INFO cross_voc_dataset_evaluator.py: 134: 0.639
INFO cross_voc_dataset_evaluator.py: 134: 0.235
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 134: 0.786
INFO cross_voc_dataset_evaluator.py: 134: 0.563
INFO cross_voc_dataset_evaluator.py: 134: 0.628
INFO cross_voc_dataset_evaluator.py: 134: 0.729
INFO cross_voc_dataset_evaluator.py: 134: 0.806
INFO cross_voc_dataset_evaluator.py: 134: 0.762
INFO cross_voc_dataset_evaluator.py: 134: 0.620
INFO cross_voc_dataset_evaluator.py: 134: 0.580
INFO cross_voc_dataset_evaluator.py: 134: 0.432
INFO cross_voc_dataset_evaluator.py: 134: 0.557
INFO cross_voc_dataset_evaluator.py: 134: 0.528
INFO cross_voc_dataset_evaluator.py: 135: 0.578
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.518s + 0.001s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.369s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.371s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.383s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.386s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.380s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.379s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.381s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.378s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.378s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.377s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.379s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.376s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.550s + 0.001s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.379s + 0.003s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.377s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.375s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.379s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.380s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.387s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.386s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.383s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.386s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.388s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.386s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.380s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.522s + 0.001s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.416s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.387s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.378s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.372s + 0.002s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.376s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.371s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.372s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.372s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.370s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.369s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.372s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.373s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.506s + 0.001s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.357s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.355s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.344s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.341s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.339s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.340s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.340s + 0.001s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.343s + 0.001s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.345s + 0.001s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.345s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.344s + 0.001s (eta: 0:00:04)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.344s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.653s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [17 28  6 46  7 26  8  9 59 57 11 47 60 56 24 36 40 25 41 62 58 32 50 13
 33 22  4 29 23 45 20 37 19 34 12 51 27 39 53 30 18 54 55 31  3 35  1  2
 61 10 48 38  5 65 64 43 42 15 52 44 14 49 63 16  0 21]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3003
INFO voc_eval.py: 171: [ 3 15 18 12  9 16  5 21 20 11  7  0 22 14  6  1 17  2 13  8 10 19  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.7456
INFO voc_eval.py: 171: [ 46  43  29  45 199  52  56  57 257  62 222 193 220  89 202  88  74 153
 124 130 122 291 181   4 236 203  59 104 108 107  82 109 111 115  60 228
  78 170   8 120 149 139 255 211 271  15  98  65 195  66 169 227 106  90
  48  94 290   6 254  63 288  38 136  91 289 140 201 161 282 171 292 276
   3 224 273 100 133  87 118  19 110 134 229 129 137 214 165  55  93 217
  51 244 213  24 156 197  92 281  26 284 191  64  33 231 278 232 206 268
 215  35  12 266 142 256  44  50 219 208  97 186 272 218 112  22 259  11
  58 155 243 123  36 143 117  96  84 166 174  17 152 146 150 264 119 184
 252  71 178  99  86  10 274 131 275 116 251 235 196 293  77  67 249  73
 102   0  53  28 242 151 287 105 144 260 245  49 241  21 183 101 163 185
   9 216 154  16 126  42 230  13 237  41 246 167 103 179 207  14 180 269
  27 205 121 190 267 261  32 225 248 138  31 187  81 162 160  76 145  61
 247 234 114  34   7  40  79  80 238 147 148   1 175 226 188  30 159 176
 210 253 283 233  83  70 277  95 198 212  18 192 125  23 223  68 263 250
 128 204  69 280 279  37  54 189 239  85 135  47 285 182 209 194 164   5
  75 270  20 157 172  39  72 240 258 173 221 158 141 286 200 127 168 177
 265 132 113 262   2  25]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4654
INFO voc_eval.py: 171: [ 64   2  66 121 102  67  71  10  81  80  79  93  86  82 137  76  43   1
  54  99   6  23  26  83  96 125  13  32  97  65  69 119  20  98  49  31
 118  28  16  25  14  70  21 105  44  33  47 131  73 136 106  22 111  48
 113 100  60   7  37  68  42 108 133  72  17  11 130 112  24  90  35 132
  36  61 114 117  15   3  41 127  77 122  34   4  56  74  57 109  12  91
  40  27  88  55   8  85 116  18 120 115  51  50 104  84 126  59  92 103
  38  46 107  63  87  95 110 123  58  39 124  89 101  53  62 129  19   9
  45  30   5  78  29  75   0  52 128  94 134 135]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.5118
INFO voc_eval.py: 171: [65 60 26  0 62 54 46 64 44 33 30 24 13 82 53 43 45 49 63 23 28 66 25 81
  1 68 31 50 29 14 32 70 41 27 52  4 34 39 22 51  3  9 36 57 15  5 10 11
 76  6 61 56 67 47 71 72 73 69 12 35 20 55 37 17 59 74  7 18  2 48 40 42
 79 75 78 80 77  8 19 16 21 58 38]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4958
INFO voc_eval.py: 171: [16 44 15  9 30 23  8 33 19  1  0 17  4 27 24 18 25 46 10  2 29 13 14  3
 11 37 31 22 38 28 41 39 26 20 36 35  5  6 42 40 45 12 34 32  7 43 21]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8948
INFO voc_eval.py: 171: [ 80 165 159 118 102  52  53  87 149 173  58 109 161 157 160 112  40  26
 119  23  91  17 167  51  55 147 120  76 128  81 153  79 145   0 135   7
 166  18  63 146 107  20  10 108  54   5 126  41  59  64  82 150 122   2
 127 139  46 131 115 103  34  97  89  93  96 123 171  92 113 168  73 152
  57 175  78  86 106  43 164 136   4 138  36  94 151 172  24 105  19 121
 125  37  66  95  69  68 140  62 158  33  84 176  71 114 100 163  50 148
  72   6  29   8 143 129  65  25  45  77 156 101   1  48 130  74  98 133
 116  99  67 170  90  88  15 110   9  12 134  30   3  35  16 117 124  39
 104  83  70 154 142 141 132 137  75  49  11  44  28  14  13 155  21  22
 144  60  42 177  32  47  56 174  27  31  85 169  38 162 111  61]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6821
INFO voc_eval.py: 171: [ 3 54 28 46 57 49 23 60 27 25  2 39 14 62 59 45 41 44 12 37 19 58 50 34
 48  0 11 26 42 36 13 30 22 20 33 18 10 15  6 24  8 55  1 29  5 32 47 16
 38 53  9  7 31 43 21 52 61 56 17 35 51 40  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1732
INFO voc_eval.py: 171: [129 206 124  44  97 122 229 114  59 111 193 196  32  42 216  40 207  37
  98  17  69 166  71  53  72 174 154 130   8  10  75  76 141 169  16 175
  82  41 170 113 198 167  11 226 189  57  24 197 188  33 158   3 123 184
 100  73 159 143   0  30 112   4  46 132 160  26 155  61 119 225  43 142
   5 214  22 121  60   6 107 215  48 183  45  36  70  38  18 145  80 224
 116  93 137 136  87  58 190 176  77 213 220  39 106  85   9 126 133 156
  79 208 101 144  67  55 127  88  19 181  64   2   1 187 162  94 204  15
  74 200 171  62 212  20 149 110  86 180  63 222 179 109  51  54 150  25
 177  50  21 172  56 139  14 161  68 217 120 228  65  83 205 191 117 199
 173  35  49 152  81 185  84 165  29 115 230 135  89 157 202 134 131 103
  31  12   7 146 192  13  92 118 218 203 108 163  34 151  78 164 168 153
  99  90  27 211  91 105 138  66 178 148 182 209 195  52  23 201 104 140
 147 227 194 221  47 128 219 223 102 186 125  96 210  95  28]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5637
INFO voc_eval.py: 171: [36  7 13 14 28 17 16  3 20  2 15 34 12  1 53  0 19 68 45 44 35 23 67 84
 11 24 66 87 60 42  8 40 52 54 71 29 43  6 26 76 63 82 69 61 22 32 37 21
 62 65 80 38 31  5 85 79 18 81 83 50 55 25  9 49 73  4 46 59 27 86 56 78
 30 77 64 51 88 57 10 41 47 72 48 39 33 74 70 58 75]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.7183
INFO voc_eval.py: 171: [ 40  83  85  93  84  59   6  22 100  76   2  58   1  20  69  17  82  96
  35  78  43  87  47  41  72  46  29  28  91  79  94  24  56  37  67  57
  97   8  51  32  26  33  77  21  86  16  36  75  63  53  61  34  11  45
   0   3  62  52   4  71  64  30  74  23  98  81 102  27  92   5   9  70
  39  14  65  95  66  19  18  15  13   7  38  50  68  80  90  44  12  55
  31  49  25  42 101  60  48  10  88  54  89  73 103  99]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.4921
INFO voc_eval.py: 171: [19 80 44 21 70 58 30 41 22 10 16 37 54 50 61 45 53 12 11 18 34 71 65 62
 26 36 33 59 47 31 15 20  9 72 69 24 67  2 23  5 28 74  3 77 40 14  0  1
 39 43 73 27 75 60 51 49 46 56  4 63 57 29 38 66  7 48  8 55 76 78 35 52
 64  6 68 17 25 79 13 42 32]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6191
INFO voc_eval.py: 171: [ 91  17  16  80  78   3 108  46  28  62 105  59 104  49  67  18  87  66
  52  71 100  29  30  72  21  83  40  96  48  79  45   4  92  44   1  47
  50  37  68 110  15  93  27  54  51  64  42  14  99  25  85  95  33  77
  57  12  31  89  34  22  70 101 106  10  23  26 113  61   2 109  73  84
  65  35  38  36  94  58  90   0 112  53   9  41  97  82  88  60  55  98
  43 114  74  13  20  11  39   6  19   7 102  32  76 103  75  56   8  86
  81 107  63   5  24  69 111]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.6557
INFO voc_eval.py: 171: [ 9 25  2  3 18  6  5 12 22 11  1 27 17 26 21 14  0 15  7 19 13 20  8 28
 10 16  4 23 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8481
INFO voc_eval.py: 171: [  0 409 408 407 406 405 404 402 400 395 394 410 393 385 384 380 378 377
 375 371 369 354 353 392 411 413 415 491 487 486 483 480 472 469 468 467
 464 463 461 460 458 457 453 435 433 432 430 420 419 417 351 348 347 340
 278 277 276 275 274 271 270 263 262 253 251 248 244 242 239 238 237 236
 234 233 232 225 217 279 493 285 300 339 333 332 331 326 324 323 322 318
 317 316 315 314 313 312 310 309 307 306 304 303 302 301 290 215 497 499
 765 757 748 747 746 739 738 727 718 717 766 716 714 710 707 706 705 704
 700 696 688 687 715 768 769 770 844 843 838 837 836 833 832 831 830 823
 805 804 802 800 792 790 786 784 778 777 774 772 771 683 669 666 659 599
 597 592 585 581 580 570 569 567 562 553 543 537 533 530 528 526 524 514
 508 507 506 504 600 498 601 608 658 657 656 654 653 652 651 650 649 645
 644 641 639 637 636 632 627 625 620 614 613 610 609 602 214 851  47  42
  27  43  22  45 144 124 158 188  19 145 146 192  18 157  86 194  17  67
 167  48  41  90 160  56  34 169 170 166  72  33 134 175 131 135 136  73
  74 128 182 164 162  36  75  29  37  28 120 133  92   2  58 202  10 204
 196  53 205 112  51 207 111  63   3   9   7  59  52 206 110  95 213  55
  57  14  65 117  93  12  11  64 197 198 210 147 199 113  54  94   1 114
   4 165 631 667 330 626  70  69 670 633 148 673 638 387 726 679 603 412
 728 191 439 449 563 542 258 811 534 582 481 825 834 529  96 518 517 515
 216 106 482 434 782 426 171 174 176 416 744  30 421 422 423 611 298 125
 376 596 185 289 288 287  26 471 477 628 629  66 450 448 428 126 500 711
 755  39 352 835 695 325 813 358 793  20 370 731 342 265 327 281 181 754
 187 501 103 510 362 360 243 224 761 240 701 799 646 762 102 245 130 280
 587 822 841 350  35 623  13 810 115 261 785 532 750 640 635 756 725  78
 489 155 484 401 548 381 365 531 538 443 758 577 193  21 252 672 451 221
 812 824 655 473 740 444 268 745 660 294 246 338 536 565 511 847 447 454
 541 209  79 372 452 566  88 797 398 122 485 299 208 266 399 845  31 346
 571 817 575 172 138 505 311  38 462 732 583 159 414 789 803 212 190 509
 256 388  68 549 383 721 361 429 305 709 568  46 842 478 573 180 186 806
 734 818 676 226  85 105 150 446 520 751 184 264 418 379 564 702 490 591
 547 183 156 662 149 386 154 341 760 108 345   5 617 488  89  84 572 373
 255 624 722  99 820 356 424 220 366 118 788  49 230 846 273 604 742 749
 127  50 671 527 557  82 431 574 829 152 685 293 593 153 247 560 254 780
 116 589 540 364 396 664 129 218 634 698 556 576 104  32 668 648 269 763
 794 647  80 465 140 588 558 819 720 584 123 368 227 142 296 555 703 781
 495 678 712  44 677 427 349 455 343 442 590  76 519 767 798 425 759 201
 848 691 681 708 445 692 282 729 723  87 559 787 815 168 151 438  81 545
  98 682 764 550  15 535 850 773 249 470 139 594 121 736 546 195 466 229
 459 752 686 403 516 219 684 578 367 179 100 606 849 344 228 808 337 694
 690 161 109 320 211 357 513 141  23 286 523 390 119 267 693 724 737 283
 618  62 544 586 674   6 642 391 363 783 328 621 178 779 661 689 397 235
 525 552 475 713 231 554 579 605 492   8 374 436 840 259  77 437 137 334
  91 319  24 308 292 796 775 335 496 272 177 291 101  71 297 521 598  61
 355 776 336 456 479 107 663 173 257 295 675 809 733 735 163 730 382 476
 616 827 719 222  25  97 619 474 441 522 630 821 502 595 807  83 814  40
 839 329 743 241 680 612 200 828 615 801 607 826 189 284 697 699 816  60
 250 359 389 665 791 132 203 143 260 321 494 223 741 753  16 795 643 503
 440 561 622 512 551 539]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7718
INFO voc_eval.py: 171: [ 35 127  19  12  11  39  37  85  23  90  99 119  46  29  50  69  86  82
 126   1   2 131 130   5 106   9  10  13   6  77  67  92  21  26  44  20
  28  22  16  32  43  70  17 122  27  59  91  80  78  18  41  31  34  74
  49 109  30  97 111 115   8 128 118 108 110 120   0  81 105  72  63 133
  62 129 123  98  42 125  79 124  47  66   4 107  88  87 103  96  95  71
 114 116  57  24 113  36  83  76  14  45  25 112  56  84  64  61  60   3
  52  93  40  53  48  94 121 102  73  75  51  33 132  65  55   7  89  58
  68  15  54  38 117 101 104 100]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.6066
INFO voc_eval.py: 171: [48 47 51 50 59  2 75 52  4 79 43 45 78 20 62 77 56 29  6 53 18 46 76 13
 36 27 57 15 44 16  7 25 55 65 85 66 84 83 17 30 70 71 31 34  9 41 80 64
 24 14 49 39 19 86 23 32 74 67 35 12 40 37  1 88  3 11 69 82 72 58 10 21
 42 60 73 61 54  0 63 87 26  8 68 38  5 22 81 28 33]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5335
INFO voc_eval.py: 171: [ 5 17  0 30 19 32 46  2  1 40 14 27 31 49  8 18 11 25 15 48 13 47 38 33
 26 24 35 20 41 12 34  9 10  6 43  3  4 23 36 45 22  7 39 44 42 29 28 37
 16 21]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3676
INFO voc_eval.py: 171: [53 26 25 67  7 69 37  9 39 33  4 41 43 59 20  8 22 17  6 38 56 18 68 51
 73 15 75 16 71 30 55 36 40 35 57 31 72 61 44 50 65 27  5 28 19 24 12 52
 64 11 10 66 54 47 62 63 46 60 29 58 32  1 48 49 45 74  0  3 21 23 34 14
 13 70  2 42]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5684
INFO voc_eval.py: 171: [ 0 38  2  3 30 27 22  8  9 18 31 12 20 40 24 23 29  7 25  1 10 13 28  6
 37 19 36  5 39 34 14 15 16 21 33 32 17 26  4 11 35]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.4819
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5748
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.300
INFO cross_voc_dataset_evaluator.py: 134: 0.746
INFO cross_voc_dataset_evaluator.py: 134: 0.465
INFO cross_voc_dataset_evaluator.py: 134: 0.512
INFO cross_voc_dataset_evaluator.py: 134: 0.496
INFO cross_voc_dataset_evaluator.py: 134: 0.895
INFO cross_voc_dataset_evaluator.py: 134: 0.682
INFO cross_voc_dataset_evaluator.py: 134: 0.173
INFO cross_voc_dataset_evaluator.py: 134: 0.564
INFO cross_voc_dataset_evaluator.py: 134: 0.718
INFO cross_voc_dataset_evaluator.py: 134: 0.492
INFO cross_voc_dataset_evaluator.py: 134: 0.619
INFO cross_voc_dataset_evaluator.py: 134: 0.656
INFO cross_voc_dataset_evaluator.py: 134: 0.848
INFO cross_voc_dataset_evaluator.py: 134: 0.772
INFO cross_voc_dataset_evaluator.py: 134: 0.607
INFO cross_voc_dataset_evaluator.py: 134: 0.533
INFO cross_voc_dataset_evaluator.py: 134: 0.368
INFO cross_voc_dataset_evaluator.py: 134: 0.568
INFO cross_voc_dataset_evaluator.py: 134: 0.482
INFO cross_voc_dataset_evaluator.py: 135: 0.575
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.479s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.347s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.347s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.360s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.367s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.368s + 0.002s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.369s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.373s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.374s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.374s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.373s + 0.002s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.377s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.378s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.528s + 0.001s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.384s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.393s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.384s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.379s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.377s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.388s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.387s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.388s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.388s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.388s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.387s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.382s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.546s + 0.002s (eta: 0:01:07)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.369s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.365s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.367s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.368s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.362s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.362s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.365s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.365s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.365s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.366s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.367s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.368s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.487s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.382s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.381s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.370s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.371s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.371s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.374s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.371s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.373s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.370s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.368s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.365s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.363s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.143s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [10  9 13 27 16 54 30 11 12 37  4 66 72 55 26 43 52 69 74  5 61 73 29 38
 63 28 14 71 44 51 36 25 24 70 65 18 20  6 68 34 58 19 75 59 21 31 64  3
  2 60 39 53  7 17 22 77 46 33 57 48 76 49 35 32 23 42  8 45 41 15 67  1
 56 62  0 47 40 78 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.4129
INFO voc_eval.py: 171: [19  3 23 11 16  4 21 24  8 26 22  2 18 25 13  0 10  6  9  7  1  5 12 17
 20 15 14]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6517
INFO voc_eval.py: 171: [221 117  90  78 212 213 218  64  62  61  56  48  47 142  33 245 112  91
 290 134 258 133 201 222 120 136  10 116 189 188 248  60 108 323   4 127
 163  86 118 286  63  67 324 266 153  97  95 322 211 257 170 326 251  50
 325 304 313  19  92 152 167 287  94  89 141 102   3 256 254 228 110  45
 190 147  96 220  41  81 303  53 185  52 161 293  28 276 143 150 168 311
 312  70 179 274 259  85 318  71 240 246 215 243 148 314  14 101  59 128
 299 281 172  42 130 263 321 174   6  43  16 171 151 275 182 204 135 144
 181 268  22 235  36 173 282 214 169 184  29 229  80 156 298 132 180 210
 296 113 310  21 131 249  12  58 267  87 233 149  54 241  65 234 104 195
   0  98  26 225 253  15  75 289  30 271   2 203 146 273 309  66 208 162
 272 100  20 317 145 291  83  55   7 216 320 269 316 265  24 166 262  99
 226  88 197 244  49 155 194  39  73 165  35 294 160  23 122 231 223 255
 111 295 200 252  25 227 129 328 219 176 105 140 278 199  74 106 260 109
 209 277  38 192  76  11 327  13 159 198 183 308 292  57 202 187 279 237
 114  32 261 125 306  51   8 236  93 329 207 115  40  46  77 217 154 239
 124   5 302 319 196  37 305 307 230 232 238 186 178 137 270 283  82 157
 264 123 175 107  34  18 315  84 297  68 247  27 139 300 193   9   1 206
 158  72  17 242 205 288 250  44 284  31 138 126 285 301 224 121  79  69
 177 103 164 119 280 191]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4888
INFO voc_eval.py: 171: [ 69   1 127 101  78  77   8  76  75  72  67  64  62  96  81  54   0 138
  88  45  66 131  46  92  21  79  12  63  30  38  68  17  27   5  97  34
  91  33  28  59 125  24  20 134  49 104 106  14 137 136 112  71  93  52
  47 107 123  23  36  35  65  37  58  87 124   6  15   9  29  44  39 102
  10  40   2  22  70  57  51 133 108  80  48  41 103 132   4 109  31  43
 128 119  25  56 116   3  84  85 114 118 135  94  55  42  98  82 100  50
 105  95 113 110 122 115  32  90  26  60 111  11  16 129 117  74  13 126
  53 130  61   7  89  99  86  19  18 120  83  73 121]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4887
INFO voc_eval.py: 171: [ 42  80  73  72  66  86  63  90  45  43  85  37  84   1 105  41  38  34
  67 106  35  95  89   2  88  60  20  36  79   4  17  70  50  59  39  71
  18  32  76  19  40  26  10  49  23  58  48  16  28  96  22   3  30  25
  77  69  74  56 100  31   7  81  21  47  94  91  87  82  11  51  99  57
 102   8  15  14  97  29  68  75  98  13  44  78  92  53  93  62   5 103
  52 104  55   6   0   9  12  54  61  83  65  64  33  24  27 101  46]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4914
INFO voc_eval.py: 171: [15 14 42  7 30  6  1 21 34  5  9 19 24 29  0 39 27 16  8 12 11 20 40  4
 10 31 36 23 18 25 28 13 32 37  2 43 41 17 33 26 22 35  3 38]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8810
INFO voc_eval.py: 171: [ 15 105 104  40  41  22 135  96  48 107 142 143  34 146  90  78  68  74
 158   4  42 144   0  44  94   6 132 131 149  69 140  70  65  55 112  57
 134 130  43 139  93 113  19  10 159  18   9  46  32  17  28 102   2  92
 148 152  38 124  26  66  81 109  80 118 106  71  62 123  35 157  52  91
 111 101 138 115  61 127  67  85 126  60  82 116  45  77   3  51 137  56
  97 156  21 145  72 122  75  73 160  30  37 119  27  64  23   1 136  84
  79 161 153  54  20  16  24   5  25  59  39 133  99  89 155   8 129  86
 141 121 117  14 103  58   7  11 114 108 120 147 110  53 125  83 128  36
  98  12  76  49 154  87  13  31  88  63  29  50  95 150  33 100  47 151]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6760
INFO voc_eval.py: 171: [80  5 39 37 57 88 62 92 91 32 52 34  1 72 89 94 71 59 18 65 66 36  3 17
 26 25  2 56 13 61 73 30 28 19 12 41 15 47 69 20 33 14 45  8 95 22 46 10
 40 24 58  6 23 49 85 84 53 21 82 93 44 78 63 35 60 51 75 48 38 42  4 11
  0 74 81 16 54 31 76 83 43 70 64 86  7 79 29 87 67 77  9 68 27 50 90 55]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.2549
INFO voc_eval.py: 171: [125  82  81  79  78  77  75  67  65  59  52  49  48  46  45  44 111 126
 127 138 217 227 215 214 202 190 189  40 184 181 180 174 166 145 144 231
 183  38 112 225  25  24 247  18  15  33  17   7 128  68   1 210 173 134
 243 137  28 156 158 232 209  86 219 167 132  54 175  89  50   2 164  47
  76  88  42  26  39  69 160 205 146 203 100 233 194 115  93 148 168 142
  57 114 108  58  32  99  60 151 106  29  63 161 239 237  43 220 120  83
  16  71 242  87 140 152 123 211 186 193 229 192 178  72 201 165 157  66
  41  73 235  35  90  27 224 196  14  74 182 118 197  70   3   8   4 249
 223 176   6  11 130 187 177 230  80  36 129 222 102 228 170 154 212 185
  62 240   9 155  85 159  61  37 198 244 213 221 195 208  98 121  53 149
 105  20 218 153 206  21 191  23  13 248 122 169  34 104 143 234  30  94
 107   5 204 150 207 236  31 131 116  96  10  97 139  51 147 117  92 172
 133 188  56 199 200  91  95 110  12 171 250 119 109  55 241 245 163 226
 179  64  22  19 135 238   0 141 136 101 216 103 113 162 124 246  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5469
INFO voc_eval.py: 171: [ 8 39 18 17 32 38 20 19  2  1 16 21 24 62  3 14 69 50 80  0 57 23 48 37
 68 71 75 77 44 43  9 67 85 65 58 35 83 25 33 89 82 56 42  4 28 74 15 10
 41 36 29 31 53 12 76 45 84  7 70 26 22 27 88 55 40  6 66 61  5 30 46 87
 72 11 51 47 86 73 63 13 34 59 60 64 52 78 81 49 54 79]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.7089
INFO voc_eval.py: 171: [ 60  93 171 140 142 144  16 161 119  35   7   9  63  36  69 164  53 133
 134 152 139  38  67 130  46 118  64 128  90  81  82  47  50  40  18   2
  37  13 157 100  54  72 108  14 117 137 162 148 131  75 165  73 116 132
  21  61 174  44  49  85  58 145 173  51 123  78  24 110 146  98  87 122
   4  39  48  65  84 115 127  42  29  11  77  10 160 147  88 104 143   0
 112 113 177  71  94  23 102  59  41 176  62  99  45  86 149  70 159 101
 167 163 166  97  91 125  52   8 168  26 129  20  89  80  22 169  25  30
  31 111 136 107  79 158  68 124 154  83   3 141  17 120  96   6  33  57
   5 106  28  55 121 103 126 151  66 114  95 150  74 175 153  27 155  12
  15   1 138 135  92 170  34  76  56  19 172 109 156 105  32  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5636
INFO voc_eval.py: 171: [ 40 138  81  45  77  71 100  58  19 124 108  74  33  73 104  21  95  18
  47  70 110  64 128  29  66   6  46  75  52  49  55  30 125   3  38  85
 126  32  31 107  99  22  82 118  94  12 114  89 130  60 113 122 133 109
  42   5 102  27  17   8  51 115  59  20  90  50  14 105  54  96  39 101
  13   1   2  86  92  68  57 111   0 134 120  23  24  76  91   4  56  48
  72  34 112  97 127  36  28 131  26  80 136  69 116  37  25  63  43 129
  16 135  10   9  15  41  88 103  35  79  98  62  61  44 132 123 117 106
 137  78  87  93  65 139 119   7 121  83  67  11  53  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.5538
INFO voc_eval.py: 171: [81 70 11 92 90 68 91 40 65 12 55 23 52 60  5 43 75 44 85 13 83 84 28 58
 74 41 45 72 64  4 35 88 46 59 39 38 69 86 30 26  0 29 82 16 19 21 18 95
 27 33 93  1 89 48 79 14 54  6 57 77 87 71 61 73 80 25 66  9 36  7  3 17
 53 76 62  2 10 67 63 94 37 51 78 47 24 31 15 22 42 32 56  8 34 20 50 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7361
INFO voc_eval.py: 171: [14  2 28 15 10 16  3 26 20  1  8 17 22 13  9 21 18 29  0 23 12 30  5  7
 27 24 19  6 25  4 11]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8300
INFO voc_eval.py: 171: [  0 421 420 419 418 417 416 415 411 410 407 406 405 404 403 400 391 390
 389 387 386 382 381 380 372 370 422 423 424 429 496 492 489 488 487 486
 479 478 475 472 468 467 369 464 452 449 448 447 443 442 438 437 436 433
 431 430 462 497 366 362 296 295 294 293 292 291 285 284 283 277 276 272
 267 264 257 255 254 252 251 250 249 248 247 246 245 302 304 306 307 357
 356 353 347 346 340 339 338 337 336 335 332 365 331 329 327 326 324 323
 322 321 320 318 315 314 311 330 498 502 509 768 762 758 757 752 750 740
 738 737 732 731 730 729 728 726 725 721 720 719 718 714 713 707 705 701
 778 779 783 784 860 857 853 852 851 850 848 847 846 845 842 841 687 831
 822 817 816 807 805 803 801 791 789 787 786 785 823 686 684 677 597 593
 592 582 581 574 565 559 558 553 548 547 605 545 540 538 535 532 529 526
 523 520 519 515 512 510 542 239 610 613 676 675 674 672 671 670 669 668
 667 663 662 655 612 654 652 648 645 642 641 635 627 626 622 621 620 614
 653 238 865  40 186 117 188 116 115  13  22 193 110  24 198 201 101  28
 202  99  12 171 205  96 207  30 208  95  91 185 118 119 122 169 168 166
 160 155 173 154 174 175 153 152 176 209 144 178 142 140 138  14 180 137
 132 131  20  21 127 143   8  98   7  36  70  68  63 222  61  60  59  58
  57  56  37  55  73  54  38 228 229  39 231 232  49  48  47  46  45  44
  43   1  75 221  77  31 214  84  81  79 215   4  78   3 219  76  15 218
 533 657  97  23 561 651 516 712 435 616 649 156 413 575 401 524 594 770
 615 503 554 528 490 105 544 609 644 473 744 465 683 133 459 454 808  69
 444 549 782 646 522 797 374 348 827 305 298 230 181 344 265 197 373   2
 368 839 363 184 259 223 830 343 463 355 664 849 840 745 128 828 145 345
 656 527  41 491 856 623 755 499 279 517 200 256 764  29 690 818 577  17
 378 595 167 697 480 392 351 501 360 552 813 399 106 506 715 534 396 397
 211 108 483 220 767 146 217 598 234 494 746 225 112 771 297 505 461 678
 590 194 195 204  25 564 774 806 769 441 560 724 319 716 539 756 751 643
 775 385 546 445 700 763 765 572 633 679 120 804 689 157 271 147 375 681
  90 270 859 500 141 858 149 637  32 722 829 261 384 275 557 586 673 619
 328 753  93 235 600 604 262  86 585 824  27 224 196 579  71  92 531 754
  85 130 458 371 440 571 588 578 367 810 242 734 583 466 570 855 601 536
 691 199 163 450 576 268 699 504 666 398 587  74  53 798 741 203 427  16
 177 748 739 796 773 485   6 580 309  94 521  34 861 161 735 760 584 114
 244 508 290 428 650  51 781 361 349 364 313 456 187 333 629 863 170  89
 812 236 308 434 426 631 408 821 300 802 134 541 636 685 591 704 446 451
 820 111  52 124 359 625 103 341 263 266 717 665 702 439 303 608  82 733
 189 640 589 617 165 563 693 795 402  67 599 139 379  42 414 206 299 227
 425 776 409 766 470 514 457 260 233 469 281 800 377 471 811  10 698 835
 596 513  87 289 688 278 113 814 123 240 102 569 191 556 432 511 703 109
 695 567 164 819 376 507  18 826  50   5 287 708 104 694 388 274 706 864
 354 543 125 482 460 172 179 793 838  66  83  65 834 551 562 383 477 790
 280 780  62 393 573 253 317 282 243 530 100 394 135 833 316  80 634 566
 749 121 210 837 759 854 159 736 843  64 182 602 647 606 611 129 342 555
 709 815 618 628  11 761 312 639 150 226 727 258  19 624 310 742 183 325
 395 484  33 358 772 658 747 481 288 301 630 550 537 777 158 638 743 286
 334 659 696 607 241 794 455 710 603 836 269 453 862   9 148 660 792 216
 680  35 476 493 192 151 661 723 799 825 692 632 518 412 273 809  26 474
 107 136  88 212 711 352 237 682 126 788 568 350 832 213 844 162 525  72
 495 190]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7728
INFO voc_eval.py: 171: [ 47  27  38 144  40 136  46  48  20  19  18  50  54  32  14  12 146 117
  72 109   6  85  88   2   1  13  29  68  81  89  96 150  16  23  35 145
  34  30 137  78  24  99  33  26   7  15  62  37 131  73  31  44 127 100
  17   5  10 110  90 118  82  84  49  66 135 139 148 103  61 125  53  74
 153 122  87 123 147 104  65  63  91 115 130  97   3  21  58 140 108  69
  77  64  59 132  45   0  39 152  75 116 138  76  83  98  51  28  80  79
 154  41 119 121 142   4  67  43 151  11  56   9  22  55  42 133 124 114
  71 149 107  57  60  95   8  94  70 106  92  86 134  25 126 143 111 120
 113 112 129 101 105  36 128  93 141  52 102]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.6171
INFO voc_eval.py: 171: [59 53 69 84 60 61  2 86 62 54  3 88 68 25 67 52 21 87 63 65 16 70 46 19
 35  6 56 58 22 15 78 38 39 95  7 83 51 75 90 96 97 47 40 18 24 72 74 32
  9 28 55 64 50 93 33 89 66 27 44 48 36  8 91  1 29 85 30 42 98 20 11 79
 10 94 26 77 17 81 14 12 82 43  4 76  5 37 57 23 34 41 49 73  0 31 80 71
 13 92 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5190
INFO voc_eval.py: 171: [12 43  0 31 46 32  5 21 80 78 54 65 42  1 39 15 37 57 33  3 11 23 30 38
 20 10  7 69 24 47 34 63 60  4 14  8 40 55 73 50 16 18 25 44 77 41 74 61
 66 53 28  6 79 62 52 19 13 56 71 48 36 29 59 58 68 27 35 51 70 67  9 45
 72 49 17  2 22 75 76 64 26 81]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.4073
INFO voc_eval.py: 171: [18 17 38  4 50 53 14 24  6  3 32 27  2 39 28 10 31 55 15 29  5 12 11 19
 36 40 13 52  1 26 41 42 21 30 54 37 57  7 47 25 22 35 45  9 20 16 23 33
 44  8 51  0 48 56 43 34 49 46 58]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5349
INFO voc_eval.py: 171: [51 19 26 16 15 34 12 42 37 65 49  4  3  1 46 47 36 66  2 31 61 50 33 45
 17 18  9 20 29 24 35 63 21 13 40  7 56 32  8 52 10 57 27 54 23 25 11 44
  5 38 55 53 67 22 14 48 59  0  6 30 43 64 39 28 58 41 62 60]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5839
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5860
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.413
INFO cross_voc_dataset_evaluator.py: 134: 0.652
INFO cross_voc_dataset_evaluator.py: 134: 0.489
INFO cross_voc_dataset_evaluator.py: 134: 0.489
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 134: 0.881
INFO cross_voc_dataset_evaluator.py: 134: 0.676
INFO cross_voc_dataset_evaluator.py: 134: 0.255
INFO cross_voc_dataset_evaluator.py: 134: 0.547
INFO cross_voc_dataset_evaluator.py: 134: 0.709
INFO cross_voc_dataset_evaluator.py: 134: 0.564
INFO cross_voc_dataset_evaluator.py: 134: 0.554
INFO cross_voc_dataset_evaluator.py: 134: 0.736
INFO cross_voc_dataset_evaluator.py: 134: 0.830
INFO cross_voc_dataset_evaluator.py: 134: 0.773
INFO cross_voc_dataset_evaluator.py: 134: 0.617
INFO cross_voc_dataset_evaluator.py: 134: 0.519
INFO cross_voc_dataset_evaluator.py: 134: 0.407
INFO cross_voc_dataset_evaluator.py: 134: 0.535
INFO cross_voc_dataset_evaluator.py: 134: 0.584
INFO cross_voc_dataset_evaluator.py: 135: 0.586
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 1999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.561s + 0.001s (eta: 0:01:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.404s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.388s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.377s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.379s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.375s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.375s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.376s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.375s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.376s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.378s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.487s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.411s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.403s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.391s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.393s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.400s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.407s + 0.001s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.402s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.396s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.394s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.390s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.391s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.387s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.463s + 0.001s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.390s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.392s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.392s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.381s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.376s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.375s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.376s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.374s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.378s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.379s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.378s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.456s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.370s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.374s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.371s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.365s + 0.001s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.367s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.372s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.371s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.369s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.370s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.369s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.369s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.369s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.891s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 7  8 45 25 28 15  9 44 56 10 11 59 38 63 64 24  4 61 41 39 34 36 32 48
 43 54 26 60 51 31 30 50 58 16 29 12  2 27  0 19 20 13 33  3 37 62  1 35
 46 14 53 23 57  6 47 55 42  5 18 21 65 22 40 17 52 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3748
INFO voc_eval.py: 171: [20  3 16 14 10  5 22 19  8 23 24  0  2 15  1 18  7 11  6 13 21  4 17  9
 12]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5902
INFO voc_eval.py: 171: [178  97  24 177 100 217 175 110 188 170  92  38  37  76  75  65  49  50
  51  53  44  36 182  52  90 203  84 221  71  95  96 231 152 105 106   3
 245   5 241 122 160  10 132 192  59 195  80  40  16 116 171 220 244 151
 242  77  78 144  60  91  31 233  48 176   2  79 135 239 209 115  13  56
 121  81 238 196 229 180 190  68 246 123 120 117 153 104  74 214 148  30
 236 193 167 140 181 174  83 128  41 234  42 118 215  28 222 184  94 157
 165  11  15 125 134 207  22 243 189 197 163 103 227 173 102  67 127 129
  57 159 138 101 156  73 219 200   0  17 194 141  43 210  70  62  98   6
 249 226 204   1 119 186  86 208 142 169 113 183   8 124  85 230  63 228
  82 218 145  14 185 212 111 112  34 130 158 237 187  99 133  47   4 211
  45 199  35  25  87 146  18  54 201  12 107 240 248  66 114  46 202  58
 247 166 155 223 172 198 206  55 131  72  29 109 126 205  88 108 149   9
 154 225 164 139  20  89  23 137  26 168 179  69 161  33 232 143 150   7
  32 235 224  61  21  27 136  93 216  39 191 162 213 147  64  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4311
INFO voc_eval.py: 171: [131  73  72  76  70  79  66  61  60  58 117  95  75   3   8  87   6 120
  12  40  34   2  59  50  74  90  92  29  13  62  41  28  22  21  89  19
 125  42  15 113  97  23  24  27  91  93  64 130  44  86 108  25  69  30
  94  99  47  35  43 127 114   5  57 103 121  55   7 104 119  56 100   9
  46  83  80 124  51  52  32 107  20  11  77  71  37  16 102 111 116  39
 118  10  18  31 112 105  78  26 126 115  68  33 123   0 122  17 106  45
  81  82 109   4  88  65  63  85  48 128  49 101 110  36  98  96  67  54
  84  14   1  38  53 129]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4962
INFO voc_eval.py: 171: [  0  91  89  88  87  86  82  71  70  67  64  47  45  43  38  21 109  61
  42  37  77  36  68  44   1  92  76  98  39 108  34  40   3  18  58  74
  25  17  73  59  75  50  28  41   9  20 103  99  19  31  48  93  10   7
  14  96  16  80  26  56  32  30  69  94  83  49   2  24 102 101   5  52
   8  22  46  29  85 106  62  72  23  95  79  51  63  60  53  65   6  90
  78   4  11  84  13  35  27  57 104  54 100  97  66 105  81 107  12  55
  33  15]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4852
INFO voc_eval.py: 171: [15 43 14 27 31  7  1 19 18 23  5  4 25  0  6 24  8 17 22 35 12 11 26 44
 16  9 28 20 21 36 41 32 39 37 10  2 38  3 30 42 29 33 34 13 40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8960
INFO voc_eval.py: 171: [  0  27 177 176  45 142 141  54 140  56  58  61 128  65 125 118  87  88
  89 102 182  23  96 196 198  19 189 206 191 193 185 197   8 192 208  84
   4  20 175  72 124 121  22  11  73  59  91 150 151 160 103 180 143 173
 168 104  36  77 137 149 209  44  64 112 169  71 129 123 138  49 119  78
 144  68  33  86 122  25 200  53 166 155  70  48  79   2   9  30  29  80
  69   3  97  94 195 158 212  34 179 156  46 204  26   1 134  21 162  98
 186 105 207  35  62 116 108 183  38  92 190  28  99 146 106  82 152 111
  10 184  32  85 170 171 159 211 164  24 135  39 110   7 163 199  16 201
 145 181  93  75 157 194 210  43  52 202 113  14 165  67 127  95  47 114
 115 203  12 130  13  76 126  50 161 167 153  31 148 174 154 120 100  81
 147 109  66 178  17 139  15 107  42  63 205 133 188  60 132 131  18 187
 101  90  57 117  74  37   5 172  40  83   6  41  55  51 136]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6792
INFO voc_eval.py: 171: [35  2 17 38 12 16 29 36 24 37 31 14 23 27 25 28  1 19 15 39 32  6 26  0
 11  9 20 18 13 10 34 22  7 30  3 21 33  5  8  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1818
INFO voc_eval.py: 171: [  0 134 133 132 118 117  91  87  85  82  81  80  78  68  66  62  57  54
  53  51 135 143 144 150 251 239 238 230 229 228 224 223 203  50 202 196
 194 193 187 186 179 165 164 153 197  47 263  15  18  19  31  38  34  25
   8  41  26   1 188  11  70 260 120 151 181 149 232 247 248  42 216 140
  10 167 168 253 146 100 155  36 169  27  73  69 147 226 207 103 156 110
  59 258 142  56 182 126  52 136  29 221   9 215 259  79  45 112 257 237
  98  89  77 242  95  28 124  46 212 241 176  76  35 130 214  61 121 208
 252 137 246  72 210  83  33  30 198  71  17  67 189   4  63  48 245  88
  94 211  60 162 200 231  37 191  65 190  75 213  24  12 206  86 234 205
   7 128 178 225 163 175  64 170 227 192  93   5  49 240  13 262  39 235
 261 161  74  97 127 141  14 148 171 217 244 125  96 249 104 184 138  22
 139 204 123  92 102  44 195 220  90 108   2   6 101 154  23 209  58 152
 158  55 111 122 180 177 183 254 159 109 160 174 172 145  20 113 106 255
 219  99 166  40 199  21 119  16 173 250  32 218 107 222 129 114 201 243
 105 256 185   3 236 233 116 131  43 115 157  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5576
INFO voc_eval.py: 171: [33  5 26 10 13 31  4 14 16  1  3 12 11  2 42 15 36  9 63 39 38 17 51 46
 52 18 60 30 53 61 50 28  6 66 22 47 57 40 43 35 24 37 58 64 19 29 25 48
 23 41 65 59 49 32 20 56 55 62 54 27 34  8 21  0 45  7 67 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5850
INFO voc_eval.py: 171: [108 106 119  51 105 127  10  25  76   3  90  55  26  28  54 104  99  58
  97  72   4  36  44 113  78 114  88  64  13  37  57  89 123   5  61 100
 121  27  98 117  34  59  42  96  65  70  52   8  39 129  32  62  68  18
 102 111 109  75   1   9  87  49  92  33  63  35  19  82  11  60  12  29
 125  50  91  46  95  16  85 122  17  77  30  74  15  38  45  79 118  84
 110  67  40  56  71  22   6  53  66 107   2  86 116   7 124  81 120  69
 103  80  21  14   0  93  83  48  47  73  41 126  94  43 130  24 115 128
 131  23 112  31  20 101]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5207
INFO voc_eval.py: 171: [15 66 13 36 26 35  8 60 46 32 17 38  7 50 19 44 51  5 28 43 11 31 61 56
 34 16 14 24 41 47 37 64 18 62 21 49 42  2 59  6 25 29 39 54 48 53 27 65
 30 40 57 45 20  1 55 33 12 58  4 23 52 22 10  9  3  0 63]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.5927
INFO voc_eval.py: 171: [67 59 57  8 48 33 78  9  3 49 19 79 46 72 53 62 80 10 36 31 52 29 50 39
 74 32 69 37 61 38 58 71 22 63 34 35 65 73  0 24 13 23  1 76 82  5 14 70
 42 64  6 60 30 83 41 17 28 40 47 81 66 27 55 21 26 75 20 56 44 11 18  2
 68 77 43 54 16 12 15 51  7 45  4 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7265
INFO voc_eval.py: 171: [26  4 14 15 12 16 25  6 11 21 20  3 19 28 27  0 29 23 17 22 13 24  7 10
 18  5  8  2  1  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8167
INFO voc_eval.py: 171: [  0 407 405 404 403 401 399 397 395 392 389 388 387 408 386 384 383 382
 381 379 377 376 373 372 371 370 366 385 365 411 413 477 475 474 471 470
 468 464 463 462 460 454 453 412 451 446 445 444 443 436 435 433 428 427
 425 424 418 447 478 363 357 293 291 290 288 286 285 283 282 281 279 278
 268 294 267 265 263 262 260 259 258 257 256 255 253 251 248 266 359 295
 297 356 355 354 350 349 348 346 339 334 332 330 329 296 326 318 317 311
 310 309 308 307 303 302 301 300 298 322 247 479 483 718 717 712 708 704
 697 693 692 684 682 681 676 720 675 673 672 671 669 665 664 663 662 657
 652 650 646 674 643 721 723 786 784 781 780 779 775 774 773 772 771 767
 766 722 760 752 748 747 742 741 739 738 735 729 728 726 724 757 481 637
 631 565 564 563 562 561 560 557 550 548 547 546 535 567 534 527 519 511
 503 501 497 496 495 493 492 486 484 529 633 571 573 625 624 623 622 621
 619 618 617 616 615 614 610 572 609 604 603 602 600 598 597 593 591 586
 578 577 574 606 246 792 150 187  33  12  18  19 188  11 128  61  62 189
 126  36 125  50 191 124  10 121 194 120  46  44 195  43  54 117 116   7
  63 192 197 186 184 160 161 162 158  55 165 166 157 156 154 153  53  34
  84 135  13  56 175 149  57  58  17 139 179  52 182 138 137  51 136 173
 198   8 104  23  75 105  24   2 201 103 222 223 224 225   1 226 102  39
 101 229 231  25 233  38  96 236 237  28  29  90  89  88  86  80 106 159
 220  70 111 211  71  68 205  41 204 202  40 203   4  73 219 218  67   3
  42  35  72  69 714  95 488 755 394  26 594 151 414 500 630 733 509 530
 508 612 440 437 472 473 746 432  21 420 456 753 596 499  22  30 756 362
 181 340 768 331 327 778 178 210 269 700 228 699 252 238 242 782 658 312
 656 174 193 498 455 688 687 667  91 592 711 634 409 249 315 276 705 118
  37  14 504 423 245 777 284 709 659 323 439 450 512 176 342 744 715 183
 694 426 378 749 605 232 706 636 683 353 129 515 601 230 552 208 164 595
 459 458 710 374 364  66 113 146 736 200 549 398 541 107 422 419  81 122
 241 628 375 476 487 668 185 130 171 466 361 168 177  99 367  65 480 627
 502 141 533  49  78 613  97 190 620 531 785 482 145 611 761 328 341 520
 325 273  79 569 457 523 199 140 703 513 544 635 391 131  76 642 695 532
 206 119 537 132 207 271 783 540 524 402 216 250 215 750 213 568 333 553
  83 400 505 417 240 545 679 654 351 155  82 522 514 321 645 538 678 180
 314 698  31 277 234 685 607 115 525  94 320  16 196  98 640 306 769 518
 581 410 555  45 406 344 696  20 465 108 754  48 558 494 661 536 319 239
 788 579 489 680 209 554 100 539 599  87 790 737 590 110 791 570 243  47
 112 707 261 608 649 713 763 506  93 653 393 507  77 583 169 274 740 352
 762  74 638 142 651 452 212 438 727 589 758 254 380 585 543 292 448 641
 235 526 442 421 719 660 689  27 289 776 765 764 716   5 485 626 588 730
 751 429 109 144 666  60 632 360 170 575 770 441 467 275 745 516 227 648
 163 280 551 347 629 148 304 691 217 644 415  59 655 517 670 430   6 324
 313 510 787 127 123 368 214 491 580 528 584 725 270 358 134 147 576 338
 152 701 702 542 686 587 114 316 172 734 305 299 336 264 345 461 287  64
 559 369 732 469 244 690 431 743 582 677 167 221 490 789 449 556 647 343
 416 143 434 337 521  32 566  85 133 731   9 272 639 396 759  15 335  92
 390]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7659
INFO voc_eval.py: 171: [ 18 147 146  22  23 123  25  19  41  28 137 117  32  96  34 116  38  56
  16   1   2 153 150  94   6  52  46  77  11  12  50  51  15  90 127  74
  33  24  35  31  30  86 105  14  13 101   7  55  49  66  83 138  79  10
 133  17 136   5 129  80  89 149  87  37 131 140 132 157 148  65 111 110
  97  71  63 106 134  81 122  53   0  47  59  98  70 121  99 114 139  43
 141 128  62  68 158 126 102  21  61  69  39  91  57  20 104  84  93  82
 115  44  58  26  42  85   8 120  29 130 113  88  76  36  73   4 156 144
  75 152 119  78 124 107 145 108   9  67  27  92  45 100  95 109  72  54
 155 125  64 135 112 118  48 151   3 143  40  60 154 103 142]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.6015
INFO voc_eval.py: 171: [51 45 49 59 69 52  3 53 71 43  4 68 21 57 72 19 46 58 28 56 16 50 79 44
 13 18 70 62 54 38 26 27 15 24 29 22 81 14 34 20 80 78 31 42 66  6 64  1
 23 32 36 75  5 10 76 48 60  2 47 74 82 12 55 17  7 41  9  8  0 77 63 61
 40 37 35 73 39 65 30 25 33 67 11]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5094
INFO voc_eval.py: 171: [32 44 12 21 72  1 34 62 51  4 15  5  2 49 11 45 33 56 24 38 39 66 73 61
 65 36 64 43 46 55 22 27 13 16 58 69 48  8 63 37 29 40 60 10 19 30 59 50
 35 41 31 25 42 28 18 20  6  7 70 14 57 67 68 53 26 54  9 23  0 71 17 52
  3 47]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3194
INFO voc_eval.py: 171: [23 21 45  7 61 64 16 35 38 27  6 12 37 49 33  2  1 13  9 42 18 52 36  5
 24 63 34 11 44 51 54 68 14  3 47 59  4 20 50 29 58 41 25 60 62 65 32 55
 39  0 10 56 15 28 70 30 40 26 66 31  8 53 46 57 17 71 48 22 67 69 19 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5220
INFO voc_eval.py: 171: [44 19 18 35 37 39 47 23  5  4 59  2  1 49 28 41 14 61 32 46 45 10 31 30
 34 24 21 12 58 57 36 42 25 13 22 17 26 60 11 55 15 40 48  3 29 54 27  9
 43 53 16  7  6 50  0  8 33 56 51 20 52 38]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5713
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5612
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.375
INFO cross_voc_dataset_evaluator.py: 134: 0.590
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.496
INFO cross_voc_dataset_evaluator.py: 134: 0.485
INFO cross_voc_dataset_evaluator.py: 134: 0.896
INFO cross_voc_dataset_evaluator.py: 134: 0.679
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.558
INFO cross_voc_dataset_evaluator.py: 134: 0.585
INFO cross_voc_dataset_evaluator.py: 134: 0.521
INFO cross_voc_dataset_evaluator.py: 134: 0.593
INFO cross_voc_dataset_evaluator.py: 134: 0.726
INFO cross_voc_dataset_evaluator.py: 134: 0.817
INFO cross_voc_dataset_evaluator.py: 134: 0.766
INFO cross_voc_dataset_evaluator.py: 134: 0.601
INFO cross_voc_dataset_evaluator.py: 134: 0.509
INFO cross_voc_dataset_evaluator.py: 134: 0.319
INFO cross_voc_dataset_evaluator.py: 134: 0.522
INFO cross_voc_dataset_evaluator.py: 134: 0.571
INFO cross_voc_dataset_evaluator.py: 135: 0.561
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.458s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.372s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.362s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.364s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.369s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.369s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.370s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.365s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.364s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.366s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.367s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.438s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.379s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.393s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.380s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.378s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.385s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.391s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.390s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.389s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.389s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.385s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.385s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.383s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.642s + 0.001s (eta: 0:01:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.417s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.399s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.396s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.390s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.388s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.387s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.392s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.389s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.390s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.389s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.390s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.386s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.574s + 0.001s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.386s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.373s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.369s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.364s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.364s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.362s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.363s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.363s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.359s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.360s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.360s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 56.484s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [15 28 25  7  8  9 43 51 42 53 10 12 56 57 34 37 41  5 58 26 54 38 49 29
 27 24 31 33 46 13 23 48 44 45 55 30 50 32 52 59  0  1 36  3 60  2 14 40
  4  6 16 20 17 18 39 22 35 19 11 21 47]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3470
INFO voc_eval.py: 171: [14  3 11  7  9 13  4 16 17  6  0 12  5  1 10  2  8 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.5914
INFO voc_eval.py: 171: [213  50 168 129 150  33  73  35  22  36  68  42  62 188  47  48  49  72
  89  51 120 158  92 166   6 174 172 102 175 241 113 106  97 227 191   3
  71 104 194  94  13  77  81 216  87 179  91 237  38 199  75  56 214 173
 239   5 232 121  86  30  74 134   2  54  57 149 233 143 226 230 192 118
 117  46 151 242  10 111 206 190 115 229 240  65  76  39 210  78 137 182
 184  40 112 187 171 235 211  25  28 231 164 223 193 218   8 154 238 195
 126  41 204 170 123 132 177 161  95 100 133  93  52 101  19  63 147 205
  11  99 119 234   7 160 136  14 180 189 198   0  12 200   9 185   1  79
  59  55 125  98  90  82 222 186 156  23  32 228  24  67  43 202 244  80
  15 139 217 153  45  69 127 131 122 144 146  83 208 245 207  18 178 209
 225 128 145  84 197  17  60  31 116 203 140 109 165  96   4  34  53 103
 224 157 236 142  64 130  21 243  88 141  66 159 108  85 220 155 221 219
 152 135  58  20 107 114 110  16 167 124 163 196  27  61 181 212 138  44
  70 183 162 176 201  37 169  29  26 105 215 148]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4497
INFO voc_eval.py: 171: [ 64  69  72  74  75  76  62  78  10  80  87  53   6  94 112 116   1  65
 127  44  77  35  89  91  63  30   0  20  24   5  67  45  88  28  29  14
  11  92  46  90 123  26 109  19  68 126  71  86 125 101  49  59  98  23
  96   3 108  48  31 104  36  58   7   2  34  66  52 115  99 119   8 102
  27  22 122  84 118  51  21   4  37 114  70  43 105   9  55  79  33  25
  54  32  39  60  50 110 107  18 120  16  83 121  47  97 106  42 124  73
 111  13  85  15  82  17  61 128 103 117  12 113  40  41  38  95  81  93
  56 100  57]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4602
INFO voc_eval.py: 171: [ 0 33 37 38 41 58 61 62 31 65 68 74 77 78 79 80 81 66 30 96 19 39 34 64
 95 55 82 86 32  1 45 21 17 53 67 52 69  3 35 28 23 36 70 15 16 87  9 43
 26 93 18 83 20 25 11  5 84  8 13 14 44 92 27 10 71 42 90 60 63  4 75 22
 88 46 50 72 56  2 59 29 12 76 49 54 91 51 73 47 94 48 89 24 85 57  7 40
  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4912
INFO voc_eval.py: 171: [14 13 31  7  1 25  0 23 18  6 17  5 21 19  4  8 16 26 24 15 20 27  9 32
 22 30 28 10 12 11  3 29  2]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8897
INFO voc_eval.py: 171: [107 161 159  99 157 153 116  39  81 149 148 162  46  50 105  54 118  56
  70  85  73  74  76  47 163 117  23   6 178  16   4   1 169  22 166  75
 104  18 124  20 168  87  62  49 147 103  48 176 152 125  10 144 134  52
  37  86 126  64  41 119 130 100 113  31  28 120 171  66 179  72  58 108
  93  57   2 114  60  17   7   9  88 143  79 156 146 141  82   3  63  19
 142  32 177  59 151  97  77 102 135  61 165  71 181  30  40 131  67   0
  89 175  51 180  90   8 160  42  34  83  45  25  21 136  44 121 111  11
 172  65 112   5  29  69  36 158  26  94 127 132  24 155  43 106 139 138
  33 137  91  14 122 173 167  27 140  96  80  84  55 115 164 109 123 128
 154  78 129  53  38 133  13  15  12 170 145 150  92  95 174  68  35 110
 101  98]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6890
INFO voc_eval.py: 171: [46  2 23 51 39 17 21 42 38 49 19 34 32 50 48  3 37 41 10  9 26 52  8 28
 30 20 31 40 24  7 35 16 22  1 12 14 47 27 15 29 13  6 18 44  4  5 45 36
 33 43 11  0 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.2222
INFO voc_eval.py: 171: [  0 164  74  73  72  70 166 167 172 173  58 185  54 188 189  49  47  45
 163  44  77 158 117 116 115 114 226 123 124 125 128 130 102 101 139 141
 151  83 156  78  43  60  29  16  41 203 201  22  23 197  12 214 195  31
  15  35   5  36 193 194  37 140 212 131 157 120 143  61 103   8 153  51
 223  62 142 122 211   1 129  32  48  24  91  92 126 135 217  71 190  81
   6  46  27 133 127 221  99 109  38  87  39 184  97 177 154 118 202 169
 222  26 183  28 220  33  86  69  68   2 210  14 186 113  40 206  59 204
  53 176   7 180 108 161 104 181 215 170  64  80 111 187 147  56  57 160
 198  75 191  65  79 119  63  21  34 137 196 175 159 209 199  84 136  82
 110  85 144  55 205  30 149  11 227 192   3  18 165 138  52 178 225 105
  94 155  93 216 107   4  98 200 182 224 148   9  20 132  90 213  66  76
 146 162  50 179  88  13  96  10  19 152 145 174  17  42 171  25  89 150
 106 121 100 168 208 134 207 112 218  67 219  95]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5616
INFO voc_eval.py: 171: [36  7 29 17 14 34  4 16 20 18 13  3  1 15 47  2 42 19 33 57 58 68 52 39
 41 70 23 11 59 65 69 24 56 53 38  6  8 49 48 64 40 62 22 43 73 25 32 26
 72 27 74 63 54 37 60 31 30 35 67 21 61 46 71  5 66 55 28 51  9 10 50 12
 44 45  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5893
INFO voc_eval.py: 171: [ 99  45  68  22  95  97 107   8  82 114   2  52  48  88  25  94  49   3
  90  64  40  33 110  12 105 103  34  23  80  43  62  55  91  38  59  81
   6  51  60  31  29   4  89  54  24  61 109 102  66  57 116 101  32  71
  72 113  92  35  56  17   0  14  87  46  30  78  41  16  83  76  85  65
  93  70   5  26  13 106  19  69  75 100 111  11  98  20   1  67  39  74
   7  84  10 108  27  79  63   9  53 118  47 104  73  37  86  96  15  44
  58 112 115  50  36  77  18  21  28 117  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5509
INFO voc_eval.py: 171: [13 61 56 16 34 35 24 11 30  5 45 47 18  7 43 27 29 37  4 48 42 40 12 20
 17  9 57 22 26 33  6  0 46 38 15 51  2 39 25 55 53  1 52 19 10 60 58 32
 28  3 21 59 31 50  8 41 23 49 44 14 54 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.5869
INFO voc_eval.py: 171: [66  8 44 76 32 56 54  9  2 18 77 60 42 71 78 48 50 10 35 36 49 59 30 47
 33 31 38 75 21 61 55 27 70 67 72 22 13 64 69  0 34  1 74 15 12 40 26 62
  5 11  6 20 23 80 57 29 37 39 43 81 17 53  3 79  4 25 68 24 58 19 73 51
 14 46 16 65 63 41 45 28 82  7 52]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.6813
INFO voc_eval.py: 171: [25  4 12 13  8 14  5 23 20  7  3 15 19 27 28 11 26 18  2 24  9  0 22 21
  1 10 16 17  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8300
INFO voc_eval.py: 171: [  0 440 439 438 437 434 433 432 431 430 429 428 423 421 420 441 418 414
 410 409 407 406 405 403 402 401 400 397 395 394 393 417 442 444 446 492
 491 490 489 488 486 484 482 479 478 477 475 473 472 470 469 467 447 448
 449 450 454 456 391 457 459 462 463 464 465 466 458 389 388 384 327 325
 324 318 317 315 313 311 310 309 308 307 306 304 302 301 300 282 283 284
 286 287 289 328 290 293 294 295 296 297 298 291 493 329 331 383 382 381
 380 379 378 377 376 375 373 372 371 368 367 366 365 362 336 337 339 342
 345 346 330 347 352 353 354 355 358 359 351 494 496 499 711 710 709 708
 707 705 704 701 698 694 690 689 685 683 682 679 678 656 657 659 660 661
 662 712 663 668 669 670 673 674 677 667 653 713 717 769 767 766 764 763
 762 761 760 759 757 756 755 753 751 750 749 744 720 724 725 726 727 730
 714 732 734 736 737 741 742 743 733 279 652 650 581 580 578 576 571 565
 564 561 560 559 555 553 552 551 550 549 548 501 503 508 516 517 521 583
 523 534 535 536 538 540 545 524 651 584 587 645 644 643 638 637 633 629
 623 620 619 617 616 611 610 609 608 607 588 589 590 591 592 595 586 596
 600 601 602 603 604 605 597 278 387 774  54  55  57  59  62 184 183 182
 181  53  63 179 178 175 173  67  69  70 169 167  65 166 187 188 212 211
 210  44 206 205 204 203 202  52  49 198  50 196 195 194 193  51 190 189
 199  71 163  72  90  91  95  98 102 103 129 104 277  89 105 108 127 126
 125 124 112 117 119 120 106  88 136 137  73  74 161 160 159 158 157 156
 155  76 152 151 150 149  82 144  86 140 139 138  87  43 217 192  42 252
  17 251  18  19 249  20  22  23  24 254 218 245 244  25 241  27  29 238
  32  33  34 246 236  16 257   1   2   3   4   7   8 269 268  10 267 256
 266  11 263 262  12 261 260 259  13 258  14 265 235 121 230 221  41  40
 228  35 225 229 224 232  38 222 219  37 227 223  39 348 321 476 471 107
 579 201 699 500 392 695 131 361 702 697 231 164 419 654 174 646 114 452
  28 655 562 191 723 416 239 369 338  48 141  96 100  36 745 122 537 413
 622 360 728 451 577 357 513 520 316 176 522 370  85 197 247 531  84 691
 495  81 502  66 612 208  77 455 468 240 768  68  97 130 688 216 145 721
 363 681 132 233 399 453 142 606 215 615 680 133 498  30 412 765 573 598
 771 505 323 326  80 474 271 109 542 386 526 519 512 116 529 177 511 185
 557 396 627 118 641 436 684 541 274 569 292 356 665 483 200  78 544 525
 585 237  21 528 624 599 485 514 349 285 566 445 333 172 480 248 510 186
   6  47 276 649 664  83 213 460 411 773 675 738 666 518 404 568 312 527
 398 154  94 207 319 530  99 424  92 770 558  15 621 101  45 672 692 350
 593 546 556 574 696 686 631 506 113 772 532 305 110 148 123 272 636 341
 435 639 242 344 255 746 640 408 153 415 180  46 632 533 752 628 687 504
 209 322 570 147  75 320 700 497 111 626  60 515 715 563 299 234 614 572
 128 314 226 618 214 634 575 739 676 303 748 264 422 374 162 143 735 335
 547 716 481 507 165 146 706 722 539  56 630 613 747 509 729 280 758  93
 334 275   9 719 731 461 171 253 343 250 135 340 635 567 425 170  31  79
 115 658  64 288  61 332 671 487 543 427 648 426 703 594 754  26  58 443
 693 168 647 364 740 582 554 270 273 718 390 243 281 385 625 220 134 642
   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7460
INFO voc_eval.py: 171: [131  31  33  34  74  37  39  44 116  48  49 110  51 108  54 100  97  57
  92  91  87  83  26 140  72  23   1   2 146   6   7  10  11  12  13  24
  15  16  14 142 141  18  21  22 135 118  46  77  19  32  28  65  29  80
 126 121   5   9 130 145 122  36  86 103 143  63  94 133  75 124 149   0
  84 107  68  93  67 119  61  52 125  78 134 117 112 104 113 136   4  64
  58  56  95  42  45  70  76 150  82  38  81  89  66  88  98  17  85  71
  50  41 138 109  20  59  60  73  99 148  35  25 139 111  47   8  79 101
  69  55  62 120 114  27 106 147 128 137 102  90  30 127  43  40 123 105
   3  96 144 115 129  53 132]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5884
INFO voc_eval.py: 171: [38 51 41 62 44 43  2 37 65 45  3 19 64 49 36 50 39 40 70 61 47 14 16 63
 54 26 17 48 15 24 31 12  5 23 13 71 25 27  4 22 18 35 67 32 69 20 58 59
  0 30 60 28 29 46 21 33 53  7 57  6 42  1 55 56 66 52  9  8 10 11 34 68]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5121
INFO voc_eval.py: 171: [29 38 12  2 64 19  6 27 43  4 39 54 15 42  1 47 20 11 32 28 13 37 57 21
 31 65 40 49 59 33 23 58 30 24 18 16  9 61 50 46 10 14 56 36 55 53 62 25
 35 41 60  7 17 22 34  8  5 45 63  0 51 48  3 52 66 26 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3378
INFO voc_eval.py: 171: [18 19 54 37 51  6 14 22 29 27  2 39  5  8 30 25  7 56 12 15 11 10 41 26
 57 34 28 55 53 43  3 23 42 49  1 20 21 48  0 24 36  4 17 32 35 52 13 40
 46 47  9 31 45 58 38 16 50 33 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5662
INFO voc_eval.py: 171: [31 43 14 13 28 38 10 30 17 32 37 34  4  3  1 21  2 44 36 26 35  7 18 24
 27 15 16  9 41 42 20  8 40  6 11  5 19 22 29 39 25  0 33 12 23]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5172
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5604
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.347
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.450
INFO cross_voc_dataset_evaluator.py: 134: 0.460
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 134: 0.890
INFO cross_voc_dataset_evaluator.py: 134: 0.689
INFO cross_voc_dataset_evaluator.py: 134: 0.222
INFO cross_voc_dataset_evaluator.py: 134: 0.562
INFO cross_voc_dataset_evaluator.py: 134: 0.589
INFO cross_voc_dataset_evaluator.py: 134: 0.551
INFO cross_voc_dataset_evaluator.py: 134: 0.587
INFO cross_voc_dataset_evaluator.py: 134: 0.681
INFO cross_voc_dataset_evaluator.py: 134: 0.830
INFO cross_voc_dataset_evaluator.py: 134: 0.746
INFO cross_voc_dataset_evaluator.py: 134: 0.588
INFO cross_voc_dataset_evaluator.py: 134: 0.512
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.566
INFO cross_voc_dataset_evaluator.py: 134: 0.517
INFO cross_voc_dataset_evaluator.py: 135: 0.560
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 2999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.550s + 0.001s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.391s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.371s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.368s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.369s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.371s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.369s + 0.001s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.368s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.368s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.368s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.369s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.370s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.613s + 0.001s (eta: 0:01:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.404s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.396s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.381s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.380s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.381s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.391s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.391s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.388s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.387s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.387s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.386s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.385s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.454s + 0.001s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.371s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.376s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.391s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.386s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.383s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.383s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.380s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.378s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.375s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.375s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.376s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.378s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.578s + 0.002s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.404s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.376s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.367s + 0.002s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.376s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.373s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.375s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.370s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.375s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.376s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.376s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.379s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.378s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.168s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 9 25 14 26  7  6 38  8 48 10 39 53 51 34 32 55 37  5 24 56 29 35 52 46
 27 22 31 12 40 41 45 54 21 30 23 28 42 47 50  4  0 57 20 58 11 33  1 13
  3  2 15 18 17 16 44 36 19 49 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3529
INFO voc_eval.py: 171: [14  3 11 12  7  9  4 17 16  6  0  5 10  1 13  8  2 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6923
INFO voc_eval.py: 171: [224 235  87  25  52 164  47  61 186 177 152 163  34  71  36  37  38  70
  66  76  42  90  91  50   3 117 110  49   7 198 169 104 239 213 171  48
  15 101  95 144 172 127 189  57  85  80 192  84 102 211 118 131  69  56
  89 237   5 170  72  73   2 230 115 138 190  53 143 114 214  29  46 240
 228 233 223 112 145  64 108 187 188 238 204 227  12  40  75 142 134 191
  92 208  74 185  41 109 181 160 180  31 122 193 167 229 236 120 215 231
 220   9 209  78  28 116 234 149 202 129  93 166  97 174 157  14  98  39
  16   0  62 203   8  51  99 182 178  21 128  10 155  65  55 113   1 197
  26 196  43 226  45  20  13  96 219  33  59 139 123 133 148  27  88 130
  81   6 135  82  67 212 183  94  18 150 175 205 124 242  79 200 206 243
 119 241  60 100 126  24 184 222  83 125 201  32 195 207 140 136 179 199
 161  19 232 107  63  23 151 141 132 153   4  86  17 137 106 121 176 168
  11 103 194 218 221 217 210 216  22 156 159 147 162  68 165  58  44  35
  77 105  30 154 173 158 225 146 111  54]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4052
INFO voc_eval.py: 171: [ 64  42  51  57  58  59  61 128  67  69  70  71  72  73  10  75  83   6
  89 113 117   1  29  84  87  34   0  23  27  43  60  14   5  19  11  85
  28  25  86 124 111  88 127  20  82  44  66  63 126  55  30  99  22  94
  47 109  91  35   3  46 104  54   7  31   2 119  96  21  62   4  26 118
 116  41  50 122   8 100  49  65  79 105   9 115  53  17  36 108  32  33
  74  56 110  76  37  92  48  39  15  24  13  12  18  45 129 107  40 121
 114  68 112 103  52 120 125  98  95  38 106 123  16  97 102  78  81  90
  93 101  77  80]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4752
INFO voc_eval.py: 171: [  0  34  35  39  40  41  45 100  58  61  33  65  68  69  70  73  78  81
  82  83  85  66  32 101  20  84  89  38  86   1  47  56  72   3  36  71
  22  55  17  37  30  25  74  90  15  16  96  28  87  21  44  46   9  18
  27   5  43   8  88  13  11  29  95  76  14  10  67  93  23  64  49  91
  79   4  97  63  48  31  75  92  94  42  53   2  60  50  54  98  57  12
  80   7  59  62   6  51  99  77  26  52  24  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4925
INFO voc_eval.py: 171: [12 11 29  1  5 23 15  4 21  3  0 16 19 18  2  7 14 22  6 13 24 30 20 25
 17 28 26  8 10  9 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8648
INFO voc_eval.py: 171: [ 50 156 152 151  41 150  48  49  51  52  56  58  63  72  75  76  77  79
 119 118  84 117  88  90 111 108 160 162 103   4  20 182   6 164  21 170
  23  19 172   1 173 166 165  24 155 126 107 125 105 180 137 147  10  54
  39  89 115 127  66  33 104  29 175 132  44 183 120 116 122  74  59  68
 143   2  62  70  97  61 112  42  17 159 146 181 144  18  92   7   9 154
   3  82  80 138  64   0  34  85 179  60 169 106 186  73 149  32  94 101
  53 184  26  93   8  47 133  22  67  46 121 176 139  86  11  27 161 163
  25  36  45  96 145  38 113   5 128  95 100 171  30 110 168  98  71  15
  35  28 157 158 141 177  99  31 142 148  65 167  78  13  16  87  40  55
 130 140 131  43 123 174 134 114  81 129  83 185 136 124  91 178 135 153
  37 109  14  12 102  57  69]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6727
INFO voc_eval.py: 171: [40  1 20 45 34 37 18 14 33 29 16 43 44  7 42 30  8  2 24 32 36  6 23 46
 27  5 21 17 28 31 13 12 35  0 19 41 15  4 10 38 11  3 26 39 25  9 22]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.2041
INFO voc_eval.py: 171: [111 121 120 119 116 112 110 109 100  99 124  98  76  75  72  71  70  68
  61  59  57  81 126 134 136 219 208 201 199 194 192 191 190 186 185 180
 168 167 162 161 159 158 153 151 146 138  53  48 223  22  21  39   1  29
  43  42  17  44  13  46  27  33   6  16  35 127 118 125  30 135 210 137
 209 148 152  60   2   9  88  36 122  89  47 130  23  69 187  25  79 129
  86   7  96 123  45  38  37  95 172 181 105  50 213 218  82 217  40 164
  26 149 113 200 184  73 204 178 156  63  84   3  67  24  66 101 107 216
 143  62  31 171 203 108 104   8  15 155  51  58 206 188  28 195  77 176
  83  20 174 165  78 212  64  55 207  32  56 114 182 170 132 154 106  54
 193 196  80 139 131  52 202 189 222   4  18 160 144 173  10 214  12  90
 103 150 142 177 102 133 211  94  11 221  19  97 198 220  91 141 147 128
  49  85 166  74  41 179 117 169 163  65   0  14 197  93 115 175  87   5
  34 183 205 157 145  92 140 215]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5585
INFO voc_eval.py: 171: [34  6  3 32 26 14 12 13 17 15 10  2 43  0  1 11 40 47 31 16 52 53 62 39
  8 37 64 20 57 60 54 21 63  5 36 51 48  7 45 44 38 49 19 22 24 68 66 41
 23 35 30  4 58 55 67 46 18 33 59 61 65 29 28 27 56 42 25  9 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.5909
INFO voc_eval.py: 171: [  8  65   2  20  43 113  78  96  92  94 106  50  23  46  84   3  91  44
  62  86 110  38  32  11 104  21  76  41 101  60  87  36  77   6  48  27
  57  58  31  22  29  85  99   4  98  52 109  55  59  30  68  89  33  54
 115  53 111  45  39  16  13  74   0  72  90  15  75  28  79  81  95  83
  63  67  24  12  66  17   5 105  69  97  18  56  37  25  10  80 117  70
  71   9   7  35 112  61 108  64  34  26  19 102  51  40  73 116   1  82
 100 107  93  88  47  49 114  14 103  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5393
INFO voc_eval.py: 171: [65 17 14 59 36 26 37  7 12 34 47 49 19 31 29 39  8 45  5 44 50 42 13 60
 18  6 21 10 28 23 54  0 40 48 35  2 16  1 27 56 58  3 55 41 33 30 64 20
 62 61 25  9 43 52 22 32 11 24 46  4 57 51 15 38 53 63]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.5644
INFO voc_eval.py: 171: [56 54  9 44 76 66 10 32  3 78 16 60 71 77 42 47 12 51 35 50 59 36 31 75
 30 38 34 61 48 19 55 67 27 72 20 14 33  0 74 62 18 70 37 25 13  1 15 53
 57 80 64 29  7 69  6 79 43  2 21 81 46 17  5 24 11 40 22 58 68 52 45 73
 39 65 41 63 28 49  8 26 23  4]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7365
INFO voc_eval.py: 171: [ 4 22 11 13  8 12  5 20 17  7  3 15 10 24 16 23 18 25  9  2  0 14 21 19
  1  6]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8300
INFO voc_eval.py: 171: [  0 448 447 445 444 443 442 441 440 439 436 435 434 433 431 430 429 425
 403 404 405 407 408 409 449 410 416 419 420 421 422 423 412 450 451 452
 476 477 478 481 483 485 475 487 490 491 492 493 495 498 488 401 473 470
 455 456 457 458 459 460 471 461 463 464 465 466 468 469 462 399 397 396
 311 313 315 316 319 321 310 324 328 329 330 331 332 337 325 338 309 307
 289 290 291 293 294 295 308 296 298 300 301 302 303 305 297 499 341 347
 377 379 380 381 382 383 376 384 386 389 390 391 394 395 385 345 375 371
 348 349 353 354 355 356 374 357 361 362 364 367 368 370 360 287 500 506
 710 709 708 707 706 703 702 700 699 697 695 692 688 687 683 681 680 655
 657 658 659 660 665 711 666 668 671 672 675 676 677 667 712 713 716 753
 754 757 758 759 760 752 761 763 764 765 766 768 770 762 654 751 745 718
 719 720 724 725 726 746 730 734 735 738 739 743 744 733 652 651 650 558
 559 560 562 563 568 557 573 576 577 578 580 581 583 574 584 554 551 515
 517 522 523 533 534 552 535 538 544 547 548 549 550 537 501 585 587 616
 617 620 625 628 633 614 634 640 641 646 647 648 649 639 586 613 607 588
 589 592 593 594 597 608 598 600 601 602 604 605 606 599 286 387 284 154
 153 151 150 149 144 139 138 137 136 135 131 128 156 127 124 123 122 118
 117 116 112 111 107 106 105 104 103 126 157 158 159 200 198 197 195 194
 193 192 191 190 189 188 187 186 183 182 181 180 179 177 175 174 173 168
 167 166 164 163 161 160 102 101  96  94  37  36  34  33  32  31  28  27
  26  24  23  22  21  19  18  17  16  15  13  12  11  10   9   7   6   4
   3   2   1  38 201  39  41  89  88  87  86  85  84  80  74  73  70  69
  68  67  66  65  61  60  59  58  55  54  53  52  51  50  49  48  43  42
  40 202 775 266 229 228 227 226 252 225 223 222 221 220 219 218 217 276
 278 269 279 268 255 244 243 242 258 239 259 236 260 235 261 247 262 233
 264 256 267 216 257 211 205 249 203 204 282 283 250 210 209 372 121 363
 339 619 230 728 502 207 140  35 536 373 519 196 698 489 237 575 415 359
 689  99 693 642  47 418 747  95 472 350 513 520 521 525  82  83 176 130
 317 454  75 721  79  63 769 494 609 238 467  64 570  97 686 231 132 146
 402 611 365 453 603 541  29 133 358 108 679 482 214 414 505 388 245 528
 678 497 215 141 767 271 327 582 474 518 682 772 567 527 595 596 398  78
  81 510 323 663 512 277 234 540  20 637 511  76 622 543 115 479 292 484
 274 178 553 334   5 662 413 524 556 516 119 673 526 564 529 285 508 774
 566  93 771 171 504 246 669 351 206 624 212 320  46 312 446 571 411 740
 155 618 531 185  90 199 661  98 406 645 437 545 326 393 691 694 670 100
 113 109 272  45 714 184 629  14 684 514 148 532 152 342 773 530 438 400
  71 352 240 503 635 748 254 685 632 417 426 636 120 299 696 736 627 344
 561 623 496  44 750 322 509 690 755 732 340  57 590 612 147 306 621 569
 572 318   8 253 653 145 208 143 615 336 129 304 480 314 723 546 704 555
 346 674 232 749 110 335 741 630 626 428 224 610 729 265 643 701 424 539
 378 263 737 248 142 162 343 717 715 213 705  62 280 392  56 275 507 134
 172 273 366 656 125 170 731 486 722 369 288 251 565 631 742 169 281 241
 644 664 165  72 270  30 591 756 542 114 638 579 333  25 427  91  92  77
 727 432]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7434
INFO voc_eval.py: 171: [ 89 142  75  29  73  31  99  33  34 102  66  26 109  39 132 110 116  57
  44  45 119  54  48  38  49  51  23   1   2  84 148  93   6   7 146  94
  10  24  11  13  14  15  16  81  18  19 144  21  22  12  77 127 134 122
  32  28 143   5 124   9 131 145  36  64  86 104 126   0 151  68  78 120
  69  85  96 133  95  62 108 118 128 105  53 136   3  79 115  65  76  41
  97  58  46 100  56  50  71 114  37  82 152  83 111 140  67  90  72  17
  91 138  42  52 150  20  61  40  60 101 103 113  27 125  87  63   8 117
  43  25  70 141  74  47  88 107 121  35  80 130   4  55 147 149  30 112
 135 106 139  92 129 123  59  98 137]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5653
INFO voc_eval.py: 171: [53 42 39 44 45 66  2 38  3 69 46 17 68 50 37 40 41 51 67 55 75 65 47 25
 14 12 49 15 23  5 31 13 10 11 22 27 32 77 73 20 24  4 16 18 76 36 60  0
 63 30 72  7 70 59 58 64 48 43 26 28  6 19 56 57 61 21 29 71 33 35  9 34
  8 54 52  1 62 74]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4964
INFO voc_eval.py: 171: [26 37 11  2 64  4 27 18  3 42 39 57 52 14 41  1 10 36 12 28 20 45 46 38
 21 31 55 22 58 32 59 29 51  9 61 65 15 47 23 17 43 16 35  8  7 24 13 54
 25 30 50  6 53 60 19 49 33 40 44 48 34 62  5 66  0 63 56]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3415
INFO voc_eval.py: 171: [17 16 51 35  6 48 13  5 28  2 20 26 37 29  7  8 54 24  9 14 11 39 10 25
 32 55 27 52 41 40 18 45 21 50  4  1  0 12 53  3 31 19 33 43 22 44 49 34
 36 38 15 30 47 46 56 42 23]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5655
INFO voc_eval.py: 171: [ 0 14 13 21 26 10 28 40 29 31 34 35  3  2  1 30 17 41 24 33 32 38  6 25
 23 18 15  7 16 27 39 19 22 20  5 12 36 11 37  4  9  8]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5149
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5603
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.353
INFO cross_voc_dataset_evaluator.py: 134: 0.692
INFO cross_voc_dataset_evaluator.py: 134: 0.405
INFO cross_voc_dataset_evaluator.py: 134: 0.475
INFO cross_voc_dataset_evaluator.py: 134: 0.492
INFO cross_voc_dataset_evaluator.py: 134: 0.865
INFO cross_voc_dataset_evaluator.py: 134: 0.673
INFO cross_voc_dataset_evaluator.py: 134: 0.204
INFO cross_voc_dataset_evaluator.py: 134: 0.559
INFO cross_voc_dataset_evaluator.py: 134: 0.591
INFO cross_voc_dataset_evaluator.py: 134: 0.539
INFO cross_voc_dataset_evaluator.py: 134: 0.564
INFO cross_voc_dataset_evaluator.py: 134: 0.736
INFO cross_voc_dataset_evaluator.py: 134: 0.830
INFO cross_voc_dataset_evaluator.py: 134: 0.743
INFO cross_voc_dataset_evaluator.py: 134: 0.565
INFO cross_voc_dataset_evaluator.py: 134: 0.496
INFO cross_voc_dataset_evaluator.py: 134: 0.341
INFO cross_voc_dataset_evaluator.py: 134: 0.565
INFO cross_voc_dataset_evaluator.py: 134: 0.515
INFO cross_voc_dataset_evaluator.py: 135: 0.560
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.537s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.384s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.387s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.386s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.391s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.391s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.389s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.389s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.387s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.389s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.386s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.386s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.386s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.486s + 0.001s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.399s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.396s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.388s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.391s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.388s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.397s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.394s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.393s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.396s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.396s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.397s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.393s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.480s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.387s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.378s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.380s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.376s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.372s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.374s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.378s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.376s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.379s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.375s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.615s + 0.001s (eta: 0:01:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.393s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.385s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.377s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.377s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.379s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.378s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.377s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.372s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.370s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.370s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 60.595s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [21 37 24  6  7  9 14  8 10 46 38 51 49 31 53 33 22  4 36 54 27 34 50 44
 25 39 12 19 40  5 52 20 23 43 41 26 30 29 56 45 18 55 48  0 47 32 13  3
  2  1 42 17 15 35 16 11 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3402
INFO voc_eval.py: 171: [ 3 15  8 12 10 13  4 17  7 18  0  5 11  1 19 14  9  6 16  2]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6744
INFO voc_eval.py: 171: [ 94  36  37 142  41  45  46 100  48  50 211  59 125 116  35  65  69  70
  74 107 103  83  84 220  86  88  89  90 101  68  33  47   3  15 167 232
 162  24 169 235 174 170 183   7 195 149 161 129 189 186 115 226  71 209
 168  79   2 231   5 208  54  55 236  29  72 113 112 187 230  51  44 223
 141 135 143 201 219  75 110 140  62 106 184  73  91  39 234 132  12 185
 205 108 182 224 178 188  30  40 158 177 190 206 165 225   9 233  77 212
 118  28 216  14  49 120 146 199  92 227 127 164  64 171 154  16  97  57
  38   8  96  20 222 111   1 180  53 179 200  60 126  43 114  42 194  10
 145 122 175   0 215 151  26  13  27  78  21  66 133 193 172  95 202  32
  87 137 210 238  93 228   6 153 131  80  58 121 197 134  98  82 136 203
 176 123  99 218 214  18  81 124  23 239 138 173 196 229 192 147 117 181
 139  25  19  22  31 105  61 128 198 150 217 104 159 204  67  34 213  76
  85  11 221 237 160 119 207 152   4 148 102 144 156 157 191 109  63  17
  56  52 166 163 155 130]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4306
INFO voc_eval.py: 171: [ 53  75  79  74  32  73 115  87  37  88  71  68  12 130  64  63  45   7
  93  62  61   2   1  77  76  46  26  91  13 119   6  16  65  30  89  23
 113  31  28  90 125  92 129  21  84  47  70  67  33  95  58 128  98 102
  25  97  49 111   4  48  38  24 121  34 107  99  57   8  52  44  66  22
   5   3   9 124 103  17  51 118 108  55  69 120  36  18  29  11 110  59
  80 112  27  82  15  78 106  39  14 116 127  42  50 131 122  20  96  35
 123 117  40 109  72  54 100 101 114 126 105  43  86  94  81  19  60  56
   0  83  85  10  41 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4673
INFO voc_eval.py: 171: [ 0 31 32 36 37 38 41 95 55 58 62 63 65 66 67 69 74 77 78 79 80 82 30 29
 96 18 85 81 35  1 44 52 68  3 53 70 33 16 34 27 20 23 71 14 86 15 92 43
 42 19 25 83 24 17 84 40  8  7 12  5 10 26 73 13 64 89 88  9 93  4 21 60
 28 45 47 75 72 61 54 87 46  6 39 59 50 91 51  2 90 11 76 22 48 57 49 94
 56]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4909
INFO voc_eval.py: 171: [13 12 30 24  1  5 16  4 22  3 19 20  2  0 17  6  8 15 26 25 14 23 10 31
 21 29 11  9 27 18  7 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8015
INFO voc_eval.py: 171: [ 51 114  89 161 157 156  41 155  84 121  82  50 165 107  53  54  81  58
 131  80  65  77  67 130 122  52 167 123 170 177 175  95 112  18  19  20
 178  21 111 109  93   6   4  24 171   1 187 169 160  10 151 185 118  94
  39  56 141  70 132  33  44 108  29 180 119 137 126  74 124 149 115  78
  72  63 189   2 102  62  66  23 164  42  17  71 186 152 147   9 159   3
   7   0  97  88 142  86  34  90  79 173  64  32  55 192 184  99 154  49
  68 110  98 190   8 106 138 143  26  48 125  22  11 166  91 181  36  25
  45 101 103  38  31 133  30 113 174 162   5 182 150 116  27  15 168 105
  28 176 146  76 153 135 104 148 100  35 172 139  13  59  92  69 179  83
  43  16  61  87 191 183  60 158 144 134 140  57  40  96 136  37 127  85
  12 163  75 120 129 145  73 128 117  47  14  46 188]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6718
INFO voc_eval.py: 171: [45  2 23 50 37 17 41 49 36 21 19 47 32 48  9 33 10 39  3  8 30 35 51 28
 24  7 26 15 20 13 31 42 38 34 46 22  1  5 14  4 18 12 27 29 43 16  0 11
  6 40 44 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.2218
INFO voc_eval.py: 171: [112 102 101 100 191 192 193 194 200  83 111  77 202  73  72  71  69  63
  61  59  55  76 224 113 114 170 164 163 161 160 156 154 150 148 141 139
 184 136 186 128 126 123 122 121 187 118  50  48 169  40  37  36 212  35
  14  17  18 211 210  22  30  23   7  45 221   1  46  44  28 124 188  24
  90  49  62  70  31 129 132   2 155  10 127 140  91  86  47 120  81 138
 131  85  26   8 125  41 182  97  98 108 174 220  52  39 215  42 219  27
 165 201 151 103 205  87  65 115  67 180 110 159  25 144 185  84 204 173
 106  68  32 218  29   9 208 158  78 189  16  21 183  53   4 177 178 109
 196  34 209  54  60  74 157 116  57  80 214  66  82 133 107 134 197 195
  64 142 172  56 105 167 203 225 135 199   5  19 175 190  58 162 207  12
 166 145  93  13 104 223  96  79 222 179 216  99 143   0 152 119  33  92
 181 198 146 213  51  20  89 130  94  43   6  11  75 149 206 171  88 168
  15 176 117  38 153   3 147 217  95 137]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5436
INFO voc_eval.py: 171: [ 3 34  7 16 14 27 32 17 13 19 12  4  1 43  2 15 40 31 48 53 18 10 54 64
 39 37 66 55 58 24 65 20  8  6 36 52 61 49 45 38 44 30 22 50 23 70 46 26
 69 41 25  5 68 35 21 33 59 60 51 56 11 67 62  0 42 29  9 63 47 28 57]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.6933
INFO voc_eval.py: 171: [ 76  18  91  89  63 102 109   7  42  93  82  21  44  48   2   1  45  88
  60  37 106  30  10  84  41  74  99  25  19  47 100  75  58  29   5  54
  85  50  20  27  55  83 105  95  96   3  56  28  53  52  32  51 111  43
  14 108  36  66  70  72  87   0  38  12  26  73  13  65  61  15  79  11
  77  92  22  40  35  16  33  67 101  64   4  49  24 107  23  69  94  31
  68  81  17  78  34  71  39 113  59 104   9  98  46 112  62 103   6  86
  97  57  80  90 110   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5436
INFO voc_eval.py: 171: [14 67 36 62 17 26 37  6 12 50 34 52 19 31 39 28 47  5  7 53 23 46 18 10
 44 21 63 13 40 35 51  8 29  0 16 57 58 27 59  2 61  3 43  1 30 33 11 64
 65 45 66 25 20  9 55 32 22 49 38 56 41  4 60 48 42 54 24 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.5891
INFO voc_eval.py: 171: [52 29 50 62  8 41 73  9  3 75 56 16 74 67 39 44 47 11 32 33 31 55 28 72
 57 46 35 27 43 63 51 19 25 13 21 20 30 69  0 18 34 70 60  5 12 53 26 66
 24 49  1 77  7 22 65 58  2 76 15 78 10  4 64 54 23 40 37 71 48 36 14 17
 61 59 38 42  6 68 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7363
INFO voc_eval.py: 171: [19  3 10 12 11  4 18  7  6 15  2 14 13  9 21 16 22 20  1  8  0 17  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8430
INFO voc_eval.py: 171: [  0 423 424 425 428 429 430 422 431 433 434 436 437 438 439 432 440 421
 419 398 399 400 401 404 406 420 407 410 411 412 413 414 416 409 396 441
 445 468 469 472 474 476 478 467 479 482 483 485 486 489 490 481 444 466
 464 447 448 449 450 452 453 465 454 456 457 459 460 461 462 455 491 395
 393 316 319 320 322 323 324 314 325 330 331 332 334 337 340 326 341 311
 308 290 291 292 293 295 296 310 297 301 302 303 304 305 306 299 394 342
 347 372 373 374 375 376 377 371 378 382 385 386 387 388 390 381 346 370
 367 348 349 350 353 354 356 368 358 360 361 362 364 365 366 359 492 497
 505 680 682 684 685 686 687 677 689 693 694 695 696 697 698 690 699 676
 670 644 645 646 647 652 654 672 655 658 659 664 665 666 669 656 643 700
 705 738 740 742 743 744 745 737 746 748 749 750 751 753 755 747 703 736
 731 706 707 711 712 713 716 732 718 720 723 724 728 729 730 719 641 640
 639 549 550 552 553 558 563 548 564 567 568 570 571 573 574 565 575 547
 541 506 512 513 523 524 525 544 527 533 536 537 538 539 540 528 576 577
 578 610 615 618 623 624 628 607 629 631 634 635 636 637 638 630 606 604
 603 579 581 582 583 586 587 588 589 590 591 594 595 596 597 598 289 288
 380 286 149 148 146 145 144 139 135 134 133 132 131 128 124 123 122 120
 119 116 115 114 112 108 107 103 102 101 100  99  98 151 152 153 154 194
 192 191 190 189 188 187 186 185 184 183 182 181 180  97 177 175 173 172
 169 168 166 165 161 160 159 158 157 156 155 176 195  94  90  35  34  32
  31  30  29  27  26  25  24  23  22  20  19  18  17  16  15  13  12  11
  10   9   7   6   4   3   2   1  36  37  38  39  89  86  85  84  83  82
  80  76  71  70  68  67  66  65  91  64  59  58  57  56  53  52  51  50
  49  48  47  46  43  41  62 196 137 198 246 245 197 244 242 239 238 237
 249 234 230 228 225 223 222 221 220 219 231 250 251 252 285 284 282 281
 280 278 277 274 273 271 263 262 261 260 258 257 255 254 253 217 216 760
 214 215 213 203 199 204 205 211 212 210 510 609 493 355 363  45 508 352
  78  33 526 201 678 232 443  79 681 480 343 515 118 566 714 503 511 126
 458 484  75 653 208 599  92 708  61 351 140  63 754 127 233 675 357 313
 601 279 671 104 560  72 129 667 668 171 531 403 209 473 227  28 240 752
 584 517 463 592 442 265 321 136 379 557 488 518 572  77 392 530 502 509
 427  74 327 389 475 496 229 649 546 532 495 113 501 272 470 585 660 543
 650 627 613 759 756 268  21 514 164   5 507 170 241 556 307 757 111 287
 498  73  88 504 516 402 554 562 519 608 345 179 435 206 150 405 725 648
 408  93  44  87 200 193 521 657 315 679 110 426 633 335 534 105 417  69
 758 397 673 266  42 344 522  95 143 651 235 622 487  14 384 735 625 494
 721 701 259 248 520 446 312 247 178 612 317 117 294 391 626 683 499 451
 674 733  96 500 605 739 614 147 226 602 106 167  40 688 580 318  55 298
 617 619 329 722 202 338 717 726 333 710 561 600 138 471 300 218 551 309
 663 661 741 611 207 535 243 125 141 702 691 256 529 642 328   8 339 555
  54 236 336 545 620 269 616 121 267 559 383 704 734 369 418 275 662 142
 264 477 174 709  60 276 130 569 621  81 283 692 542 727 593 162 270 224
 715 415 163 109 632]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7407
INFO voc_eval.py: 171: [ 46  31  33  34  70  38  39 129  95  43  86  45  91  47 125  49  52  82
  55 100 117  79  64 114  73 107  29  28 109  26 144   2   6   7  10 138
  11  12  90  14  15  16  17 141  18  19 140  21  22   1  23  24   5  32
 120 122 130   9  75 139  36 128  62 142  84 103  66 124  93 132 147  67
   0  92 106  76 118  83  60 116  51   3  77 133 113  56  63  41  54  94
 126  69  96 148  37 104  80 108  81 134  48 136  65  13  87  44  40  50
 112 146  20  88  71  74 102  58  27  61  68 115  85 121  25 137  72 111
 123   4  59   8 143  53 119  42  35 145  78 105  98 135  30 110 127 101
  57  97  99  89 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5486
INFO voc_eval.py: 171: [43 44 54 40 46 66  2 39  3 69 47 51 68 17 38 41 52 67 76 42 65 48 25 14
 56 12 50 11 15  4 23 13 31 10 18 22 27 32 75  5 20 24 77 16 73 28 37 60
 63  7  0 30 72 62 59 49 70  6 45 64 57 36 29 61 55 19 71 34 26  8 35 21
 33  9  1 74 58 53]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4928
INFO voc_eval.py: 171: [39 28 12  2 67  4 19 29  3 41 45 15  1 22 55 44 49 13 40 11 30 34 23 38
 58 48 59 60 21 62 56 31 33 50 26 64 10  8 17 25  9 54 37 68 18 57 53 16
  7 14 20 32 61 35 63 47 66  0 65 46 24 36 43  5 42 27 51  6 52]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3246
INFO voc_eval.py: 171: [18 51 54 36 17  6 14 28  2 21 26 38  7  8 29 57 24  5 15 11  9 40 10 25
 33 12 58 13 27 22 42 19 55 41 48 53  1  4  0 56  3 23 35 47 46 31 59 37
 34 20 52 45 30 49 50 39 16 43 32 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5647
INFO voc_eval.py: 171: [20 25 15 27 28 12 11 10 29 34 36  4  3  2  1 30 40 23 32 41 31  7 38 22
 24 16 13 26 14  9 39  8 18 19 21  6 17 35 33 37  5  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5144
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5617
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.340
INFO cross_voc_dataset_evaluator.py: 134: 0.674
INFO cross_voc_dataset_evaluator.py: 134: 0.431
INFO cross_voc_dataset_evaluator.py: 134: 0.467
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 134: 0.802
INFO cross_voc_dataset_evaluator.py: 134: 0.672
INFO cross_voc_dataset_evaluator.py: 134: 0.222
INFO cross_voc_dataset_evaluator.py: 134: 0.544
INFO cross_voc_dataset_evaluator.py: 134: 0.693
INFO cross_voc_dataset_evaluator.py: 134: 0.544
INFO cross_voc_dataset_evaluator.py: 134: 0.589
INFO cross_voc_dataset_evaluator.py: 134: 0.736
INFO cross_voc_dataset_evaluator.py: 134: 0.843
INFO cross_voc_dataset_evaluator.py: 134: 0.741
INFO cross_voc_dataset_evaluator.py: 134: 0.549
INFO cross_voc_dataset_evaluator.py: 134: 0.493
INFO cross_voc_dataset_evaluator.py: 134: 0.325
INFO cross_voc_dataset_evaluator.py: 134: 0.565
INFO cross_voc_dataset_evaluator.py: 134: 0.514
INFO cross_voc_dataset_evaluator.py: 135: 0.562
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 3999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.511s + 0.001s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.387s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.383s + 0.002s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.381s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.383s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.385s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.387s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.388s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.388s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.389s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.389s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.391s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.390s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.533s + 0.001s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.387s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.388s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.384s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.380s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.383s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.385s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.380s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.379s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.377s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.378s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.377s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.514s + 0.001s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.394s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.387s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.394s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.386s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.381s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.381s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.381s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.382s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.382s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.383s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.382s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.382s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.467s + 0.001s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.385s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.371s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.370s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.377s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.373s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.375s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.376s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.378s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.378s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.376s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.376s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.376s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.785s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 9 39 15 23  6  7 25 10 46  8 51 38 49 33 31 53 22 54  4 36 28 26 44 34
 50 40 12 24 52 20 21 41  5 55 27 43 42 29 57 30 19 45 48 32  1  3  0 13
 17  2 47 11 35 16 37 56 14 18]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3548
INFO voc_eval.py: 171: [ 9 14  3 12 11  7  4 16 18  6  0  5 10 13  1  8  2 15 17]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.6840
INFO voc_eval.py: 171: [ 85  34  35  36 146  87  40  83  44  45  46  47  48 106 165 102 169  56
 184 230 100  98  93  63  89  66  67  68  88  72 190  32 164 113   3 207
   7 139 205  14 216  82 157 158 126 181 178  23 162 123 227   5  77 114
  69  70 163 231 232   2 204  27  52 111 222  43 138 110 182 140 137 220
 132  50  53 226 180 109  59 179 198 105 215  38 130  73  90  11  71 107
 229 201 219  49 174 183 185 154  39   8 202 173 116  75 228 208 119  29
  13 160  26 212   9 177 195 159 221 143  54 108 223 166 125  15  91  19
 218  42  96  95 224  37   1 124  20 189 196 175  76 150 142  25  12 171
  51 211  64 148   0  57 112  30  94  24 167  55 234  62 197  41  10 188
 129 134  86 206 170  21 120 118 233  92   6 168 127  97 172 121  99  78
 133 122  17  80 131 214  33 210  79 176 152  74 193  22 199 191 104 136
 194 135 187  65 147 101 213  28 149  18 103 144 200 235 209  61 225  31
 217  84 155 117 141 156 153 115 186  58  81   4 203 161  16 145  60 128
 151 192]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4502
INFO voc_eval.py: 171: [  0  43  51  59  60  61 126  66  69  71  36  72  74  75  77  85  86  89
  91 112 115  73  31  63   1   6  11  44  29  12  27  25  15 110   5  87
  62  88 121  22  21  30  24 125  90  45  82  32  57 124  65  68  96 108
 100  47  93  35  95   3  37  46 117  97   7  50  28  64 104  42 120  34
   8   4 105  56  67  16 101 107  49  53 116  58   2  76 114  23  10 109
  78  14  79  38  33 103  52  13  17  41 123  48  20 118 119 127  26  94
  18  98 113  99  92  81 111  70  80 122  84  39  54 102   9 106  19  83
  40  55]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4884
INFO voc_eval.py: 171: [ 0 30 32 35 36 37 41 90 52 54 57 29 58 61 65 66 69 72 73 74 75 77 80 60
 28 91 17 76 34  1 43 31 49 63 62  2 15 50 33 19 27 22 64 81 13 14 42 78
 87 18 25 16 39 24  7 40 11  4 68  9  6 79 26 59  8 12 85 46 70 88 56 38
 82 20 44  5 55 67 71  3 51 48 83 47 10 45 21 89 53 23 84 86]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4909
INFO voc_eval.py: 171: [13 12 30 24  1 16  5 22  4  3 17 19 20  2  8  6 15 14 10 26 25 21 31 23
  0 27 29 11  9 18  7 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8152
INFO voc_eval.py: 171: [ 93 157 156  40 153 152 151  50  51  52  53  56  64  65 128 127  77  80
  81  82  84 122 121  89 120  95 114 112 111 106 161 163  54 109  18  20
  21 174 173  24 183  11 171 167 166   5 165   2   7  19 117 181 148  57
 129 138  94  70  41  46 107  33 118  74  29 123 176 134 125 184  62 101
  72  79   3 115 160  43  61  17  71 147  66 146 144   1  16  10 155 182
  88  96   4  49   8 139  85  97 170  31 110 105  78  63  98  34  55  90
 185 187 150  67  26 135   9 177 140 180 124  22  48  12  91 162  25  32
  36 113 158 178 100 164 102  99  30  28 169  68 104 130  27 116  14 103
 172 179  39  44  87   6  92 143 145  59  13 132 175 149  83  60 154  76
  23 136 168  15 119 133  37 186  35  75 141  42  58 126 137 131 159  86
  38 108  45 142  47  73  69   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6723
INFO voc_eval.py: 171: [ 2 46 24 51 39 18 48 34 38 50 42 20 22 49  9 41 10 37 30 35 32 26  3 52
  8 16 28 21 43 23 15 33 40  5  7 36  4 14  1 17 12 29  0 13  6 19 11 31
 47 45 27 44 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1731
INFO voc_eval.py: 171: [113 130 129 128 125 124 123 122 120 116 115 114 104 103 102  92  84  79
  77  76  74  73  72  71  64  63 131 134 138 140 223 214 213 212 204 202
 198 196 195 194 190 189  62 184 171 166 165 163 162 157 156 155 151 149
 142 141 172  60 227   7  17  38  29  41  18  14  31   2  45  37  46  47
  24  49  10  36  51  56  23   1  32 191  42 139  50  27 218  88  25  82
  48 222 126   8 127 185  99 176  94 100 133 110  40  53  43 105  85  28
 168 221  54  89 180 203  69 207 152  67 160 117  33 112  86 107 183 188
   9  70 145 175  26  30 118  61 186  78 220 210  16 206 199 192 179  22
  55 111  58  81  65 159 216  35 158   4 211 135  66  83  12 143 136 200
 205 169  75 174 226 197 109  57   5 146 177  20 201 137  98 193  59 147
 209 121 108  97  93 164  21   0 106 153  13  80 225 217   6 144 150 181
 224 182 101  34   3 215  11  52  44 170 167  39  87  90  95  96  19  91
 132 208  15 173 154  68 161 148 187 178 219 119]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5326
INFO voc_eval.py: 171: [36 34 28  3 16 15  7 17 13 18 20 46  4  2  1 14 50 42 33 19 56 65 10 57
 69 41 39 60 24 58 21 66 63  8  6 52 38 55 47 45 40 32 23 26 70 48 53 62
 43 71 61 27 25 22 29 35 31 68  5 12 11  0 37 44  9 54 51 59 49 67 30 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.6679
INFO voc_eval.py: 171: [ 64  93  91  89 101  77  19  43 110   2   7  22  45  49  83   1  46  88
  61 106  38  31  11  85  75  97  26  48  42  21  55  76  20  86  30  98
  59  56  28   5 104  51  84  53   3  95  54  96  52  29  57 112  44  39
  37 107  14  33  71  73  67  15  87  74   0  16  92  10  13  62  27  23
  65  80  36  66  17 108  25  78  41  60  12  34  68   4   8  35  18  99
  24  32 114  47  70 103  72  40 113  50  69  58 102  82   6  79   9 105
  94  90  63 111 100  81 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5471
INFO voc_eval.py: 171: [71 64 16 14 34 24 35  6 50 12 32 54 18 29 38 26 55 47  5  7 22 43 17 10
 46 20 52 65 33  8 27  0 60 15 13 25 59 61  2 63  3 28 31 42  1 39 45 68
 66  9 70 11 40 19 30 21 57 37 49 53 67 48 23 58 41 62 56 36 51 69  4 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6285
INFO voc_eval.py: 171: [50 62 75 40 73 29  7  8 52 56 67 15 74  3 44 38 47 10 33 32 57 69 28 63
 55 43 72 30 35 46 27 25 19 51 20 12  0 21 34 70 49 53 60 66 18 65 11 58
  1 31 26 78  5 22  2  4  6 76 24  9 42 64 14 36 17 77 23 39 54 45 48 61
 68 41 37 16 71 59 13]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7638
INFO voc_eval.py: 171: [18  3 10 11 12  4  7 17  6 15  2 14  9 16 20 21  8 19  0  1 13  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8430
INFO voc_eval.py: 171: [  0 427 428 429 431 432 433 426 434 436 438 439 440 442 443 435 444 425
 423 403 404 405 406 407 408 424 409 414 415 417 418 419 420 412 401 445
 448 475 476 477 478 479 481 474 482 486 488 489 493 500 502 485 447 472
 468 449 450 451 452 454 455 470 456 458 459 461 462 464 465 457 400 398
 395 324 325 326 328 333 334 320 335 340 341 342 343 345 346 336 347 319
 317 294 296 297 298 299 300 318 302 305 308 310 313 314 316 304 348 349
 350 375 376 378 380 381 382 374 384 388 389 390 392 393 394 387 372 371
 370 353 354 355 356 358 359 360 361 362 364 365 366 367 368 369 504 506
 507 509 678 680 682 683 684 685 675 686 690 691 692 693 694 695 687 696
 674 667 642 643 644 645 650 652 669 653 656 657 661 662 663 666 654 697
 700 702 736 738 739 740 741 742 735 743 745 746 747 748 751 752 744 734
 730 729 703 704 708 709 710 712 714 716 717 718 721 722 726 727 728 641
 293 640 637 547 549 550 554 559 560 546 561 564 566 567 569 570 571 563
 572 545 540 510 520 521 522 523 524 544 525 533 534 535 536 537 538 530
 573 574 575 606 612 615 620 621 625 605 626 628 632 633 634 635 636 627
 603 602 600 577 578 579 582 583 584 585 586 587 590 591 592 593 594 599
 638 291 758 289 158 157 156 155 154 153 151 150 149  22 147  23 145 159
 144 142 138 290  24 135  25 133 132 131 130 129  26 126 143 161 162 163
 196 194 193 192 191 190 189 187 186 185 184 183 182 181  18 180 179 178
 177 176 175 172 171 170 169  19 167  20 166  27 123 121  29  83  82  39
  80  79  40  76  75  42  71  69  68  67  66  65  64  63  60  45  59  58
  46  56  47  48  53  52  51  50  84 199  38  36  30 120  31 118 117 116
 114 112 111 110 107 106 102 101 100  99  98  97  96  94  32  91  90  89
  33  34  86  35  85  37 200 152  49 226 276 237 224 278 239 240 241  11
   9 244 279 280 221 267 275 246   7   6  12   2   1 233 271  13  10 272
   3 230 274 234 235   4 265 227 219 245 258 284  16 248 255 207 285 206
 205 286 249 250 287 253 251  17 218 209 208 215 257 256 247 217 282 283
 210 268  15 212 211 214 125 511 337 705 140 352 344 351 357 562 197 679
 203  78 651 480 676 397 487 753 453 228 664 124 104 528 673 164 595 229
 508 556  61  93  62 665  72 597 307 469 749 236 553 580 273 223 204 225
 127 437 373 517 315 513 668 568 260  28 460 386 134  77 588 173 466 499
 312 492 527 658 498 552 542 321 757  74 383 471 491 608 505 529 301 647
 543 430 238 648 165 581 113  73 266 514  21 263   5 281 624  88 109 512
 399 160 551  70 754 501 195 338 503 604 174 402 309 496  44 518 201 148
 188 671 655 516 755  87 723 484 108 329 646 531 396 557 541 677  43 261
 631 421 103 519 733 515  95 391 422 141 756  14 339 254 231 619 622 243
 611 288 483 385 719 609 323 601 707 242 490 497 681 732 441 306 672 737
 105 698 115 198 410 623 295  55  92 720 222 576 598 725 715 303  41 379
 146 311 614 327 699 332 216 639 119 596 270 660 292 467 558 616 607 136
 330 659 363 202 413 122 269 688 532 526 262 548 613 213 555 137 252 494
 731 331 168 610 411 629   8  54 277 264 322 701 139 377 706 565 259 649
 617 711 713 446 232  81 724 630 689 473 495 750 128 463 539 618 220 670
 416 589  57]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7338
INFO voc_eval.py: 171: [ 90  31  32  33  34  38  39 131  83  43  44  92  46  87  47  49  97  53
  55  80 101 119 117  64  74 110 109 127  29  71 140  10   6  11  28  12
  13  14  16  17  18   7 144 142  21  22 146   2  23  24   1  26  19  76
  63   5 133 122 124 143 130  85  36 141  78   9 126 105  67 150  68  93
   0 120 108  94  77 132 102  84 118  61  58  51  98  41   3  65  56 128
 135 151  95  70  40 111 136  48 106  37  81 137  15  88  66  60 114  82
  20  72  45  75 116 148  89 104  59  69  42  62 123 121  86   8 112  73
  25  27 107 139 145 113  50   4 115 100  79  54  99 129  35 103 147 125
 149  57  96 138  91  52  30 134]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5525
INFO voc_eval.py: 171: [53 65 42 44 45 46 40  2 47  3 67 18 51 68 39 43 41 52 15 48 64 74 66 56
 25 13 50 12  5 16 19 24 14 32 11 28  4 33 21 76 22 75 26 38 17 29 72 59
 62 61  7 58 31  0  6 71 37 49  8 55 63 69 30 70 60 57 36 20 54 23 35 27
 10  1  9 34 77 73]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4908
INFO voc_eval.py: 171: [39 67 29 12  1 19  4 30  3 41 45 53 15 22 44  2 49 11 63 13 40 31 48 59
 21 38 24 35 58 57 34 32 20  8 17 55  9 26 33 10 68 46 18 28 56 52  7 47
 23 16 25 37 50 36 60 61 54  5  0 14 27  6 62 64 51 66 42 43 65]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3291
INFO voc_eval.py: 171: [36 52 55  7 17 16 13 28  2 21 26 39 29  6  8 24 58  5 11 14 10  9 41 25
 33 18 27 43 56 49 42 22 57 54  4  1  3  0 48 19 23 35 47 15 40 34 31 46
 53 37 50 59 30 45 32 38 51 20 44 12]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5606
INFO voc_eval.py: 171: [29 15 28 42 12 11 10 20 36 38 26  4  3  2  1 30 32 23 43 34  7 33 40 22
 24 16 14 13  9 27 21 41  8 19 18  6 17 35 39 37  5 25  0 31]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5149
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5647
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.355
INFO cross_voc_dataset_evaluator.py: 134: 0.684
INFO cross_voc_dataset_evaluator.py: 134: 0.450
INFO cross_voc_dataset_evaluator.py: 134: 0.488
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 134: 0.815
INFO cross_voc_dataset_evaluator.py: 134: 0.672
INFO cross_voc_dataset_evaluator.py: 134: 0.173
INFO cross_voc_dataset_evaluator.py: 134: 0.533
INFO cross_voc_dataset_evaluator.py: 134: 0.668
INFO cross_voc_dataset_evaluator.py: 134: 0.547
INFO cross_voc_dataset_evaluator.py: 134: 0.628
INFO cross_voc_dataset_evaluator.py: 134: 0.764
INFO cross_voc_dataset_evaluator.py: 134: 0.843
INFO cross_voc_dataset_evaluator.py: 134: 0.734
INFO cross_voc_dataset_evaluator.py: 134: 0.552
INFO cross_voc_dataset_evaluator.py: 134: 0.491
INFO cross_voc_dataset_evaluator.py: 134: 0.329
INFO cross_voc_dataset_evaluator.py: 134: 0.561
INFO cross_voc_dataset_evaluator.py: 134: 0.515
INFO cross_voc_dataset_evaluator.py: 135: 0.565
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.477s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.367s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.376s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.370s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.376s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.372s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.373s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.373s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.373s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.376s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.380s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.572s + 0.002s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.402s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.389s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.382s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.373s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.378s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.384s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.379s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.383s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.381s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.381s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.381s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.379s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.493s + 0.001s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.384s + 0.001s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.387s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.386s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.385s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.379s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.379s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.380s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.380s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.382s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.383s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.384s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.382s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.577s + 0.001s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.408s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.389s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.387s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.390s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.392s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.388s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.385s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.385s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.386s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.383s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.381s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.379s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.817s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 9 14 25  8  6 22 39  5  7 46 51 38 49 33 31 53 23 54  3 36 28 26 50 44
 34 40 24 11 52 20  4 21 55 41 43 27 42 29 57 30 48 19 45 32  0  2 16 47
 12  1 10 15 35 18 17 37 56 13]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3565
INFO voc_eval.py: 171: [15  3 13 12  7  9  4 17 19  6  0  5 11 14  1  8  2 16 18 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.7431
INFO voc_eval.py: 171: [ 36  87  35  37 169  85  41 165 164  45  46  47  48  49  33 226  82 162
 215  57 158 229 139 157  64 146  67  68  69  83  88  73 113 205 122 126
 184  98  93 178 190 181 207   7   2 106 100 102  89  24  15 230  78   3
  70  71 111   5 163 114 231  28 204  53 221  44 182 138 180 137 110 140
 132 219  54  51 225 108  60 198  74 179 105 214  39 130  90  12  72 107
 228 201 218 183  50 174  29 185 154  40  76 173 202 119 227   8 116 208
  14  27 211 160 220 177   9 195 159 222 143 109  63  55 166 125  16 223
 217  91  20  43 196 189  96   1 124 175  21 150  38  26 171 142  95  77
  52 112 148  13 120  65 210  56  58   0 233 167  25  31  94  10 188 134
 129  42 197 118  22 170 232 206 123  97 172 168 127  92   6  99  81  86
  79  18 133 209 213  34 121 152 131 193 176  23  75  80 199 194 135 136
 191 101 104  66 187 147  30 212  11  19 103 144 149 200 234  32 224 216
 155  62  84 156 115 153 117 141 203  59 186   4 161 192 151  61 128 145
  17]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4282
INFO voc_eval.py: 171: [  0  38  45  46  53  61  63 130  68  71  74  75  76  77  78  80  88  89
  92  94 116 119  33  27  65  11   1   6  12  29  15  31  62  23  90   5
 114  91  64 125  22  32 129  26  93  47  85  34  59  70  96 128  67  99
 112 103  37  49  98   3  39 121  48   7 100  30  58  52  66 107  44   8
  18 124   4 111 108 104  69  16  51 118  55 120  81  60   2 113  79  10
  24  35  40  14  82  36  13  43  28  50 127 106  21 123 122 131  97 101
 102  19 117  84  95  54  83  72 110 115  41  25  86 126  56  87 105  42
   9  73  57  17  20 109]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4809
INFO voc_eval.py: 171: [ 0 32 34 37 38 39 41 92 55 57 60 31 61 64 68 69 72 75 76 77 78 80 83 63
 30 93 19 46 79  2 36 52 33  3 66 65 16 53 35 21 29 24 67 84 14 15 45 81
 26 27 89 20 17 42 43  8 12  5 71 28 10 82  7 62  9 13 88 90 49 40 59 73
 22 47 85 58 74  6 70 54 51 86 11  4 48 23 50 87 25 44 56 18  1 91]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4891
INFO voc_eval.py: 171: [13 12 30 24  1 16  5 22  4  3 17 19 20  2  6  7 10 15 26 14 25 21 31 23
  0 27 29 11  9 18  8 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.8152
INFO voc_eval.py: 171: [ 54 171 170 168  83 166  82  79 127 131 132 161 157 156  67 155  43  66
  59  55 109  53 172  52  25 125   1 112 115 116   5  97   7 119 188  95
  84  11  91 124 179 178  86  18  19  20  21 176  51 133 122  96 152 163
 186  41  72  57 143  47 110  76  34 123  30 128 181 138 126 189  64  74
 104  81   3  68 120  44 165  73  63  17   2 150 151  23 148  10 159  98
  90   4 187  87   8  50 144  99 113 174  32 108  80  92 154  65 100  35
  56 192  27 190  69 139   9 182 145 185 129  49  22  33  12  26  93 167
 162  37 118 183 169 102 105  31  70 106  28 175 107  29  15 121 177 134
 184 146  40  45 147   6  48  89  60  94 153  13 136 149 180  85  61  78
 158  16 173 137  38  24  62 191 103  36 140 141  77 114  42  46  58 135
  39 164  88 130 111 101   0  75 117  71 160 142  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6759
INFO voc_eval.py: 171: [ 2 45 24 50 38 37 18 49 34 47 41 20 22 48  9 10 26 35 32 40 51  3  8 28
 16 30 14 21 42 23 15 46 33 39 17 36  7  5  4 19 29  1  0  6 13 12 31 11
 27 44 25 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1930
INFO voc_eval.py: 171: [113 130 129 128 125 124 123 122 120 116 115 114 104 103 102  92  84  78
  77  76  74  73  72  71  64  63 132 134 138 140 222 214 213 212 204 202
 197 196 195 194 190 189  62 184 171 166 165 163 162 157 156 155 151 149
 142 141 172  60 226  25  38  37  36  31  29   2  45  46   7  24  47  49
  23  41  17  56  10  14  51   1  18  42 126  32 218  88 139  50  27 221
 191  48  86   8 176 185 127 100  99  94  82 133 110  53  40  43  28 105
  54 168 220  89 203 207 160  69 152  66 112  33 117  85 108 179 183 188
  70 145   9 175  26  30 118  61  80 186 158 206 210 192 219  16 180 199
  22 111  58  55 216 159  65   4  35 211 135  83 200 143  67 136  81 205
 198 169 109  75 174 225  12  57   5 177  20 201 146  98 121 164 209 193
 137  59  97 147  93 107 153 106   0 217  13  79 224   6 144 150 182 181
  52  44  34 101  21 223 215   3  39  90  87 167  95  96  19 170  91 131
 187 208  11 173 154  15  68 161 148 119 178]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5308
INFO voc_eval.py: 171: [34 15  3 28 36  7 16 13 18 17 20  4 46  2  1 14 50 42 33 19 56 10 57 66
 69 41 39 60 58 24 21 67 63  8  6 52 38 47 55 45 40 32 70 48 26 62 53 71
 43 61 22 27 23 25 29 35 68 31  5 11 12  0 37 44 65 59 49  9 51 30 54 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.6668
INFO voc_eval.py: 171: [ 64  93  91  89 101  19  43  77 110  49   1   7  45  83   2  22  46  61
  88  38 106  31  11  85  75  96  26  48  42  21  51  55  76  30  20  86
  98  59  56  28 104   5  84  53  94  95  54   3  52  29  57 112  44  39
 107  37  14  71  33  67  15  73  87   0  74  62  13  27  16  23  65  10
  92  80  36  17  12 108  34  66  25  97  78   4  60  41  68   8  35  18
  99  72  24 114  47 103  32  81  40 102  70 113  50  69  82  58  79   6
 111 100  63 109  90   9 105]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5415
INFO voc_eval.py: 171: [71 64 18 15 36 37 26  7 50 13 34 54 20 31 39 55 28 47  6  8 24 43 19 11
 46 52 22 65  9 35 29  0 17 60 59 14  2 61 63  4 27 33 30 42 40  1 45 68
 41 66 10 12 21 70 32 23 57 48 53 49  3 58 25 67 56 62 69 51 38  5 44 16]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6384
INFO voc_eval.py: 171: [51 76 74 53  8  9 41 63 30  3 16 68 57 45 39 75 48 11 34 33 58 32 29 56
 64 44 73 36 28 26 47 20 52 21 70 13  0 50  1 22 71 35 66 67 54 61 19 12
 59 27 79 31  5 23  2 43  6  4 77 10 65 37 25 15 78 24 40 55 46 17 49 69
 62 42 18 14 38 60  7 72]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7489
INFO voc_eval.py: 171: [19  3 10 11 12  4  7 18  6 15  2 14  9 16 21 22  8 20  0  1 13  5 17]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8430
INFO voc_eval.py: 171: [  0 429 430 431 432 433 434 428 437 439 440 441 442 443 445 438 446 425
 423 403 405 406 408 409 410 424 411 413 414 417 418 420 422 412 447 449
 450 478 480 481 482 483 484 476 485 488 491 492 494 495 499 487 474 471
 470 451 453 454 455 456 457 458 460 461 462 463 464 466 467 469 400 508
 399 397 324 326 330 331 332 333 323 336 340 341 345 346 347 348 339 349
 322 319 296 297 300 301 302 303 321 304 307 309 310 313 315 318 305 350
 351 352 376 377 379 380 381 384 375 385 387 388 392 393 394 395 386 374
 373 372 353 354 356 358 359 360 361 363 364 365 366 367 369 370 371 398
 510 513 515 684 686 688 689 690 691 681 692 696 697 698 699 700 701 693
 702 680 673 649 650 651 656 657 658 675 659 662 663 667 668 669 672 660
 703 706 708 744 746 747 748 749 750 743 751 753 754 755 756 759 760 752
 742 737 736 709 710 714 715 716 718 720 722 723 724 727 728 732 733 734
 648 647 645 644 553 554 559 565 566 567 551 569 572 573 575 576 577 578
 570 550 549 548 516 526 527 528 529 530 531 537 538 539 540 541 542 543
 546 579 295 580 583 617 620 625 626 631 632 612 633 638 639 640 641 642
 643 634 611 609 608 584 585 588 589 590 591 592 593 596 597 598 599 600
 605 606 581 294 383 766 186 185 184 183  30 182 181 180  31 179  99 187
 176 174  32 172 171  33 169 100  34 166 165 164 175  67 188 190 213 212
 211 210 209  23  24  25  26 204  27 189 203 200  95 198 197 196 195 194
 193  98  54 191  49 214  35 101 145 144  53 117  52 119 120  42 140 292
 137 116  51 135 134 133 132 131  46  47  50 123 127  48 122  36 146 114
 102 161  37 160 159 158 157 156 155 154 153 147 103 152 151 149 108  38
  39  40 109  41 112 113 104 215  28 216   7   8  81  10  65  83  11  12
  13  14 263  80 262 261 260  84 258  85 256 255 254 253 252 251  63 250
  66   4 291 290 289 288  68  69 287  70 285 284 283 270  72 280   1 278
  76 277   2  77 276   3 273 272 281 249 124  16 233 232 231  19  60 229
  20 225  59  90  21 223 222 221  91  92 219 218  57 235  86  87 244 240
  17 247 239 242 245  61  18 238 682  79 511 685 517 506 514 142 711 493
 486 402 342 128 362 568 355 207 761 601 670 357 671 126 534 562 168  64
 679 201 475 106 459 603 312  94 234  62  73 241 757 279 586 558 230 129
 208 228 523 444 674 320 519 378  29 574 465 265 136 391 472 505 317 533
 594 545 504 765 557 389 664 327 497 613 512  75 477  45 306 653 535  22
 167  74 654 435 243 547 268 115 520 271 286 587 404   5 498 629 111 518
 762  89 343  78 555 509 199 507 162 610 314 661 205 524 192 763 178 150
 502 407  71 536 522 677 652  88 729 490 334 110 401 683  44 525 266 563
 105 426 396  97 764 427 521 740 143  15 637 344 259 390 607 627 614 624
 236 616 489 496 226 293 248 299 704 246 725 503 415 678 713 448 107 311
 177 739 118 745 721 582 687 202 628 308  56 329 735  93 227 148 726 731
  43 338 619 121 604 316 705 621 298 325 220 602 560 666   6 564 473 274
 665 544 335 138 267 125 206 217   9 368 500 618 635 646 694 139 552 532
 337 275 257 419 556 416 738 382 269 615  55 170 712 571 655 264 707 141
 328 622 237 282 623 717 719 636 436  82 452 741 501 758 479 173  96 224
 595 730 468 695 561 676 421 130 163  58 630]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7349
INFO voc_eval.py: 171: [ 38  32  33 134  75  39 130  44  46  47  48 122  50  31 120 112  54  55
  72 103  99  81  84  94  93  65  89 113  29  34  16  11  12 144  14  15
  28  17  18  19   7 143  10   6 146  21  22  23 148  24   2 142  26   1
  64   5 145 135   9 125 127 133  36  86  77  79 129  68 107 152   0 137
  69  95 123  96 111  76  85 104 121  62  59  52 100   3  42  66 131 136
  57  97 114  41 153  71  49  37 108 139 138  82  90  67  61  13 117  83
  20  73  78  45  91 150 119 106  60  43  70  63 124  88   4 115  27  74
 118 110   8  25 141  51 147 116 126 102  80  56 140 101 149 151  98 128
  58 132 105 109  35  40  30  87  92  53]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5519
INFO voc_eval.py: 171: [53 46 45 44 66 42 40  2 47  3 68 18 51 69 39 43 41 52 14 15 65 75 48 67
 25 57 50 12  5 16 19 24 32 23 11 13 28  4 33 77 21 76 38 26 73 17 60 29
 62 59 63  7 31  0 37 72  6 49 56 70 64 30 71 61 20 58 22 55 35 36 27  1
 10  9 34  8 78 74 54]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.4946
INFO voc_eval.py: 171: [39 29 66 12  1  4 19 30  3 44 41 52 15 22 43  2 48 13 11 40 63 21 47 31
 58 38 35 24 56 34 57 32  9 20 17 54  8 49 26 33 10 67 18 45 28 55 51 25
  7 46 16 37 23 53 59 36 60  5  0 14 62 50 61 42 65  6 27 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3210
INFO voc_eval.py: 171: [17 51 54 16  7 35 13 21  2 28 38 26 29  6  8 24 57  5 12 14 10 11 40 25
 32 18 27 22 42 48 41 55 56 53  4  3  0  1 19 47 23 34 58 46 39 15 33 45
 31 36 49 52 44 59 30 50 37 43 20  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5676
INFO voc_eval.py: 171: [42 26 30 41 20 31 15 29 12 11 28 35  1 37 10  4  3  2 33 23  7 32 24 22
 16 39 14 13  9 21 40 19  8 18 17  6 27 34 38 36  5 25  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5222
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5672
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.356
INFO cross_voc_dataset_evaluator.py: 134: 0.743
INFO cross_voc_dataset_evaluator.py: 134: 0.428
INFO cross_voc_dataset_evaluator.py: 134: 0.481
INFO cross_voc_dataset_evaluator.py: 134: 0.489
INFO cross_voc_dataset_evaluator.py: 134: 0.815
INFO cross_voc_dataset_evaluator.py: 134: 0.676
INFO cross_voc_dataset_evaluator.py: 134: 0.193
INFO cross_voc_dataset_evaluator.py: 134: 0.531
INFO cross_voc_dataset_evaluator.py: 134: 0.667
INFO cross_voc_dataset_evaluator.py: 134: 0.541
INFO cross_voc_dataset_evaluator.py: 134: 0.638
INFO cross_voc_dataset_evaluator.py: 134: 0.749
INFO cross_voc_dataset_evaluator.py: 134: 0.843
INFO cross_voc_dataset_evaluator.py: 134: 0.735
INFO cross_voc_dataset_evaluator.py: 134: 0.552
INFO cross_voc_dataset_evaluator.py: 134: 0.495
INFO cross_voc_dataset_evaluator.py: 134: 0.321
INFO cross_voc_dataset_evaluator.py: 134: 0.568
INFO cross_voc_dataset_evaluator.py: 134: 0.522
INFO cross_voc_dataset_evaluator.py: 135: 0.567
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 4999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.383s + 0.001s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.361s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.355s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.362s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.369s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.377s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.377s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.378s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.380s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.382s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.381s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.384s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.386s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.439s + 0.001s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.411s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.405s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.396s + 0.001s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.396s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.400s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.404s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.400s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.395s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.398s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.397s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.399s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.395s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.428s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.378s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.380s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.393s + 0.002s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.391s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.389s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.390s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.392s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.387s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.387s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.387s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.387s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.388s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.548s + 0.002s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.394s + 0.002s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.384s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.374s + 0.001s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.370s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.372s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.379s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.379s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.380s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.377s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.375s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 57.632s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [10 27 16 41 11  8  7 24  9 49 54 40 52 33 35 25 56 57  5 38 30 28 47 53
 36 26 42 13 55 22 58  6 23 43 46 29 44 60 31 32 51 21  0 48 34  3 18  1
 45 50 14 12  2 17 37 20 19  4 39 15 59]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3582
INFO voc_eval.py: 171: [15  3 13 12  7  9  4 17 19  6  0  5 11  1 14  8  2 16 18 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.7431
INFO voc_eval.py: 171: [157  67  64 181 214  57 122  68 184  48  47  46  45 178  41  50  37  69
 206 102 189 100 106  98  93  73 113  88  87  85  83  82 204  89  36 126
 164 139   7 225 169 162 163  35  15  24 165 146   2 158  33 228 114 203
  78 229  71  70 111   3 230 220   5  28  53 138 110 218  44 180 182 140
 132  54  51 224 108 137 196  74  60 105 179 213  39  90  12 128  72 227
 107 183 200 217  49 174  29 154 185  40 119  76 173 201   8 226 116  27
 210  14 207 177 160 219   9 159 194 109 221  63 143  55 166 222 125 216
  16  91  43  20 195 190  96  95 124 175   1  26  21  38 171 150  77 142
  52 120 112  56 148  65  13 209 232  58  10 167   0  31  25  94 134 188
 130  42 118 197 176  22 170 231 172  97 205 123 168   6  92 127 192  99
  86  81 208  80  79  18 212 121 152  75 131 133  23 223  34 136 135 101
 193 198  30 211 104  66 187 147  19  11 149 103 145 144 199 233 215 155
  62  84  32 156 186 115 202 117  59 153 141 161   4 191  61 151 129  17]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4460
INFO voc_eval.py: 171: [  0  38  46  47  54  62  63  64 132  69  72  33  75  77  78  79  81  90
  91  94  96 118 121  76  27  66  11   1   6  29  31  12  23 116  92   5
  93  65 127  22  15  32 131  26  86  48  95  34  60  98  71 130  68 101
 114 106  37  50 100   3  39 123  49   7 102  30  59  53  67 110   8  45
   4 113 126 107 111  52  70  16 120  56  82 122  61   2  80 115  10  24
  35  40  28  14  83  36  17  44  13  51 129 109 125 124 133  21 103  99
 104  97 119  19  55  85  41  73 112  84 117  25  87 108 128  89  57   9
  74  58  42 105  18  43  88  20]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4959
INFO voc_eval.py: 171: [ 0 32 34 37 38 39 41 92 55 57 60 61 63 64 67 69 71 74 75 76 77 78 79 83
 31 30 93 18 46  1 36 66 33 52  2 65 15 35 53 20 29 23 68 84 14 45 13 80
 27 19 89 16 42 43 11 25  7  4 28  9 81  6 62 12  8 88 90 59 40 49 72 21
 47 85 58 73  5 91 70 54 86 51 10 48  3 22 87 50 24 17 44 56 82 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4889
INFO voc_eval.py: 171: [13 12 24 30  1 16  5 22  4  3 19 17 20  2  6  7 10 26 15 14 25 21 31  0
 23 27 29 11  9 18  8 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.7951
INFO voc_eval.py: 171: [ 93 156 152  42 151 150  50  51  52  53 107  58  65  66 128 127  77  80
  81  82 124  84 122  89 121  95 116 113 112 109 161 163  54  11  18  20
 173  17 171  24 174 183   7  19 166   5 165   1 167 181 147 129 119  94
 158 138  40  56  70 108  74 120  46  33  29 125 184 176 134 123  63 102
  72  79   3  67 117  43  71 160   2  62  16 145 146  22 143  10 154  88
  96 182  85  49   4   8  97 139 110 169  31 106  78  64  90 149  98  34
  26 187  55 185  68 135   9 180 140 126  48  21  32  25  12  91 157 162
  36 115 178 100 164 177  30 103 170  69 104  27  14 105 172  28 118 130
 179 141   6  44  39  47 142  92  87  59 148 132  13 175  83 144  60  76
  15 153 168  23 133  37  61 101 186 111  75  45 136  35 137 131  86  57
  41 114  38 159  99   0  73 155]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6792
INFO voc_eval.py: 171: [ 2 45 24 50 38 37 18 49 34 47 41 20 22 48  9 10 26 35 32 40 51  3  8 28
 30 16 14 21 42 23 15 46 33 39  7 17  4 36  5  6 29 19  0 13 12  1 31 44
 27 43 11 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1923
INFO voc_eval.py: 171: [112 128 127 126 123 122 121 120 118 115 114 113 103 102 101  91  83  77
  76  75  73  72  71  70  63  62 129 132 136 138 219 212 211 210 202 200
 195 194 193 192 188 187  61 182 169 164 163 161 160 155 154 153 149 147
 140 139 170  59 224   7  23  37  36  35   2  48  17  14  50  46  24  30
   1  18  28  10  22  55  44  45  40 189  89  41   8 216  47  49 124  31
 137 220  26  85  99 125 183  98 174  81  93 131 109  52  42  39  27  53
 104 218 166  87 205 201 158 150  65  68  32 107 111 116  84 186  69 181
 143   9 173  25 117  60  29  79 204 190 197 208 184 217 178  16  57  21
 177 110  54  64 214 157   4  34 156 209 133  82  66 141 198 134  80 203
 196 167 108 172  74 223  12  56 175   5  19 199 119  97 144 162  96 207
 191  92 135  58 145   0 151 106 215 105  13 222  78   6 148 142 180  43
  51  20 179 221  88  33 100  38   3 213 165  94 185  95 168  86  90 130
 171 206  11 152 146 159  67  15 176]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5319
INFO voc_eval.py: 171: [34  3 36  7 28 16 15 18 17 13 20  4 46  2  1 14 50 42 33 19 56 10 57 64
 67 41 39 60 58 25 21 65 63  8  6 52 38 55 47 40 45 32 48 68 24 62 53 43
 61 22 69 27 23 29 26 35  5 66 11 31 12 37  9 49  0 44 59 51 30 54]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.6668
INFO voc_eval.py: 171: [ 64  77  42  91  93  21  18 102  89   7   1 111  49  44   2  82  45  61
  88  37 107  30  10  84  75  25  47  97  41  20  51  76  55  29  19  85
  59  99  56  27 105   5  83  53  94  54  95   3  52  28  57  43 113  38
 108  36  13  71  32  80  67  14  73  87   0  74  62  12  26  15   9  65
  92  22  35  16  33 109  66  24  98   4  78  60  11  40  68  34  17  72
 100 104  46  23  39  31 103  58  70 114  69  50  86  81  79   6 101 112
  63  90 110 106   8  96  48]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5417
INFO voc_eval.py: 171: [70 63 18 15 36 37 26  7 13 50 34 53 20 31 39 54 28 47  6  8 24 43 19 46
 11 51 22 64  9 35 29  0 17 59 58 14 60  2 62  4 27 33 30 42 45 41  1 40
 67 65 12 10 21 32 69 23 56 66 48 52 49 57  3  5 55 68 25 61 38 16 44]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6420
INFO voc_eval.py: 171: [63 75 74 53  8  9 31 51 41 77  3 17 68 57 39 45 48 11 35 34 58 70 33 27
 30 56 64 73 44 37 29 47 16 21 52 22 13  0 50 23  1 36 71 66 67 54 61 20
 12 59 28 32  5 24 79  2 43  6 76  4 10 65 26 38 78 15 40 25 55 46 18 49
 69 62 42 60 19 14 72  7]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7516
INFO voc_eval.py: 171: [19  3 10 11 12  7  4 18  6 15  2 14  9 16 21 22  8 20  0  1 13 17  5]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8430
INFO voc_eval.py: 171: [  0 427 428 429 430 431 434 426 435 437 438 439 440 442 443 436 444 425
 421 402 403 405 406 407 408 422 409 411 414 415 417 419 420 410 446 447
 448 476 477 478 479 480 481 474 483 487 488 490 491 495 502 484 472 470
 467 450 451 452 453 454 455 457 458 459 460 461 462 464 465 466 400 397
 396 395 328 329 330 331 333 336 324 337 339 342 343 344 345 346 338 323
 322 321 296 299 300 301 302 303 304 306 308 309 312 314 317 318 320 347
 504 348 350 374 376 377 378 380 382 373 383 385 389 390 391 392 394 384
 372 371 370 351 352 355 356 357 358 360 361 362 363 364 366 367 368 369
 349 505 509 511 681 683 685 686 687 688 678 689 692 693 694 695 696 697
 690 677 672 670 645 646 647 648 653 654 655 656 657 659 660 664 665 666
 669 698 644 699 704 740 742 743 744 745 746 739 747 749 750 751 752 755
 757 748 738 733 732 705 706 710 711 712 714 716 718 719 720 723 724 728
 729 730 702 295 642 640 550 551 556 562 563 564 548 565 567 569 570 572
 573 574 566 547 546 545 512 522 523 524 525 526 527 533 535 536 537 538
 539 540 543 575 641 576 578 609 614 617 622 623 628 608 629 634 635 636
 637 638 639 630 606 605 603 580 581 582 585 586 587 588 589 590 593 594
 595 596 597 602 577 294 762 159 164 163  29 161 160 158 157 156 155 154
 153 152 151 149  31 147 146 145 144  32 139  33 293 137  34 135 134 165
  28 167 169 199 197 196 195 194 193 192 190 189 188 187 186 185 133 184
 182  24 181 180 179 178  25 175 174 173  26 171  27 183 132 131 128  93
  92  91  88  87  51  86  52  53  85  84  82  81  50  54  77  73  55  71
  70  69  68  67  66  64  62  61  60  78 202  49  96  36 125 124  37 123
  38  39  40  41  42 120 119 118  48 117 116 115 113 110 109 105 104 103
 102 101 100  99  47  43 203  58  22 239 272   3 238 237  14   2 275 241
 276 255 234 232  15 231 279 230 228 277 271 269   4 254 253 257 252 259
 251 260 261 250 249 248 262  11 245 244  13 243   9   8   1 280  12 284
 288 287 282 212  18 218 286 283 220 217  21  19 221 222 208 209  17 215
 224 289 210 211 214 290 213 291  20 510 513 707 507 359 756 353 200 679
 399  80 482 129  35 489 682 206 668 127  65 354 142 530 559 107 676 233
 600 168 471 598 456 667  95  63 311  74 229 130 278 240 555 753 583 519
 227 515 207 671 441 319  30 571 375 463 264 468 136 388 316 591 501  79
 529 542 493 500 386 325 761 554  46 508 610 661  76 305 473 650  23 651
 166 531  75 544 584 267 285 516 242 432 270   6 514 626 114 401 758 494
 340 112  90  72 198 552 506 607 162 520 759 204 658 177 313 532 150 191
 497 518 674 404 649 725  89 486 332 398 111  45 521 680 265 106 760 560
 423 393  98 517 424 503 613 341 621 633 604  16 611 492 499 258 624 235
 485 298 387 498 700 292 225 412 721 247 445 246 675 709 310 108 381 327
 735 684 121 176 741 579 307 226  94 736 201 625  57 717 731 148 727  44
 122 334 141 616 722 601 315 701 297 599 219 663 557   7 413 618 561 469
 266 662 335 273 541 615 205 365 691  10 528 126 138 631 643 549 216 274
 256 553 534 734 379 416 268 326 708 140 612  56 703 568 143 652 263 449
 170 620 619   5 713 433 715  83 737 236 281 632 223 496 754 592 172 558
  97 627 475 726 418 673  59]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7306
INFO voc_eval.py: 171: [ 75  29  31  32  33  34 133  65  38  39 129  89  55  93  44  99  46  47
  48 122  50 120 113 112  54  94  84  28 141   1   2 147 145   6   7  72
  81  10  11  12  26 143  15  16  17  18  19 142  21  22  23  24  14 103
  64  77  86 132 134   5 144 125 127  36   9  79 128  68 107 151  69   0
 136  95 123  96  76 111  85 104  62 121  59  52 100   3  42  66 130 135
 114  57  41  97 152  71  37  83  49 138 137 108  90  67  82  13  20  51
 117  73  78  45 119 149  91 106  60  43  70   4  63 124  27  88  74 115
 110 118 140   8  25 146 126 116 102 139  98 150  56 148  80 101  58  61
  40  35 109  30  87 105 131  53  92]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5104
INFO voc_eval.py: 171: [65 45 44 43 41 52 39  2  3 46 67 50 17 68 38 42 40 51 13 14 74 64 47 66
 24 56 49 11  5 31 18 15 23 10 22 12 27  4 32 76 20 72 37 75 25 16 59 28
 58 61 62  7 30 36  0 71  6 48 55 69 63 29 70 21 57 60 19 54 73 34 35 26
  1  9  8 33 53 77]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5188
INFO voc_eval.py: 171: [12 28 66 38  1 19  4 29  3 44 40 52 15 22 43  2 48 13 11 21 39 30 47 63
 37 34 56 58 24 33 57 31 17 20  9 54  8 26 49 10 32 27 45 18 67 55 51 25
  7 16 36 46 59 35 60 53  5  0 50 14 61 62  6 42 65 41 23 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3392
INFO voc_eval.py: 171: [16 51 17 54  7 35 12 21  2 28 38 26 29  6  8 57 24  5 11 13  9 10 40 25
 18 32 27 22 42 48 41 55 56  3  0 49 53  4  1 23 47 58 19 34 46 39 15 36
 33 45 31 14 52 59 44 30 50 37 20 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5687
INFO voc_eval.py: 171: [42 26 30 41 20 31 15 29 12 11 28 35  1 37 10  4  3  2 33 23  7 32 24 22
 16 39 14 13  9 21 40 19 18 17  6  8 27 34 38 36  5 25  0]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5222
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5683
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.358
INFO cross_voc_dataset_evaluator.py: 134: 0.743
INFO cross_voc_dataset_evaluator.py: 134: 0.446
INFO cross_voc_dataset_evaluator.py: 134: 0.496
INFO cross_voc_dataset_evaluator.py: 134: 0.489
INFO cross_voc_dataset_evaluator.py: 134: 0.795
INFO cross_voc_dataset_evaluator.py: 134: 0.679
INFO cross_voc_dataset_evaluator.py: 134: 0.192
INFO cross_voc_dataset_evaluator.py: 134: 0.532
INFO cross_voc_dataset_evaluator.py: 134: 0.667
INFO cross_voc_dataset_evaluator.py: 134: 0.542
INFO cross_voc_dataset_evaluator.py: 134: 0.642
INFO cross_voc_dataset_evaluator.py: 134: 0.752
INFO cross_voc_dataset_evaluator.py: 134: 0.843
INFO cross_voc_dataset_evaluator.py: 134: 0.731
INFO cross_voc_dataset_evaluator.py: 134: 0.510
INFO cross_voc_dataset_evaluator.py: 134: 0.519
INFO cross_voc_dataset_evaluator.py: 134: 0.339
INFO cross_voc_dataset_evaluator.py: 134: 0.569
INFO cross_voc_dataset_evaluator.py: 134: 0.522
INFO cross_voc_dataset_evaluator.py: 135: 0.568
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5499
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.475s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.371s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.368s + 0.001s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.369s + 0.001s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.373s + 0.001s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.376s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.373s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.374s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.374s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.374s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.373s + 0.001s (eta: 0:00:08)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.373s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.376s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.575s + 0.001s (eta: 0:01:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.396s + 0.001s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.398s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.390s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.385s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.388s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.398s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.394s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.392s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.393s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.392s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.391s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.389s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.511s + 0.001s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.365s + 0.001s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.374s + 0.002s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.373s + 0.002s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.366s + 0.002s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.366s + 0.001s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.365s + 0.001s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.370s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.375s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.375s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.376s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.376s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.497s + 0.002s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.385s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.388s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.383s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.387s + 0.001s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.383s + 0.001s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.380s + 0.001s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.377s + 0.001s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.377s + 0.001s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.378s + 0.001s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.375s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.377s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.376s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.564s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [11 27 16 41 10  8  7 24  9 48 54 40 51 35 25 53 56  5 38 30 33 28 52 46
 36 26 42 13 55 57 22  6 45 43 23 29 44 59 31 32 21  0 47 34  3 18  1 50
 49 14 12 20  2 17 37  4 19 39 58 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3543
INFO voc_eval.py: 171: [15 13  3 12  7  9  4 19 17  6  0  5 11 14  1  8  2 18 16 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.7514
INFO voc_eval.py: 171: [106  36 122  40  44  45  46  47  98 113  57 229  64 215  67  68  69  73
 182  93  89  88  87  85  35 226  82 185 100 190 179 102  83  34  48  32
 205 163 170  15 139 207  23 158   7 164 146 162 157 126   2 165  27 231
  70  71 221  78 204  52 111 230   3 114   5 110 140 219 183 138 181  50
  43 225 132  53 137  60 108  74 196 180 105  38 214 130  90  12  72 184
 228 107 201  49 218 175  28 154 186  39 119  76 174 116 202 227   8 211
  26 160  14 178 208 220   9 195 159 223 109 222  63 143  55 167 217  37
 125  16  91 197  42  19 191  96  25 124 176   1  20 172 120  51 150  77
  95 142 112  56 148  65 210  30  10  13 233   0 168 134  58  24  94 189
 129  41 118 177 198  21  92 171 232 173   6  97 169 206 123 127  99  80
 209  79  86  81 135  75 213  17 224  22 121 133 131 152 136 101 194  33
 193 199 212  29 104  66  18 147 188  11 149 103 145 200 144 216 234 155
 187 156  84 115  62 117  31 153 203  59 161 141 128 151  61   4 192  54
 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4346
INFO voc_eval.py: 171: [  0  93  65  38  90  89  80  78  77  46  47  76  75  54  74  71  68  61
  62  63  27  95  33 117   1   6 120  11 131  29  31  12   5 115  15  91
  23  92 126  64  22  32 130  26  85  48  94  34  59  97  70 129  67 113
 100 105  37  99  50   3 122  39  49   7 101  30  53  66  58 108   8  45
  17   4 125 112  52 110 106  69  81  16  56  60 121 114   2  10  79  35
 109 119  24  28  14  40  36  82  13  44  51 124 128 123 132  21 102  98
  96  19 118  84  72  83  41 111 116  55  86  25 107  73 127 103  88   9
  57  42  18  43  87  20 104]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4799
INFO voc_eval.py: 171: [ 0 32 34 36 37 38 39 41 94 55 57 61 31 62 65 68 70 72 75 76 77 78 79 80
 84 64 30 95 18  1 67 46 52 33  2 15 66 35 53 20 29 23 69 14 85 45 13 81
 19 27 25 91 16 42 43 11  7  4 28  9 82  6 63 12  8 89 59 92 40 49 21 47
 73 86 58 74  5 93 88 71 87 51 54 10 17 22 24  3 48 50 83 44 56 90 60 26]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4902
INFO voc_eval.py: 171: [13 12 24 30  1 16  5 22  4  3 19 17 20  2  6  7 10 26 15 14 25 21 31  0
 27 23 29 11  9 18  8 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.7951
INFO voc_eval.py: 171: [ 54 168 167  81 166  80 164  77 127 162 128 157  66  65 154  42 153 152
  58 107  53  52  82  50 124 172   1 109 112 113   5 116   7  95 184 119
  24  11  93 121  89 175 122  17  18  19  20 174  84  51 139 129  94 148
 182 159  71  56  40 108  46 120  74  33 177 186  29 125 134 123  63 102
  73  79   3  72  67  43 117 161   2  62  16 146 147  22 144  10  88 156
  85 183  96  49  97   4   8 140 110 170  31 106  78  64  90 150  34  98
  26 189 187  55  68 135   9 181 141 126  48  32  21  25  12  91 158 163
  36 115 100 179 178 165 171  30 103 104  69  27 173 105  14  28 118  47
 142 180 130  44  92   6  39 143  59 132 149  87  60  13 176  83 145  76
  15 155  23 169  37 133 111 101 188  61  75  45  35 114 131 136 137  86
  38 160  99 138  41  57   0 185 151  70]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6815
INFO voc_eval.py: 171: [ 2 45 24 50 38 37 18 49 34 47 41 20 48 22  9 10 26 32 35 51 40  8  3 30
 28 16 14 42 21 15 23 46 33  7 17 39  4 36  5  6 29 19 13  0 12 31  1 27
 44 25 43 11]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1942
INFO voc_eval.py: 171: [223  76  75 153  73  72  71  70 154 159  77 160  63  62  61 163  59 120
 168 169  55 162 152 148 146 115 114 113 112 121 122 123 126 103 102 101
 127 128 129 132 136  91 138 139 140  83  50  48 118 193 187 209  14  30
 191  28 201 192  24  17  23  22  47 194  18 199 186 210  10   1   2 218
  45  44  35   7  40 211 183  37  36  93   8 215 137 219 124  88  49  85
  46  26  31 188  41  81 173  98  99 181 125 131 109  52  42  39  27  53
 104 165  87 217 204 157 200 177 149  65 111 107  68  84  32 116  69 185
 179  25 143  60 172 117   9  29  79 155 189 196 203 216 182 176 207  57
  16  21 110  54  64 213 156  34   4 208  66  82 133 141 197 134 195  80
 202 171 222 108 166  74  56  12   5 174  19 198 119  96  97 161 144 190
 206  92 150 145 135  58   0 106 214 105   6 221  13  78 147  89 142 180
  43 178 220  20  51 164  33 100   3  38 184  94 212 167  95 130  90  86
 170 205 151 158  11  67  15 175]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5316
INFO voc_eval.py: 171: [33 14 15  7 35 27  3 17 12 16 19  4  2 44  1 13 49 41 32 18 54 10 55 62
 65 40 58 38 56 24 20 61 63  8  6 51 37 46 53 39 31 45 47 66 60 23 42 59
 67 21 22 52 26 28 25 34  5 30 11 64 48 36  9 50 57 43  0 29]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.6685
INFO voc_eval.py: 171: [ 42   1 112  90   7  91  93  21 103  78  18  65   2  83  44  89  62  45
  49  37 108  30  10  85  76  25  47  41  97  20  56  77  29  86  19  60
  99  57  27  51 106  54   5  84  94  55  95  52   3  28  43  58 114  38
 109  13  36  72  32  81  14  68  74   0  75  63  15  26  92  12  35   9
  22  66  88  33  16 110  67  24  98  79   4  61  11  69  34  17 102  73
  39 105  46  31 115  59  23 104  71  87  50  40  70 101  82  80   6 107
 113 111  64   8  48  96 100  53]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5277
INFO voc_eval.py: 171: [71 64 18 15 36 37 26  7 13 51 34 54 31 20 40 55 28 48  6  8 24 44 19 47
 11 52 22 65 35  9 29  0 17 59 60 61  2 14 63  4 27 33 30 43 46 42  1 68
 41 66 12 10 21 70 32 57 23 39 67 49 53 50 58  3  5 69 25 56 62 16 38 45]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6437
INFO voc_eval.py: 171: [53 30 51 63  9  8 41 74 75 68 77  3 16 57 45 39 48 11 33 34 58 70 32 26
 29 56 64 73 44 36 28 47 20 52 21 13 50 22  0 35  1 71 66 67 61 54 19 59
 27 12 31  5 23 79  2 43  6 76  4 65 10 25 37 18 78 15 40 24 55 14 46 49
 69 62 42 60 38 17  7 72]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7571
INFO voc_eval.py: 171: [18  3  8 10 11  6  4 17 15  5  2  9 13 14 20 21  7 19  0  1 12 16]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8430
INFO voc_eval.py: 171: [  0 426 427 428 429 432 433 425 434 436 437 439 440 441 443 435 444 424
 420 401 403 404 405 406 407 423 408 412 413 415 417 418 419 409 445 446
 448 475 476 477 478 479 481 474 482 486 488 489 493 500 502 485 472 470
 468 449 450 451 452 453 455 456 457 458 459 461 462 463 464 465 400 398
 395 394 328 329 331 334 335 336 327 338 341 342 343 344 345 346 340 326
 322 321 298 299 300 301 302 304 306 307 310 312 315 316 318 319 320 347
 504 348 350 374 375 376 378 380 381 372 382 387 388 389 390 392 393 383
 371 370 369 351 353 354 355 356 358 359 360 361 362 364 365 366 367 368
 349 507 509 510 678 679 681 683 684 685 675 686 689 690 691 692 693 694
 687 674 670 668 643 644 645 646 651 652 653 654 655 657 658 662 663 664
 667 695 642 696 701 737 739 740 741 742 743 736 744 746 747 748 749 752
 754 745 735 730 729 702 703 707 708 709 712 714 716 717 718 721 722 725
 726 727 699 297 640 638 548 549 554 559 560 561 546 562 564 566 567 569
 570 571 563 545 544 543 511 520 521 522 523 524 525 531 533 534 535 536
 537 538 541 572 639 573 575 607 612 615 620 621 626 606 627 632 633 634
 635 636 637 628 604 603 601 578 579 580 583 584 585 586 587 588 591 592
 593 594 595 600 574 294 759 157  41 186 185 184 183  97 182 181 180 179
 178 187 177  98  40 173 172  99 171 100 169 168  23 101 176 102 188 190
 210  46  89 209 208  17 207  18 206 130  19  94  90 201 200 199  20  42
  21 195 194 193 192 191  91 211 166  24  36 147  27 145 144 121 122  35
 123 143 142  37  34  33  30 293 125  32 137  31 134 133 132 131  28 103
 118 117 163 162 161 159 158  54 156 107  39 155 154 149 153 108 111  25
 151  26 112 114 115 116  38 150 152 212 129  16 270   3 269  68 267  69
   4  71  53  75  67  76 260 259 213 257 255   8 253 252 251 250   7   2
 273 274 292 291 289 288 287  57 286  58 285 284  59  60 282  62 281 280
  64  65 278 277 275  66   1 249 248 258 246  12 222  48  83 218  86 230
 229 228  82  51  84 247 219 226  49  13  85 220  14  50  11 232  47  80
 235 244 215 216 243  52 241  10  79 237 236 239 454 508 357 680 556 704
 505 204 197  78 753 127 487 480 676 397 528 139 126 666 598 231 352 469
 165  63 105 673 596  61  93 665  72 553 309 276 227 238 128 750 581 517
 205 225 513 669 317 438  29 373 568 466 460 135 527 262 314 589 386  77
 499 490 174 540 498 384 758  45 506 323 659 552 608 491  74  22 303 648
 471 164 649 529 582 283 542 265  73 514 240 399 512 268 624 430   6 110
 337 755 113 196  88  70 550 503 605 160 756 518 202 656 311 530 148 175
 671 495 189 516 402  87 647 723 484 330 396  44 109 757 519 677 263 557
 104 391 421 515  96 422 141 501 611 497 339 619 602 256  15 610 483 296
 631 622 233 496 290 223 697 385 442 719 245 242 410 308 672 706 106 576
 732 682 325 379 738 305 733 119  92 724 146 198  56 623 120 224 332  43
 728 715 614 599 698 720 313 217 295 411 661 597 616 558 264 467 333 555
 660   9 613 124 539 203 271 688 526 547 363 641 272 136 214 629 551 254
 492 532 731 414 377 266 609 324 705 565 138  55 618 261 700 447 167 140
 431 650 617   5 577 711  81 734 713 710 279 494 630 751 221 170  95 234
 590 625 473 416]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7318
INFO voc_eval.py: 171: [ 37  32  33  34 131  39 128  44  46 121  48  49 119  31  51 111  54  55
 102  98  93  92  89  65  84  81  72 112  29  75 143  10  11  12  28  14
  15  16  17  18  19 142   6   7 145  21  22 147   2  23  24 141  26   1
   5  77  64  86 132   9 124 126 130  36 144  79 127  68 106 151  69   0
 135  94 122  76  95 110  85 103 120  62  59  53  99   3  42  66 129 134
 113  57  41  96 152  71  50  38  83 138  45 136 107  90  67  82  13  20
 115  52  73 117  78 118 149  91 105  60  43  70   4  63  27  88 123 114
  74 109 140 116   8  25 146 125  47 101 139  40  97 150 148  35  80 108
  58  87 100  56  61  30 104 137 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5115
INFO voc_eval.py: 171: [65 45 44 43 41 52 39  2  3 46 67 50 17 68 38 42 51 40 14 12 74 47 64 24
 66 56 49 11  5 31 18 15 13 23 22 10 27  4 32 75 20 37 76 72 25 16 59 28
 61 58 62 36  7 30  0 55  6 71 48 69 63 29 70 57 21 54 60 19 73 34 35 26
  1  9  8 33 53 77]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5198
INFO voc_eval.py: 171: [12 28 66 38  1 19  4 29  3 40 44 52 22 15 43  2 48 13 11 21 39 47 30 63
 56 58 34 37 24 33 17 57 31  8 54  9 20 26 49 10 32 27 18 45 67 55 51 25
  7 16 46 36 35 59 60 53  5 50  0 14 23 61 62 42  6 41 65 64]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3450
INFO voc_eval.py: 171: [53 19 18 37 56  7 14 23 30  2 40 31 28  8  6 59 26  5 12 15 10 11 42 27
 20 34 29 13 44 43 50 57 24 58  3  0 51 55  4  1 60 25 49 21 36 48 17 41
 47 35 38 33 54 16 61 46 32 52 39 22 45  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5650
INFO voc_eval.py: 171: [ 0 36 34 30 29 28 27 25 40 19 14 41  3 10  1  2  9 11 22 32  6 31 23 21
 15 38 13 12  8 20 39 17 18 16  7  5 26 33 37  4 24 35]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5177
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5672
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.354
INFO cross_voc_dataset_evaluator.py: 134: 0.751
INFO cross_voc_dataset_evaluator.py: 134: 0.435
INFO cross_voc_dataset_evaluator.py: 134: 0.480
INFO cross_voc_dataset_evaluator.py: 134: 0.490
INFO cross_voc_dataset_evaluator.py: 134: 0.795
INFO cross_voc_dataset_evaluator.py: 134: 0.682
INFO cross_voc_dataset_evaluator.py: 134: 0.194
INFO cross_voc_dataset_evaluator.py: 134: 0.532
INFO cross_voc_dataset_evaluator.py: 134: 0.669
INFO cross_voc_dataset_evaluator.py: 134: 0.528
INFO cross_voc_dataset_evaluator.py: 134: 0.644
INFO cross_voc_dataset_evaluator.py: 134: 0.757
INFO cross_voc_dataset_evaluator.py: 134: 0.843
INFO cross_voc_dataset_evaluator.py: 134: 0.732
INFO cross_voc_dataset_evaluator.py: 134: 0.512
INFO cross_voc_dataset_evaluator.py: 134: 0.520
INFO cross_voc_dataset_evaluator.py: 134: 0.345
INFO cross_voc_dataset_evaluator.py: 134: 0.565
INFO cross_voc_dataset_evaluator.py: 134: 0.518
INFO cross_voc_dataset_evaluator.py: 135: 0.567
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
Start testing on iteration 5999
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.564s + 0.001s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.370s + 0.001s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.384s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.390s + 0.001s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.393s + 0.001s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.394s + 0.001s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.392s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.393s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.392s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.389s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.388s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.388s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.390s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.479s + 0.001s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.403s + 0.001s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.390s + 0.001s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.382s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.382s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.382s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.386s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.384s + 0.002s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.382s + 0.002s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.381s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.380s + 0.002s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.380s + 0.002s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.377s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.553s + 0.002s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.391s + 0.001s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.381s + 0.001s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.390s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.387s + 0.002s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.380s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.383s + 0.002s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.388s + 0.001s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.389s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.390s + 0.002s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.390s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.390s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.390s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 130: Testing with config:
INFO test_net.py: 131: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'BOX_OUT_STREAMS': 1,
               'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5},
          'WEAK_SUPERVISE': False},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_BOX_HEAD': False,
           'FREEZE_CONV_BODY': False,
           'FREEZE_RPN': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': False,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': False,
           'WEAK_SUPERVISE_WITH_PRETRAIN': False},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 324: training mode? : False
INFO test_engine.py: 331: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.566s + 0.002s (eta: 0:01:10)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.415s + 0.002s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.403s + 0.002s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.391s + 0.002s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.393s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.396s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.390s + 0.001s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.391s + 0.002s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.386s + 0.001s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.387s + 0.001s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.386s + 0.001s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.382s + 0.001s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.382s + 0.001s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_VOC-clipart-ideal/Oct11-06-41-35_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 59.615s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 9 22 14 39 25  6  7 10  8 46 52 38 49 33 23 51 54  4 36 28 31 26 50 44
 34 24 40 12 55 20 53  5 43 41 21 42 27 57 29 30 19 45  0 32 16  2 48 47
 11 13 15  3 18  1 35 17 37 56]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.3539
INFO voc_eval.py: 171: [15 13  3 12  7  9  4 19 17  6  0 14  5 11  1  8  2 16 18 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.7514
INFO voc_eval.py: 171: [101  35  36  37 140  41  90  46  47  48  49 229 127  58 123 114  65  68
  69  70  74 107 103  83  84  99  86  94  88  89 217  33  45 147 232 183
 186  16 180 206 233 192   5 208 170 166   7  24   2 158 165 163 159 164
  53  72 112 205 115  79  28 223   3 228  71 139 111  44 182 221 184 141
 227  51 133 109  61 138  54 198  75 106 181 216  39 130  91 230  13  73
 185 108 202  50 155 220 175  29 187  40 120  77 174 117   8 231 203 213
  27 161  15 179 222   9 209 196 160 110 224  64 144  56 167 126  38 219
 177  17 197  43  92  20 191  26  97 176 125  21 113  52   0 151 121 173
  96  57 143  78 149  10  31 212  66  14 135 168 235 225 131   1  25  95
 190  59 119 199  42  93  22 172 234   6 169 207 124 194 128  98 100  81
 211 215  87  80 136  76  18 226  82 122  23 178 134 132 153 137 102  34
 195 214  30 200 105 236  19 148  11 189  67 104  32 150 146 201 210 145
 218 156  63 188  85 116 118 154 157 129  62 204  60 162 152 142 193   4
  55 171  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.4341
INFO voc_eval.py: 171: [  0  38  46  47  54  61  62  63 130  68  71  33  74  76  77  78  80  89
  90  93  95 116 119  75  27  65   1  11   6  15  29 115  31  12  92   5
  23  91 125  64  22  32 129  48  26  94  85  34  59  97  70 128 113  67
 100 105  37  99  50   3 121  39  49   7 101  30  53  66  58 108   8  45
  17   4 124 112  52 110 120  81 106  56  16  69  60   2 114  10  79  35
 109  28 118  24  14  40  82  36  13  44  51  98 123 127 131 122 102  21
  96  42 117  19  72  83  84  25  41  55 111  86 107  73  57   9 103 126
  88  43  87  18 104  20]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.4943
INFO voc_eval.py: 171: [ 0 33 35 37 38 39 40 42 95 56 58 62 32 63 66 69 71 73 76 77 78 79 80 81
 85 65 31 96 19  1 68 47 53 34  2 16 67 36 54 21 30 24 70 15 86 46 14 82
 20 28 92 17 43 44 12  7  4 26 29  9 83 64  6 13  8 90 93 60 41 50 22 48
 74 87 59  5 75 55 94 52 89 88 72 18 23 11 45 25 84 49  3 91 57 61 51 10
 27]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.4902
INFO voc_eval.py: 171: [13 12 24 30  1 16  5 22  4  3 19 17 20  2  7  6 26 10 15 25 14 21 31  0
 27 23 29 11  9 18  8 28]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.7951
INFO voc_eval.py: 171: [ 96 156 155  43 154  51  52  53  55  59  66  67 130 129 160 126  81  82
  83  85 124 123  90 121  94 117 114 113 110  78 165  54 108 178 177 167
  18  19  11 187  20   7  21 175   5 185 169  25 171   1 170 131 162 151
  95 141  41  57  72  75  47 109 122 189  34  30 180 127 136 125  64 103
  79  74  73   3 118  68  44   2 164  17  63 149 150  23 147  10  89 158
  86  97  50 186   4  98 142   8 111  32 173 107  80  65  91 153  27  35
  99 192 190  56 137  69 184   9 143 128  49  33  26  22  12  92 161 101
 116 166 182  37  29 181 168 174  31 104 105  70  28 176 106  14 120  48
 144 183  93  45  40 132   6  60 146  61 152 134  13 179  88  15  24 148
  77  84 157 172 112  38 191 135  76  62  46 115 102 145 133  36 139 163
 138  39  87 140 100  16 159 188   0 119  71  58  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.6710
INFO voc_eval.py: 171: [ 2 45 24 50 38 37 18 49 34 47 41 20 22 48  9  3 10 26 32 35 40  8 51 30
 16 28 14  6 42 21 15 23 33 46  7 17 39  4 36  5 29 19  0 12 13 31 27 44
 25 11  1 43]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.1932
INFO voc_eval.py: 171: [221  76  75  74 152  72  71  70  69 153 151 158 161  62  61  60 117  58
 167 168  54 159 179 147  82 120 121 114 113 112 111 122 125 126 145 102
 100 127 128 131 135  90 137 138 139 101  49 162 119 185 207 189  13  29
 190  27 199  16 191  23  17  22  21  47 208 197  34  10   1  45   2 216
  44  35  43  40 209 184  36   7 192 213 123 136   8 217  48  84  80  25
  46  30 186  39  86  98 172 180  92 124  97 130 108  51  41  38  26  52
 103 164  87 215 202 148 156 198  83 106  64 110  67 115  31  68 183  24
 177  59 109 171 116 142  78  28 194   9 174 154 187  56 201 214 175 205
  15 181 206  20  53 211  63 155  33   4  65  81 132 140 133 193 195  79
 200 170 107 220  73 165  55  11   5  18 173 143 196 118  96 160  95 188
 204 149  91 134  57   6   0 212 144 105 104  88 146  77  12 219 141 178
 218  50  19  42 176 163  99   3 182  32  37  93  94 166  85 129 210  89
 169 150 203 157  66  14]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.5310
INFO voc_eval.py: 171: [33  3 27  7 35 14 15 17 16 12 19  4  2 44  1 13 49 32 41 18 54 10 55 62
 65 40 58 38 56 24 20 61 63  8  6 51 37 46 53 31 39 47 60 45 66 23 42 67
 59 22 21 26 28 52 25 34 30 11  5 64 50 36  9 57 43  0 29 48]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.6669
INFO voc_eval.py: 171: [ 42  93  91 103  90  65  83  21  18 112  78   1   7  44   2  45  89  62
  49  37 108  30  10  85  76  25  47  41  97  20  56  77  29  86  60  19
  99  57  27  51 106  54  84   5  94  55  52  95  28   3  58  43 114  38
 109  36  13  72  32  81  14  68  74   0  63  75  26  15  12  92  35  66
   9  22  33  88  16 110  11  67  24  98  79   4  61  69  34 102  73  17
  39 105  46 116  59  31 115 104  87  50 101  23  71  70  40  82  80 107
   6 111  64 113  96  48   8 100  53]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.5333
INFO voc_eval.py: 171: [69 14 62 17 35 36  6 25 12 50 33 52 19 30 39 53 47 27  5  7 23 43 46 18
 10 51 21 63 34  8 28  0 16 57 59 58  2 13 61  3 26 32 42 29 45 41 66 40
  1 11 64  9 20 68 31 38 55 22 48 65 49 56  4 67 24 54 44 60 37 15]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.6635
INFO voc_eval.py: 171: [54 30 52 58 64  9  8 42 75 76  3 16 78 69 40 45 49 11 33 34 59 71 32 26
 29 65 57 74 36 28 46 48 20 53 21 51 13 22  0 35  1 72 68 67 55 61 60 12
 27 31  5 23 19 80 44  2  6 77 66  4 10 25 38 18 79 15 41 47 24 56 70 50
 63 37 14 62 43 39  7 17 73]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.7510
INFO voc_eval.py: 171: [18  3  9 10 11  4  6 17 15  5  2  8 13 14 20  7 21  0 12  1 19 16]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.8430
INFO voc_eval.py: 171: [  0 427 428 429 432 433 434 426 435 437 438 439 440 442 444 436 445 425
 423 403 404 405 406 407 408 424 409 413 415 417 418 419 420 412 446 448
 449 475 476 477 478 480 481 474 484 486 487 488 492 499 501 485 473 472
 470 450 451 452 453 455 456 457 458 459 460 462 463 464 465 468 401 400
 398 395 329 331 334 335 336 338 328 340 342 343 344 345 346 347 341 327
 326 322 298 299 300 301 303 305 306 309 311 314 315 317 318 319 320 348
 502 349 351 375 376 378 380 381 382 374 383 388 389 390 392 393 394 387
 372 371 370 353 354 355 356 358 359 360 361 362 364 365 366 367 368 369
 350 506 508 509 677 679 681 682 683 684 674 685 688 689 690 691 692 693
 686 673 669 667 642 643 644 645 650 651 652 653 654 656 657 661 662 663
 666 694 641 695 700 736 738 739 740 741 742 735 743 745 746 747 748 751
 753 744 734 729 728 701 702 706 707 708 711 713 715 716 717 720 721 724
 725 726 698 297 639 637 547 548 553 559 560 561 545 562 564 566 567 569
 570 571 563 544 543 542 510 519 520 521 522 523 524 530 532 533 534 535
 536 537 540 572 638 573 575 607 612 615 620 621 626 606 627 631 632 633
 634 635 636 628 604 603 601 578 579 580 583 584 585 586 587 588 591 592
 593 594 595 600 574 296 758  37 168  36 167  68 165 162 161 160 158 157
 156 155  35 154 152 151  38  39 150 149 148 146  40 144 143 142 153 170
 171 172  30 200 199 198  31 194 193 192 191 190 189  32 187 186 185 184
  33 183 182 181 180 179 178 177 176 175  34 141  41 294 137  99  56  98
  97  96  57  93  58  90  89  88  59  61  63  85  84  64  65  66  83  82
  81  79  78  75  74  70 100 205 101 106  45  46 134 132 131  47 130 129
 128  48 124  49  50  51 122 121 120  52 117  53 116 115 114 113 111 110
 107 102 206  67  29 235 250 280 234  18  19   6 251 281  20   7 231 252
   9 229 269 283 236 249  11 272  15 245   4  16   3 276 247 243  13 277
 241   2 279 248 240 274 238 273  17 246 207 254 215 214  25 212 211 290
 259 266  26  27 210 209 291  12 292  10 208 228 288  24 287 284 226 225
 268 256   1 217 221  22 257  23 286 219 218 258 285 556 357 675 678 126
 507 664 504 752 596 397 203  77 703 196 479 527  60 469 138 164 598 672
 665 125 104  62 352  92  71 230 454 127 308 552 227 516 237 275 749 581
 512 204 224 316 443 668 568 373  28 466 526 461 133 261 313 589 422 490
 539 498 386 757 497 384 658  21 505 608 323  73 302 551 647 471 491 648
 163 282 582 528 264 109 541 513 511 239 624 430 112 399   5 267 337  72
 195  76 754 483  87  69 549 605 755 159 174 655 517 201 310 670 147 529
  44 494 515 188 646 402 503  86 722 330 396  43 756 518 108 676 262 557
 514 103 391 421  95 611 140 496 500 619 339 255 602 489 482 630 295  14
 622 495 232 610 289 696 222 385 718 441 410 244 242 307 105 325 680 737
 671 576 731 304 705 732 145 379  91 723 118 332 623  55  42 119 197 173
 223 727 714 614 719 697 216 321 599 411 660 312 293 597 616 555 558 263
 467 333 659 613 271 233 123 202 270 135 213 538 640 525 546   8 687 363
 550 253 554 377 265 730 531 414 609 324 136 704 565 699  54 618 260 166
 431 629 139 577 617  80 712 278 733 710 709 493  94 649 220 750 169 590
 447 416 625]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.7318
INFO voc_eval.py: 171: [ 74  29 130  31  32  33  34  64  37 127  39  88  55  91  44 120  46  47
  48 118  50 111 110  53  97  92  83  28 139   1   2 145 143  71   6   7
  80  10  11  12  26 141  15  16  17  18  19 140  21  22  23  24  14 101
  85  63  76  36 123 125 129 131   9 142   5  78 126  67 105 149  68   0
  93 121  75 132  94 109  84 102  61 119  58  52  98   3  42  65 128 134
 112  56  41 150  95  70  38  49  82 137 135  89  45 106  20  81  66  13
 115  72  77  51 117 147  90 104  59  43  69   4 122  62  87  27  73 108
 138 113   8  25 116 144 124 100 114  40  96  35 148 146 107  57  86  99
  54 103  30  79 136  60 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.5132
INFO voc_eval.py: 171: [64 51 45 44 43 42  3 39  2 46 66 50 17 67 38 49 41 40 14 12 73 47 63 24
 65 55 48 11  5 31 18 15 13 23 22 10 27  4 32 37 20 74 75 25 71 16 58 28
 57 60 36 61 30  7  0 54  6 70 62 29 68 69 56 59 19 72 53 21 34 35 26  1
  9  8 33 52 76]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.5057
INFO voc_eval.py: 171: [28 12 65 38  1 19  4 29  3 44 40 52 22 43 15  2 48 13 11 21 33 39 30 47
 62 56 58 37 24 34 17 57 31 54  9 20 26 49 27 10 32 18 66 45 55  8 51 25
  7 16 46 36 35 59 53  5 60 50 23  0 61  6 14 41 42 64 63]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.3379
INFO voc_eval.py: 171: [18 52 36 17  7 55 13  2 29 22 39 30 27  8  6 59 25  5 11 14  9 10 41 26
 19 33 28 12 43 42 49 56 23 58 50  3  4 54  0 24 48  1 20 35 47 16 34 46
 37 32 15 53 60 40 45 31 51 38 21 44 57]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.5656
INFO voc_eval.py: 171: [ 0 35 34 30 29 28 27 25 40 19 14 41  3 11 10  1  2  9 32 22  6 31 23  8
 21 15 38 13 12 20 17 39 18  5 16  7 26  4 37 36 24 33]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.5144
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.5670
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.354
INFO cross_voc_dataset_evaluator.py: 134: 0.751
INFO cross_voc_dataset_evaluator.py: 134: 0.434
INFO cross_voc_dataset_evaluator.py: 134: 0.494
INFO cross_voc_dataset_evaluator.py: 134: 0.490
INFO cross_voc_dataset_evaluator.py: 134: 0.795
INFO cross_voc_dataset_evaluator.py: 134: 0.671
INFO cross_voc_dataset_evaluator.py: 134: 0.193
INFO cross_voc_dataset_evaluator.py: 134: 0.531
INFO cross_voc_dataset_evaluator.py: 134: 0.667
INFO cross_voc_dataset_evaluator.py: 134: 0.533
INFO cross_voc_dataset_evaluator.py: 134: 0.664
INFO cross_voc_dataset_evaluator.py: 134: 0.751
INFO cross_voc_dataset_evaluator.py: 134: 0.843
INFO cross_voc_dataset_evaluator.py: 134: 0.732
INFO cross_voc_dataset_evaluator.py: 134: 0.513
INFO cross_voc_dataset_evaluator.py: 134: 0.506
INFO cross_voc_dataset_evaluator.py: 134: 0.338
INFO cross_voc_dataset_evaluator.py: 134: 0.566
INFO cross_voc_dataset_evaluator.py: 134: 0.514
INFO cross_voc_dataset_evaluator.py: 135: 0.567
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
