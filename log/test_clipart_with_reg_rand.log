INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.518s + 0.001s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.470s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.493s + 0.002s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.493s + 0.002s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.521s + 0.002s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.519s + 0.002s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.524s + 0.002s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.523s + 0.002s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.523s + 0.002s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.522s + 0.002s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.513s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.505s + 0.002s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.499s + 0.002s (eta: 0:00:02)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.833s + 0.001s (eta: 0:01:43)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.519s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.579s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.542s + 0.003s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.535s + 0.003s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.520s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.528s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.525s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.525s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.530s + 0.003s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.520s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.511s + 0.003s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.499s + 0.002s (eta: 0:00:02)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.684s + 0.002s (eta: 0:01:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.566s + 0.003s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.538s + 0.002s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.536s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.529s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.534s + 0.003s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.528s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.540s + 0.002s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.533s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.531s + 0.003s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.519s + 0.002s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.507s + 0.002s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.498s + 0.002s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.678s + 0.002s (eta: 0:01:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.536s + 0.003s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.532s + 0.002s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.536s + 0.002s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.526s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.520s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.522s + 0.003s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.521s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.522s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.521s + 0.003s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.510s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.501s + 0.003s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.490s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 76.536s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 74  57  72  77 289  76 294 210 298  46  64 144  51  70 128 223  10 226
 306 326 246 296  71 321 287  49 179 324 123  53 204 206   7  45 224  36
 318 316 322 208 309  80 166 225 220 182  15  61 102 154 285 181 125 178
 147 215 308 302  13  41 212 301 175 127  59 288  50 247  34  54 320 310
 152  81 100  35 292  11  65  38 197 119  16 148   9 184 241 219 124 230
 304   5 114 345  39 163 193 227 346 299 245  32  62  78 315 217  73 303
 295 276  52 222 130 261  90 180 145 150  98 142 196 344 323 136 214 307
  83  99 151 347  48  69 327 186 191  92 240 170  93 107 297 111 160 126
  55 330 343 203 118 300 272 268  60 234 164 331 335 314 319   6 239 117
 231  94 349  89 138  58 264 311  23 140  17 176 209 291 198  28 129 218
 199 169 187  67 236 195 190  56 312 132 338  20 159 232 131 108 275  79
  66  97 266 216 228 194 339 205 340 200 173 165 153 257 101   8  75 133
 177 341 317  33  88  82 162 342 279  42 329 202 248 280 250  47 201  84
 334 251 143 229 328  68  25 121 270 252   0   3  19 185 134  44   2 290
 313 263  43  30 211  96  18  63  91 155 103  95 115 168 337  14 267 238
 139 109 332 116   4 161 243 237 269 235 273  12   1 122 167  27 110 284
 274 192 233 156 104 286 255 113 278 135 325 282 106  26  21 112 244 207
 254 158 260  22 336 265  85 348 350 277 213 105 189 137 249 157 242 305
 271 281 146 188 172 258 262 283 333 221 174 293 253 141  40  86  31 256
 149 183  37 259  87 171  24 120  29]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1610
INFO voc_eval.py: 171: [202  75 101 346 130  55 368 343 347 307 266 363 144 200   9 270 201 344
   7 267  25 248 204 206 134 345 349 214 180 205 154 199  97  90 356  21
  20  78 311 251 237 207 369  85 166 253  79 350 269  13 370 310 220  16
 140  87 324 213 304 135 157 211  19 183   8 235 242 168 360  15 133  57
 227 309 107  43  88 359 238 217 148 366  89  22  48 336 102 317 216 255
 208 219  59 305 212 159  30 313 318 361 262  18  34 113  45 149 223 257
 116  12 352 231 131 273  33  17  27 226 127 198 141 246 224  28 308 122
 179 329  83  91 302  71 348 228 275 124 365  99 286  58  72 354 117 258
  11 218  77 232 128  44 351 106 282  35 342 367 215 289  31 285 335 143
  10  40 256 260  98 245 234 100 327 136 229 110 312 320  92 210 261 353
 225 364 221 169  50 187 290 103 167 303 284 244   2 236 137 152 357 233
  49 362   6 325 158 252 323 272 249 358  76  42  84  23  82 120 274 271
 263  41  14 139 195 259 178 341 104 243 268 355 230  93 337  81 142 241
  39 170 321 333 338 192 109  24  46 297 174 108 239 160 334  51 280 298
 314  61  94  29  86 300  36 281  80 182 209 326  68 203  32 301 306 173
 197 153 138 295 283   0 146  70 177 165 254 172 114 250 279 247 132  66
 118 193 125  26 185 181  60 330   3 164 277 161 162 163 322 287   4  37
 292 119 332 316 191 339 296 111  52   5   1 222 115  65 151 105  96 293
 156 276  62 291 112 184 150 265 186 340  63  95 264 126 194 240 176 331
 147 294  47 129  56 190  74  38 155 189 171 278  54 315 196 328 145 188
  64 319  69 288 123 121 299  53  73 175  67]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.4437
INFO voc_eval.py: 171: [ 289 1626  291 ... 1274  392 1292]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.1070
INFO voc_eval.py: 171: [211 497 499 485 259 480 484 198   3 183 240 500 637   8 491 524 489 879
   7  17 843 501 493 509 786 809 490 826 482 254 876 739 557   5 247 536
 194 505 856 748  31 251 486 508 174 223 243  41 527 377 654  42 569 212
 709 381 227 535 477 805 200 494 782 873 796 611 365 834 512 487 546 537
 374 261 638 136 530  11 252 550 234   9 649  14 889 399 640 349 613 545
 531  57 191 182 606 621 698 795 401 880 861 169 409 616 823 791 496 187
 214 253 256 502 196 540  55  37 362 504 656 806 578 260  46 419 886 658
 874 199 842 533  63  24 511 863 560 459 563 193 110 249 810 241 423 529
 411 780 470 488  66 622 623 812  15 872 175 351 697  51 213 857 447  26
 206 549 469  20 230 710 816   6 138 855 534 205 884 400  39 454 209 384
  16 617 517 239 585 556 448 148 572 461   4 804 522 605 749 379 819 233
 146 354 197 532  54 783 503 610 325 403 202 745 192 221 548 784 520  79
 104 665 516 525 242  74  18 151 584  76 663 581 222 526 385 181  60 778
 614 113 184 576 417 510 854 176 132 235 143  81 657 523 376 360 882  27
 840 472 335 405 528 583 704 495  22 700 416 742 666  68  44 574 149 226
  53 255 375 875 744 353 355 288 521 232 147 332 562 320 641 808 736  12
 859 373  47 159  52 150 787 751 817 844 705 171 869 219  49 134 492 449
 445 860 378 390 137 165  23 587 841 402 821 693 114 646  97 579 236 599
 513 168  78 201 323 559 336 883 109  13  88 618 418 668 327  43 825 329
 421  70 478 566 514 246 231  80 180 659 694 571 661 653 877 564 333 258
 444 792 807  35 406 473 153 800 278 847 542  40 319 346 732 850 642  64
 648  45 601 561 813 178  10 387 779 310  25 655 415 674  36 515 619 604
 428 612 422 864 481 152 608  28 828 506 456 687 866 867 615 369 737 652
 217 220 127 803  65 467 389 815 280 163 862 702 801 746 324 162 337 822
 141 318 195 777  73 443 669 620 851 463 145 465 271 660 541 644 853 357
 442 852 359 715 100 462 703 115 735 315 651 204 468 553 285  32 144 888
 427  59 224 643 598 167 539 424 476 342 308 845  50 313 837 398 284 270
 720 434 811 135 441 570  87 848 112 244 723 348 650 122 865  95 107 306
 450 139 439 208 356 268 121 156 849 106 670  19 426 363 464 101 160 829
 475 155 573 689 678 724 740 188 203 567 858 366 287 730 696 688 245 733
 596 262 305 647 891 364 785 714 128 625 887  84 752 328 538 277 140 588
 738 185 827 718 626 154 798 341  56 683 311 164 273 707 639 518 126 793
 120 102  77 125 429  71 684 108 483 814 547 568 870 388 395 729 474 691
 565 158 754 458 881 776 123  61 699 177 671 438 142 207 133 408 728 166
  93  72 679 330 397 824 479 437 835 302 105 635 457  92 266 636 276  94
 314  48 331 602 321 768 307  21 645 682 407 371 229 317 367 756 507 885
 420 179  83 350 291 662 586 708 555 592 368 298   1 118 352 836 838 124
 413 228 769  86 820 446 111 722 216 498 304 551 425 339  90 830 190 774
 170 394 664 878 334 383 303 593  99 460 629 686 632 343 667 760 157  62
 766 466 361 433 340 725 131 238 590 294 432 103 764 410 312 453 338 719
  33 761  58 788 316 633 558 890   2 274 706 412 297 675 300 430 215 753
 272 695 716 552 747  98 781 225 299 750  82 279 762 293 322 264 282 692
 575 839 440 237 672 685 871  89 734 603 818 519 833 775 554 797 431 765
 296 628 119 790 451 543 673 717  29 393 676  85 186  34 634 743 767 772
 770 345 391  67 344 726 435 755 267 677 116 161 594 580 172 597 404 380
 544 600 591 283 396 846 130 218 624  38 263 589 452 370  30 189  69   0
 173 712 250 210 414  91 631 713 129 577 609 269 295 386 721 281 117 789
 248 292 455 382 309 681 759 701 630 326 471 741 627 289 358 301 595 758
 799 347 286 680 773 771 731 582 868 802 831 257 392  96 372 794 265 727
 763 436 711 757 690 290 832 275  75 607]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0542
INFO voc_eval.py: 171: [ 66  97  73 178 102 231 139  60 104  85 117 101  96  90 155   7 119  44
 213 154 228  65  47 135 129  41 113 168  74  84  81   9 144 223  56 106
 224 145 142 153 116 204  52 121 176 188  16  10 137  68  50 179   2 122
 148 229  67 166 105 160 239  15 225 136 146  75   6 159 128 111   1   8
  14  77 107 120 187  78   5 165  12  80 169 211 103 138  13 134 125  54
  82 222 109 115   4  40 198 177 216 173 157 227 124 143 185 207 184  48
   3  91 149  51  11 205 100 175 186 147 162  39 158 170 203 241 132  99
  79 164 206 151 150 217  86 210  35 118  42  62 127 237 161  89 212  98
 112 140  55  59  83  76 209 230 171 183 219 221 208 199 152 163 202 167
  72  57  71  34  43 172 114 233 200 226  53 108 133 110  58 192  63 156
  88 238 174 141 181 232  70  61   0  87  49 236  95 220  20  17 126 201
  36 234 191 196  37 190 180  93  32  23  69  92 195  24 197 193  33 123
  45  29  26 182 194  22  31  18  94 235 218  19 131 215 214  28 130 240
 189  21  27  38  46  64  30  25]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.1189
INFO voc_eval.py: 171: [465 202 196 309 313 294 319 311 315 203 126 316 303 205 206 124 193 119
 270 116  67 310 285 171 488 114 197 164 208 153 217 163 204 199 133 284
 104 158  63 195 297 466 296 472 269 151 473 167 130  74 145 127  60  65
 301 212 346  61 182  53 291 341 384 129 191 166 168 478 469 218  55 455
  46 219 143 475 318 267 112 160  77 135 190 165  72 180 216 357 295 251
 149 498 260 259 390  58  73 314 328 495 492 304 113 102 192 342 343 289
 107 155 468 404 131 502 461 170 349  79 479  59 253 351 118  70 136 214
 298 230  32 142 305 402 411  62 348  82  64  48 366 476 188 326 485 290
 210  86 198 331 503 125 405  68 299 376 410  83 510 400 391 499 451 438
 128 292  84 300 122 504 243 201 287 144 352   3  15 140 388 339  40 338
 399  30 252  31 462 525 288 458  26  52 317 120 262  85 490 463 386 361
  25  51 500  81 117 146 162 109 255 453 275  97 213 392 325 137 293  88
 522  24 286 464  71  89 329 403 209 442 394 360 307  23 440 271 481 302
 277 187  76 370 350 508 242 393 419  19 506 413 233 460  78  17 437 356
   2 401 148 173 215  21 417 306 334 520 223  54 221 337 282 435 497 518
 425 254  47 139  92 211 385 382 409 312 169 487 121 467 335  69  80 256
 246 105  75  16 236 457 412 459 150 383 273  44   7  18 374  35 108 280
  29 103 389 354 397  50 132 358 470 387 477 235 106 321 373 268 228 234
 421 154  87 272 496 345 444 422   5 362   0  93 433 265 327 448  33 501
  66  22 371 320  39 407 240 175   1 332 364 372  56 185 380 395 396 123
 241 526 322 450 377 456 156 454 239 368  96 434   9 491 527 524 152 486
  94 429 513 474 514  34 340 416   4 439 505 181  11  49 231 111 134  13
 523 367  14 178 225 141 355 446 248 365 324 174 426 431 489 226 157 194
  10  37  57 333 519  43 449 263 516   8 249 279 184 493 427 443 379 229
 359 274  12 224 363 484 432 110 261 428 483 227 441 308 507 330 471 200
 115 509 266 415 189  28  98  45 176 186 183 250 521 276 347 512 232 247
 237 408 245  38  95 381 172 447 418  27 369 420  36 344 482 207 258 257
 515  20 264 220 336   6 353 511  99 159 100 378 480 323 494 244 423 278
 238 281 177 517 161 398 179 452 406 375 424 414  90 430 283 445 222  41
 436  91 147  42 101 138]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2659
INFO voc_eval.py: 171: [2151  919  484 ... 1725 1728 1680]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1977
INFO voc_eval.py: 171: [83 82 65 66 70 76 79 57  6 55 72 81 74 56  3 40 48 80 35 64 41 47 16 75
 17  7 24 38 39 87 36 19 49  0 43 42 13 77 59  2 46 60 22 53 34 78 58 67
 71 21 26  5  1 27  4 45 86 33 68 20 18  9  8 32 23 12 28 51 84 31 52 29
 85 10 54 62 37 14 11 15 73 63 50 69 25 30 44 61]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0347
INFO voc_eval.py: 171: [271 325 326 ... 732  78  75]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.1450
INFO voc_eval.py: 171: [458 112 355 468 356   0 461 795   1   4 564 493 496 568 363 406 113 360
 485 231 193 632 492 225 771   7 200 776 295 772 300 466  40 567 835 416
 192 247 121 414 149 415 201 471 836 136 481 715 742 408 539 222 264  10
   5 407 606  41 469 178   2  18 498 503 482 470 294 633 126 242 170 262
 479 631 243  25 127 802 347 292 607 796 335 186 254 180 245 301 241 514
 600 171 839  22  73 140 146 125 418 410 175 566 196 165   8 484  49 109
 405 248 308 777 130 116 190 457 336 162 508 756  13 460 604 801 371 263
  34  23 659 255 240 302 303 480 681 386 340  39 164 773 103 145 139 181
 367 217 212 169 565  37 799 605  76 220 114 362 797 172 556 751 211 648
 297 354  98 813 129 516 123 622 253 118 637  92 403 390 366 510 174 251
 768 364 827 141 441 176 179 206 224 235 185 313 721 782 746 120  33 786
 455 409 137 216 474 841 599  78 512 641 478 717 221 132  80 339 229 703
 182 500  75 509 652 238 375 236 218 317 305 258 464 397 189 635 459  15
 252 383 486 490 785 456 184  97  54 656 168 572 150 579 779  14 100  11
 805 316 788 664 434 831 135 108 265 173 634 228  43  79 232 349 789 205
 436 761 385 400 541 412 270 194 521 244 483 716 684 411 784 814 378 377
 395 325 803 546  63 658 647 819 331 249  32 617 219 304 627 754 435 424
 359 213 259 732  29 142 694 346 261 166 591 447 807 744 439 393 767  45
 778 472 642  95 616 343 202 645 289  42 373 398  20 643 781 152 540 505
 197  19 506 580 588 267 487 226 445 404 547 199 665 444 110 515 829 534
 338 696 752 276 755 657 204 260 273   6 453 552 636 625 342 573 685 501
 720 672 842 223  72 246 499 706 693 271 775 513 593 832 131 119 467 463
  36 187  87 792 274 587 571 341 851 575 638 793 535 762 361 511 230 450
 161 774 321 227 345 489 352 757 392 808 448 602 497 817 812 257 524 821
 195 268 646 682  52 651 525 495 594  46  90 544  65  24 191 462 427 766
 348  12 488 800 237 550 324 543 719  50   9 686 357  86 592 825 239 389
 581 695 507 559 337 380 619 584 365 700 438 128 704 328 731 446 344 577
 314  27 214 443  17 518 649 333 312 422  21 413  82  88 465 519  16 532
 420 183 640 660 117 280 115 476 671 402 215 759 151 311 794 699 394 432
 502 662 288 783 473 475 504  35 454 177 558 763 188 320 210  67 628 764
  53 250 209 608 822 399 491 787 852 758 138 753 585 806 358 529 272 391
 697 266 838 282 673 760 844 603 517 327 256 275  91 734 387 379 384 623
 601 586 653 124 847 667 837 269 477 449 167 769 101 798 849 526 279 310
  71 629 780 554 401 598 698 203 733  38 323 574 291 296 702 770  62  94
 578 134 382 283 596 735 765 644 198 811 711  99 713 833 423 370 790 834
 791 669 440 615 723 614  48 332 290 809 718 624 208   3  70 626 542 545
 428  83  26 306 163 368  57 610 561  96  61 147 430 654 828 569 156 207
 520 670 388 729 570 334  77 848 426 650 589 433 298 329 709 234 425 553
 374 158 286  68 318 315 676 307 396 287 148 707 710  60 748 820 705 233
 620 816 815 745 278 133 826 548 677 727  74 326 322 750 576 437 111  51
 293 122 419 687 674 853 714 528 741 810 160 527 533 284 281 452 843 285
 143 722  28 451 538 330 277 417 582 106 824 299 701 661 319 818 823 372
 830 668  64 804 726 309 678 630 560  44  84 840 655 639 728 621 666 691
 583 730 551 725 105 537  30 159  47 154 563 845 157 724 523 369  31 740
 555 688  93 850 597 351  85 104 350 737 562 738 739 442 595 107 376 712
 536 531 494 609 421 590 690 381 683 530 736 353 743 663  66  69 689 557
 522 153 708  56  59 611 747 846  89 102 612  58 692 429 155 675 680 144
 613  55 618 679  81 431 749 549]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.2731
INFO voc_eval.py: 171: [1951  967  981 ... 1861 1394 2035]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0729
INFO voc_eval.py: 171: [ 77 280  51  57 398  53 106 251 253 291  61 203 111 285  70 414 407  75
  79  55  63  81 222 352 208 118 380 400 252 293 117  94 279 308 297 381
  62 224 209  84 286  36  59 403 210  52 115  60 309 406 109 283 397  73
 355  54 301 298 204 139 105  88 402 410  71 122 270 415 307  29 419 324
 304  38  65 249  92 196 113 265  76  35 149  82  93 215  67 220 413 382
 248 225 416 205  45 356  78 273  58 264 350 303  69 211 288 399 126 318
 228  85 261 383 191 255 226  40 116 332 100 129  87  15 412  74 144 125
 119  86 124 221 330  41 148  56 207 362 357 232 274  37 268 359 195 418
 110 302  80 384  89 133 281 295 202 233 201 423 169 292  64 170 183 194
 150 146 130 333 199 349 405 223 331 227 200 275 287  66  16 214 364  83
 294 411 134 346 141  47  90 348 197 338  95 395 314 296 427 234 315 237
 123 306 131 266 408 258 313 230 339 347 358 151 107  11 206 160 363 127
 329 366 271 152  91 417 312 327 317 128 216 158 422  72 256 174 320 219
 168 254 389 143 132 190 136 354  46  31 112 335 175 247 182 154 138 269
  98 337 262  68 231 386 193 360 361 121 310 140  33 385 198 213 394 388
  39 161 137 179 187 155 244 142 316 188 391 236 192 176 351 135 162 229
 153 321 250 114 284 166 189 336 245 159  24  43 172  50 260 387 431   9
 319 163  10  27 259 353 108 120 238 278  26 325 235 300 102  19 173 421
 328 147 368 212  18 323 420 104 101  14  25 430  22  21 326 164   0 379
 165  28 365  13 375 429  17 186 290 241 184 217 103 272 157 218 340 396
 393  20 433 322 177  48  12  30  23 390 277 425 392 376   7   5   8 367
 240 424 370   3 156 345 305  34  49 369 428  96 282 378 342 246 263 178
 435 344 167 371 377 401 374 171 145 267 372   4 289  97 373 239 343 426
 434 341  42   2 404  32 181 242  99 276 311   1 185 299   6  44 334 409
 257 243 180 432]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.1196
INFO voc_eval.py: 171: [223  62 274  69  61  63 125 226 103 407  90 225 229 517  98 514 280 252
  64 309 532  85 440 253 305 118 107  72 240 478 126  75 459 529 330 315
 420 227 264 515  83 642 304 276 524 318 567 124 325 277 130 493 236 288
 122 370 519 531  89 503 275 306 303 527 498 320 254 522  19 435 284  87
 328 650 480 577 533 332 255  97 467 456 482 114 508 433 228  25 628 687
 491  65 441 571 447 310  66  82  70 591 449 381 180 258 584 102 389 285
  71 568  30 457 286 116 281 183  73 165 336 261  74 416 302 671 319   6
 342 542 604 575 587 278 230 657 161 585 483 353   2  68 662  28 241 653
 263 106  20 119 338 647 237 291 654 334 452 340 426 442 692 287  80 262
 283 245 523 174 621 100 633 168 121 453 518 578 630 379 698 673 234 631
 627 140 326 540 235 374 593  11 343 246  67 520  81 200 616 292 486 371
 350 251 250 683 501 316 308  22 484 176 629 162 243 289 466 290 543 344
 696 464 421 177  79 659  34  96 367 481 391 111 231   0 634 639 345   4
  17  29 322 462 105 418 667 167 552 495 164 193 492 569 451 649 120 348
 656   9  78 356 697 471 669 129 131  24 641 579 151 424 507 428 664 613
 513 198 163 619 677 202 485 184 607 632 169 626 110 249 620 327 341 611
 242 625 392 504 666 127 561 572 665  76 260  27 323 623 460 150 239 136
 166 496 640 635 159 547  86 293 599 216 182 414 681 314 257 574 415 205
 695 694  15 411 615 178 538 581 511 259 430 282 375 622 580 207  91 516
 170 563 505 551 610 108 636 590 548 438 337 624 312 351 651 685 661 366
 171 499 279 383 333 431  33 455 113 219 377 224   5 179 402 321 373 668
 443 149 361 354 676 535 425  54  21 678 637 208 400 199 609 644 614  10
 352 592 331 448  55 317 646 147 382  38 217 358 360 652 185 311 346 209
 550 160 175  40 693  31 189 349 181 446 101 658  16 691 220 154 244 214
   8 576 672 638 419 689 432 674 109 376 463 233  32 699 313 191 218 679
 128 655 173 468 706   1   7 268 554 192 347 476 648 104 172 601 413 221
  46 663 555 558 145 212 541 660 598 528  26 339 680 684  36 670 596 682
 146 387 701 494 324 553  92 675 386 586 645 603 469 203 439 266 194 410
 502 461 215 139 530 359 526 488 380 256 144 368 417 686 355 612 201 705
 537 643 565  37 688 608 152 690 583 702  51 594 394 454 393 267  44 378
 155 390 546 475 544 445  84 401  99 296 363 617 385 196 142 269 138 297
  56 560 423 273 300 195 197 707 497 564 211 399 557 450 573 148  23 490
 509 270 427 398 602 582 525  13 470  57  49 600 477 157  14 301 703 265
 465 362 549 247 123 369 434 298 307 158 588 605 384  95 700 405  12 570
 429 204 536  50 186 506 153 472 372 388 335 248 562 409 545 473 137 595
 534 704  41 406  77 135 112 299 206 397 618  94 133 295 444 559   3 294
 190 404 408 539 132  43  48 412 117 597 396 556 210 403 437  39  53  58
 141  45 134 272 709 143 436 232 474  93 566  35 271  60 213 188 187  59
 512 487 479 589 115  52 329 500 422 458 357 222 395  88 489 364  47 156
 510  18 238 708  42 606 365 521]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.1667
INFO voc_eval.py: 171: [ 96 250  38 249  53  97 227 276 260 186  62 277 215 174 137 195 100 194
 110 183 109 196  35  57  74  61  39 148 112 168 288 207 167 116 136 169
 140 259 156 226  69  63 164 255  52  73 262  46 232  36  21  44 254 251
 279 119 219 257 220 199  80 125 191 163 252 102 221 263 111  64  70 130
 266 115 144 283  33 235 264 135 274 166 185 139 189 239 256 285 190 213
 121 129 132 230  20 153  56  23 105  41 228  88 143 242   5 175 269 146
 141 171  18 178 245 179 233  90  42  82 122 286 278 218  87 270 158 214
  40 217  86 127 246  84  25 206  99 155 282 287 124 123  16 272 145 280
 184 170  55 229 161  65 159 240  54 103   8 265  17 165  28  68 104  77
 118 117 237 134 126  98 238  22 114 234  71   7 150 120  26 281  43   2
 113 253  27  51  10 212 131  89  47  37 133 177  72  19   4 101 241 267
 224 268 172 182 216 208 151 243 225 295 209  30 197   0 149 108 211  79
 173 107 291  31  95  81  83  24 128 106 284  59 181  66 296 205 154 236
  91 152 201 188 210 294 176 203  75  76 222 223   3 200 180  13 289  94
  58 193  32  85 231 292 162 160 198 244  14 157  67 275 261   1 290 192
 271  45 273 248 142  78 138 258  48  60   9  11 187 204 147  15  29  93
  34  50 247  49   6  12 202 293  92]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3361
INFO voc_eval.py: 171: [10758  5003 17222 ...    58  4147 13985]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1964
INFO voc_eval.py: 171: [144 442 417   4  63 142 289 276 435 416  55 381   7  48 395  58 433 441
 420  52 397 121 434 310  56  59 436 162  47 256 286 102 305 454  49 325
 404 145 447 357 375 308   6  69  94  57 314  42 147  43 386 362 278  66
 331 384 290  75  45 210 327  12 459 141 414 281 306 326 226  13  21  79
 304  17 143 415  41 424 367 398 115  44 265 313 251 224  39  89 161  78
 399 389 239 193  20 242 382 206  60 319 200  91 167  61  97 280 346  80
  71 140 174 188 284 137  72 440  67 230 258 460  35  10  92 303 334 390
  64 429 320 360 432 163 385 233 293  53  36  51 394 401 156 301 165 332
  40 423 380  46 387 190 123 413 430 229  70 350 259 438  23  98  88 260
  68 106 366  37 311  38 376 255 268 288 131 407 252  65 241 333 227 329
 108  73 383 211 317  96 118 359 114 209 378 119 124 138 365 427 403 192
 160  99 139 246  34  76 321 225 410 341 157  95 154 214  50  25 155 175
 148 421  77 277 315 177 201 377 178 451 309 182 448 271 128 250 349 228
 444 287  84 388 428 285   8 267 247 379 431  24 283 169 262  85 100 302
 204 396 351 112 126 146 318 425 152 166 340 450 292   3  18 443 170 120
 371 279 245 221 223 198 261 353 275 117 269 426 238 368 149 458   1 300
 361  33  16 391 402 218  86 373  54 312 159 342  62 356 446   2 323 180
 409 194  19 456 364 406 232 374 103 282 418 294  90 153 408  93 307 208
 179 273 295 363 369 125 249 372 244 370 445 358  15 222 189 234  82 253
 455 291 220  14 127 248 345 133 181 328 207 172 392 330 316 219 297 348
  30  29  87 272 296 187 231 298 129 344 339 101 393  27 266 337 453 168
 411 437 199 270 186 184 322 105 183 213 419 195 264 196  26 405 216 215
 343 236  74 354  81 452 104  31 158 412 132 151 130 134 113 299   0 116
 191 338 171 205 109 185 135 203  32 107 449 240 324  28  83 110 257 422
 439   9 202 263 111   5 197 336 164 217 173 352 457 235 400 274 136 150
 237 243 347 122 176 254 212 335 355  22  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1820
INFO voc_eval.py: 171: [417 182  24 258 167 418 178 438 260 168 169  33 189  26 656  77  23 644
 568 442 638  39 615 100  54  30 421 195 161 183 495 131 673 360 480 265
 616 426 391 338 448 565 121 429 669 621 334 327 552 660 137  35 196 348
 267 185 508 266 691 556 386 337 570 284 663 555 577 518 344 475 107  87
 449 200 496 665 397 124 186 576 135 147 171 478 365 567 105 330 453  45
 188 166  38 579 125 399 554  34 626 184 116 261 657 623 630  10 153 618
 443 619  90 103 371 649  40 181 173 590 273 667 389 372  57 445 507 347
 373 127 271 101 123 269 146 268 355 666  84 549 490  86  73 274 668  70
 471 645 633 409 436  76 375 634 133 132 686 361 388 540 543 420 207 384
 592 595 465 356 444 690 477 151 531 315 679 165  31 170 692 369 197 140
 622 640 296 108 272 393 671 553 329 392 203 310 515 479 561 362 193 672
 112 378 277 325 374 172 546  97 662 647 674 335 446 141 457 402 287  27
 574 431  32 341 609 625 659 150 276  29 128 209 506 351 617 263 155 352
 419 699 696 550  72 502 435 661  88 403 262  16 664 264  42 642 476 115
 427 485 520 270 379 528 620 670 225 383 339 404 145  99 259 236 350 114
 281 353 400 104 503 275 311 376 216 163 611 636 408 569 697 192 587 410
 559 109 433 593 142 292 631 363 683 202 483 596 578 328 342 119 650 199
  56 113 675  12 530 381 566 148 643 473 563 511 708 523 111 606 581 144
 285  81 289 703 504 279 396 324 367 498  20 226 658 336 682 472 354 129
 295 291  89  65 156  36 218 385 434  11 693 573  74 139 624 605 519 684
  80 299 525 136  66 106 637 318 294 210 646 333 157 413 349 407 394 562
 288 366 331 340  92 304 256 603 505 234 572 564 290 428 130 309 316 604
 571 447  41 390 159 286 539 461 424 688 695 524 149 138 405 651 677 237
 412 190 220 382 235 346 377 177  69  44 406 694 467 584 466 456  55 387
 312 486 414 158 452  17 430 488 416 710 343 110 702  98 345 117  62 364
 500  82 678 423 187 293 162 332 459 122 152 191 164 583 680 238 551 632
 580 160 560  83 303  14 211 317 370  91 143 492  53 380 358 326 126  50
 118 538 422 253 301 212 685  46  28 250 120   8 535 709 482 600 612 654
 228 676 468   2 641  71 306 213 305 280 487 460 102 217 614 681 252 411
  43 474 687 601 537   0 134 319 205 497  21 493 698  22 455 154  93 254
 223 231 314 532 501 395  52 297  18 526  64 230 653 700 308  25 357 514
 536 313 509 440 689 548 481   7 545  67  85   5 464 458 707 176 240 470
 174 222 229 415 320 208 227  75 510 232 180  48 398 529 610 282  61 557
 359 368 648  96 401   3 450 542 512 198 547  49 627 432 437 598 255 521
 522 247   1 239  94 711 439 425 489 558 244 194  51 613 441 652 463 257
 607 307 469 527 179   9 219 701 278 245 302 585 589 513 462 321 582  13
 204  37 704 534 494 516  47 233 588  95 517 499 484 629 241 215 544 298
 246 451 628 249  60   4  68 454 586 655  79 242 323  58 322 705 602 206
 214 491 635 300  78  19 221 599 251 248 224  59  63 597 594 639   6 201
 608 575 706 533 541 283 243 591  15 175]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.1669
INFO voc_eval.py: 171: [ 87 258   0 336 340  58 166  85 227 251 269  59 186 114 348  89  60  15
   1  35  99 185 352 350  43 339 106 305 361 190 237  33 291 226  90 345
  68 102 238 119  18 116 264  38 206  64 225 180 287  13 298 354  45  63
 262 115 365 196 255 360  54 270 241 169 307 266  74 230 126 283 228  17
 173 112 125 314  40 123 293  41 367 202  36 184 396 183 285  11 343  37
 300  42 170 358 109  97 232 240 369 306 175 272 320 253 245  98  95  75
 231 319 152 309 294 268  21 124 395  92 279 370 110 236 172 219 344  16
 308 338 249  88 137 244 393 204 303 399 267  26 323 177 346 167  93  78
 216  12  76 403 326 242 200 372  62 208 347 139 349 222 145 342 151 385
 363  29 201 229  66 134   8 248 271 337 302 195 234 297 288 407 220 296
 379 386 290 138  86 233 239 375 104 289 118 316 144 120 194 295 122 257
 188 246 299 278 235 250 388 155 132 121 265 142  20 390 371 394 218 247
 211 382  48 357 140 392  61   3 156 243   5 389 107 141 413 341 310 191
  44 383  14  69 210 275 366 146 153 292  71 362  23 368  31 213 324  46
 378 328 103 254   4 163 356 193 311 400 199 108  72 359 263 355 148  34
 212 325 150 351  22 181 312 364 209  30 333  25 397 179 391 260 398 174
 377 224 252 313 131 259 284 182 273  79 101   2 401 281 331 315 154 143
 135 402 168 301 165  55  73 105  27 353 335 207  84 408  70  10 321  65
 411 329 128  52 322 205 160  51 111 158  67  19  24 409 261 217 280  94
  80  57 404  50 412 129 277 282 330  81 100 159 187  28  56  77 387  96
 149 373  53   7 384 192 223 130 256 127 410 136   6 189  47  82 117  49
   9 147  91 334 133 176 161 406  83 380 113 381 332  39 221 318 317 178
 327 171 157 374 164 274 198 304 197 286  32 405 276 203 162 215 376 214]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1662
INFO voc_eval.py: 171: [ 76  72  79  73  74  16   5  17  89  71  22  63   1  83  15  92  91 122
  23  75  10  64  56  13  60  49  20  34   3  82  98  80 118  58  57  77
 107   0  59  86 120  51  84  18   9  90  43 110 117  37  40   4  27  95
  93 116  50  12  19  54  28  97  55 108  29 106  78  53 113 125  31  46
  30  70  94 124  81 104 111  62 103   8   7  39 121  21  66  11 100  44
 128  48  32  52 102 112  96  68  35  24   2  61  26   6  14  87 126 109
 115 119  65  33  42  99  45  38  88 105 114  47  41 123  85  36 101  25
  67  69 127]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0651
INFO voc_eval.py: 171: [145 153 122  31  85 152  75 180 134 142  30  80 148 137 166  56  33  78
  18 117   0  44 138 185  72  94  43  32  83   3 124 157  70 181  37  10
 193 194 127 118 173 183 128 179 131 103   7 135  36  92  22 201 172 164
  77 182 192 129 143  59 156 144  41   5  28  87 109  90 141 123  81 159
  55 149 202  69 171 161 158  14 197  42  86 116  11 155 175  58 169  29
  13  39  96  89 147  19  51  76 154 196 187  45 189 160  60   2 107 198
  97 126 132 119 199 184  61 113 162 140  38  91  16  15 190 130   8 108
 150 146  40  49 167  12 195  66 186 121  68  46  79  73 200  74 106 188
  63   9 163  20 165 105 191  50 178  71 112  26 136  67  17  21  98 100
  57 139 176  48 101  35   1 133 151   6  25 170 120  84  54 177  93  24
  52  34  27  64  95  82  62 111 168  88 125 174 104   4  47  53 102 110
 114  65  99  23 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0975
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1685
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.161
INFO cross_voc_dataset_evaluator.py: 134: 0.444
INFO cross_voc_dataset_evaluator.py: 134: 0.107
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.119
INFO cross_voc_dataset_evaluator.py: 134: 0.266
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.035
INFO cross_voc_dataset_evaluator.py: 134: 0.145
INFO cross_voc_dataset_evaluator.py: 134: 0.273
INFO cross_voc_dataset_evaluator.py: 134: 0.073
INFO cross_voc_dataset_evaluator.py: 134: 0.120
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.336
INFO cross_voc_dataset_evaluator.py: 134: 0.196
INFO cross_voc_dataset_evaluator.py: 134: 0.182
INFO cross_voc_dataset_evaluator.py: 134: 0.167
INFO cross_voc_dataset_evaluator.py: 134: 0.166
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.098
INFO cross_voc_dataset_evaluator.py: 135: 0.169
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.624s + 0.003s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.491s + 0.002s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.529s + 0.002s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.529s + 0.003s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.531s + 0.002s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.531s + 0.003s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.530s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.532s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.530s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.527s + 0.003s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.515s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.506s + 0.003s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.499s + 0.003s (eta: 0:00:02)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.694s + 0.002s (eta: 0:01:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.566s + 0.002s (eta: 0:01:04)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.572s + 0.002s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.551s + 0.003s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.556s + 0.003s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.548s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.561s + 0.003s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.556s + 0.003s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.558s + 0.003s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.550s + 0.003s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.536s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.524s + 0.003s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.513s + 0.003s (eta: 0:00:02)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.707s + 0.002s (eta: 0:01:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.539s + 0.005s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.538s + 0.004s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.558s + 0.003s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.548s + 0.003s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.546s + 0.003s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.531s + 0.003s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.533s + 0.003s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.530s + 0.003s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.522s + 0.003s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.510s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.498s + 0.003s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.494s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.740s + 0.002s (eta: 0:01:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.503s + 0.003s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.514s + 0.002s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.538s + 0.003s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.530s + 0.003s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.521s + 0.003s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.519s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.515s + 0.003s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.518s + 0.003s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.511s + 0.003s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.501s + 0.003s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.491s + 0.003s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.484s + 0.003s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 77.142s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [120 116  54  63 103  55  89  53 114  59  94 308  90  96 291  60 269  52
 178 248 105  91 305  73 194 322 256  69 185 109 304  79 404 262  75 266
   4 101 294 314 329 280 255 331  35 242 184 104 158 234  57 225  81  77
  58 197 138 330 310 140 160 199  68 173 311 332 370  61 183  11 378  13
 411 263 300 292 363 179 121 230 379 328 126 142 201 366 407 383  86 235
 387 391  17 321 373 317 307 355 406  70 309 108 401 107 282 223 231 365
 188 316 100 149  95 405  40 259 191  51 270  83 351 182 141 376 200 412
 165 313 368 139 198 159  23 296 233   1 306 128   8 180 384 215 325 320
 162 271   7 218 274 250 258 238 222  26 228 115 354 224 124 203  80 196
 137 192 295 146 318 106 335  72 398 326 377 170 261  33 324 408  67 177
   3  74 279 323  64 163 148 118  99  84 349  78 119 334 333 372 390 144
 227 102 257 217 347 169 260  98 275 288 375 168 112 151 110 278 402   9
  65 336 276 299 155 410 277 367 210 267 388 143  18  16 122 369 403  29
 350 244 123 359 221 195 136 172 284  76 189 293  97 342 186  71  62  85
  30 357 302  20  25 352  82 358 211  88 152 150 153  49 241 327  66 161
 285 129  41  39 243 246 145 346  36 393 240 268 207  50 339  92  87 386
 253 147 247 212 245  15 361 132 264  46 130  44 232 166 409 208 113 237
 249 344  19 174  48 413  93  21 111 220 209 171  22 190 397  56 206 356
 117 127 301 239 265  28   2 394  12 364 236 154 214 213 400 385 272 164
 229  14 389 374 205 340 254  45 303 251 167 157 202 312 156 414 392 360
 131  10 204 396 399 125 315  34 345  31 395   0 371  37 216  24 252 348
 362   5   6  32 353 219 415 273 193 341 226 297  42  38  27 290 338 298
 319 176  43 175 133  47 337 343 286 289 181 281 382 134 287 381 283 187
 135 380]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0222
INFO voc_eval.py: 171: [ 39  98 110 244 238 246  93 240  83 230 117  53  57  40 170  89 216 221
 195 100  42  38 112  91 247 119  56 130 111 143 190 127 102 243  55 142
 245 219  54 113 231 156 167 123 126 234 144  96 218 148 220 228 158 136
 118 242  52  95 129 217  48 121  41  23 125 225  65 193 200 163  99 229
 135 140 115  88 178 233 227  50  51  37 137 235 189 187 194  58 172   8
 157 237  90  21  62 155  22 159  49 199 152 224 198 241 153 122 114 124
  24 174  87 160 171  75 223 147 236 239  60 166 192 177 215 222 151 206
 226 146 116 131 138 208  63 232  94 161 141 120 176  44 150  59  45 134
  79  84  76 181  20 149 209 175 207  85  43 183  47 202  46  66 162   7
 145  92 180   9 196 128 103 104   5  30  18  86 139  97 186  78  74 205
 109  25  31  12 210 173  13 168   3   4 211   2  80 214 165  73 185  64
 213 203  26   6 188  70 132 106  27  35 182 154 164  77 133  72 212 191
 101 197 107 204  67 184 201  29 179  15  36 169   1 108 105  68   0  81
  28  17  16  19  71  11  82  32  69  10  14  33  61  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.2403
INFO voc_eval.py: 171: [1980  408  683 ...  125 1370 1326]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0341
INFO voc_eval.py: 171: [ 948  435  817 ...  569 1232  585]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1183
INFO voc_eval.py: 171: [221 392 193 181 114 160 544  95 400  94 199 112 294 290 339 514  91 212
 162 363 394 391 219 473 347 352 182 386  87 203 195 493 201 398 337 179
 176 156 359 360 348 334 167 220 297 518 365 344 161  88 353 480 166 305
 481 102 270 285  92 399 280 511 466 308 527 200 524 194 338 103 356 164
 230 397 104 276 345 381 477 153 384 175 144 515 534 227 510  99 331 342
 499 207 222 351 302 545 387 333 330 469 293 296 311 142 215 367 349 516
 240 168 433 155 205 117 505 301 124 185 343 277 460 163 341 225 226  98
 395 373 115 184 228 131 191 479 283 393 134 464 141 148 217 366 292 390
 484 189 488 478 320 358  28  89  97 192 133 355 487 520 216 376  96  93
 275 504 235  85 178 101 396 385 256 204 157 450 379 372 128 158 361 245
 249 374 310 523 169 378 116 289 237 171 299 208 375 325 224  23 136 247
 170  30 346 152  22 109 211 300 322 304 151  32 335 213 389 244 273 340
 100  64 319 529 531 368 415 336 177  41 214 223  86 253 143 414 218 357
 332 180 236 540 106 272 188 242 425 383 371 382 172 486 282 549 447 309
 110 108  24 443 475 491 196 426 409 350 513 307 411 496 243 198 269 547
  46 233 380 503 530 209 471 107  33 146 232 313 229 287 125  79 410  25
 291 118 159 369 404 364 122 165  15  26 490  18 174 183 354 173 428 206
 312 197  84 362   5 264  65  83 252 241 457 135 501 298 506 248 238 449
 446 145 239 388 210 502 370 303 129 412 419 437 268 130 315 279 377  10
 288  34  57 423 314 105  27 485 231 438 190  90 113  44 427  19   9   2
 278 521 526  81 472 537 258 261 406 251 295 263  56 123  80 416 202  62
 548   6 140 408  29 451  11 429 111 266 439  13 317 500 421 482 403 489
 468 321 525  12  73 420  36 138 324 326 407 234 260  63 442 150 147 255
 139 454  58 318   8  39 137  38 512 274 259 250 532 535 267 257  40 517
 405 262 458 246 509  45 546   1 281 149  54 271  53 286 539 254 424   0
 543  16  78   7 284 463  50 265  69  48 538 154  35  66 328 431 432   4
 127 465 519 497 470 445  49 323   3  37  52 462 459 413 494  75 542  43
  68 187 453  61 435 461  59  72  74 441 507  14 132 126  55 522 329 455
 327 508 528 402  17  20 316 448 456 483 541  51 417  67 422  42  60 536
 120 401 476 306 444  31 434  82  21 498  71 452 533  47 440 436  77 474
  76 121 430  70 186 492 418 119 495 467]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0548
INFO voc_eval.py: 171: [181 109 129 163 160 126 108 161  23 124  73 125 159 128 169  67 170 155
  19 177 117  63 118  33 172 180  25 151 178 119 149 174 114  54 185 182
 153  71  47 120  68 168 131  69 116 173  20 189 150 113 122  43  64 167
  84 183 104 179  76 111 107  45  10 171 186  72  17 143  66 132  55 191
 144 133  48  80  40 136  70 154 166 162  59  87  92  41 135  74 213  78
  88  38 164 148 156  65 196  42 103  44 190  82  56 145  90  28  57  98
 204 192  49 142  83 205   5 127 110 202 184 115 195 106  13 123  14 175
  46  61 187 188 152  94 214 158  96  77  27 141 194  35 212 207  79 176
 140 130 198  75  99 193  85  60 112   2   8 146 137   1 102 165 105  15
  21  97 206  39 134 121 101 208 100  86 200 197 201 199  95 147  51  16
 209 211  81  29 157   9  30 210   4  36  91  26 203 138  22  11  89  31
   3  37  18  58  34   6  53  32  62  93 139  52   0   7  50  24  12]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3170
INFO voc_eval.py: 171: [2093 1567 1500 ...   96  256 1430]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.1116
INFO voc_eval.py: 171: [144  24  21 140 123 145  80  29 111 127 139  77 116 146  17  26   7 122
 142 148  27  74  23  41  12  33 141 135  78 128  10 125 109  16   6  20
  40 138  75 124 115  11  28  31  32 131 117 114  39  48  30   3 143  91
  59 110   2 147  14  76 119  38  13  19  61   8  79  73  15 120  42   5
   4  22  60  56   9 137  57 136 112  98  18  35  99  67 100 118 106  25
  51  53  46  93  37  97 108  64  90  96  34 102   1 129 105 132  52  63
 130  95  88  62  65  85  68 103 133  49  89 121  69  87 126  44  55 107
 104  81  92  58 113  50  54  86  70   0  84 134  71  66  82  83  36  94
  72 101  47  43  45]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0283
INFO voc_eval.py: 171: [ 576  586  589 ... 1938 1846 1411]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0855
INFO voc_eval.py: 171: [ 80  49 315 333 474 221 436  52  62 326  64 314  69  51 240  74 304  98
  83 113 123  66 463 244 122 428  95 100 112 451  67 426 313 368 336 328
 462  72 104 158 468  99 182  70 101 220 452 303  50 183 103 156  61 229
 248  91  84 311  81 422 455 323 324 116 111  57 483 317 124  29 142 223
  68 130  12 537 175 361 316 371 249 331 325 340 145 185 166 146   4  94
 432 102 121 476 378 105 362  90 320 396 231 358  88 456 389 180 161 137
 475 131 450 538  24 337 138  32 433  71 143 152  75 159 227 473 109 135
 549 305 339 332 129 128 165 110 107 276 548 460 259 423 242 148 169 321
 117 407 120 106 342 144 258 467 132 173   1  97 234 147  96  23 466 418
 171 397 167 164 465  15 232 550 363 564 172 163 222  56 553 530 373 237
 108 170 542 306  58 181 374 168 272 471 268 520  26 149 179 119 482 178
 398 187 245 280 196 357 566 154 150 406 174 294 402 195  92 533 512  54
 430 563 472 126 236 392 118 157 256 318 312 139  27 364 341 239 115 439
 532 134 334 453 401 176 543 162  53  60 412  43 186  10 352 376 511 207
 355 421 420 319 136 547 253 255 448 133 554  93 544 302 155 285 457 479
   2   3 514  78  89 213 567 327 193 114 140 424 241 141 449 151 286 438
  13 264 559 269  37 322 184 447 127  19 287 484 293 513 160 254  82 153
 387 427   0 470 573 329 270 190 215 551 434  22  47 219 267 263 409  31
  30 177 354 125 443 335  42 205 556 309 262 189 217 525 431 459   5 204
 419 555 226 435 218  73 370 307  17 379  79 247 518 429 338 425 384 572
 277 571 201 490 413  38 310 388 198 557 570 562 534 522   6 377  16 192
  63  41  28 509  85  45 200 297 510 395 393 568 330 308 404 282 212 529
 399 478 405  87 369  44 400  33 521 403 274 489 391 394  36 461 558  34
 415 283 299  48 531   8 206  65 284 546  35  40  18 289 209 208 238 191
 360   7  39 290 517  46 485 211 569 437 214 203 487 292 252 250 210 233
 545 469 296 458 366 574 535  76 271  11  86 216 454 416 194 228 202 540
 539 281 278 291 480 486 243 446 481 552 199 561 526  20 565 464 197 560
 359 541 477 295 441 353 411 348 445 349 528 508  25 536 266 351 527 488
  14 279  77 500 516 273 524 350   9 523 382 519 444 265  21 343  59 301
 188 372 383 386 344 225 251 288 390 515 497 230  55 494 502 440 505 345
 491 385 414 298 417 408 275 300 375 442 495 410 367 493 235 346 224 380
 261 499 260 506 503 507 246 365 257 356 498 492 347 504 501 496 381]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.1400
INFO voc_eval.py: 171: [734 355 378 840 354 380 421 512 410 356 811 730 685 379 681 753 164 400
 406  95 387 419 384 834 812 729 394  96 100 751 746 383 754 915 140 413
 696 676 250 677 357 390 375 404 665 750 515 839 474 156  98 688 667 902
 136 490 736 707 853 363 365 251 897 104 393 392 745 672 717 733 666 532
 749 414 397 101 502 376 360 697 824 373 412 408 160 106 826 694 374 891
 371  28 607 739 906 422 619 894 712 823 825 701 747 501 714 147 679 689
 248 108 929 472 151 814 700 366 822 377 488 114  51 856 111 863 684 890
 737 828 727  27 738 832 668 725 813 718 621 103 361 817 698 719 670  99
 796 110 818 470 109 276 624 810 154 528 710 942 558 608 418 692 889 391
 900  43 636  56 498 930 287 892 941 715 278 731  55  26 887 708 495 778
 742 530 683 862 741 353 153 744  31 138  49 703 115 640 732 893 646 927
 695 799 272 302 623 500 610 273 952 112 561 797 455 649 780 161 247 644
 910 398 911 815 509 221 939 107 382 595 541 713 937 635 888 748 896 275
 674 899 682 364 756  59 372 211 538  60 935 795 852 774 395 359 821 632
 932 829 537 627 516 113 453 580 559  52 288 407 529 617 793 740 289 728
 171 611 555 628 898 102 556 704 671 612 282  67 808 675 491 176 606 643
 835 904 637 669 820 531 723 249 388  33 286 401 859 651 279  70 827 415
 105 568 389 554 699 381 504 711 722 212 399 442 243 936 511 505 534 800
 743 246 396 420 752 726 264 231 693 368 370 560 131 188  10 465 256 575
 925 928 591 477 705 283 764 386  72  30 252 673 328 513 916 369 947 267
 702 686 622 901 765 520 625  50 489 535 600 735 277 816 680  21 284 557
 385   0 338 281 280 833 301 706 259 321 417  29 664 285 215 144 214 716
 838 225 938 120 760 895 158  97  46 919 802  73 493  42 308  15 416 924
 597 857 238 709 239 447   9 720 121 798 876 858 690 923 629 227 860 216
 603 464 305 724  63 274 759 469 768 836 460 492 653 578 524 630  71 122
 773 403 135 174 292 536 367  25 411 721 159 125 405 809 475 172 480 539
 837 819  68  45 908 497 148  57 352 330 205 687 306 362 618 213  74 270
 451 830 173 143 792 139 553 132 903 662 663 616 402 337 587 409 265  32
 232 358 471 452 123 831 145   3 200 169 691 189  34 590 507  84  75 909
 130 242 794  35  78 920 861 864  81 437 141 581 918 567 445 170 678 118
 510  37 806 789 548 540 791 648  62 448  90 291 626  38 206  64 913 943
 912 207 185 458 208 294   7 181 599 224 582 133 344 905 801 134 652 803
 609 882 192 175 854  48 298 785  88 633  47 946 346 569 300 334 163 868
 921 517 758 855 931 933 508  41 917 533 210 463 777 229 781 443   8  76
 177 349  86 949 804 182 155 162 503 589 631 940 253 518 307 209 656   5
  69  85 934 223 579 149  39 521  36 944 790 117 303 124 296 461 525 620
 584 313 331  77 439 254 219  66  65 614 763 157 127 304 907 240 260 348
 566 642   1 466 914  14 805  87 424 660 634 444 316 190 542  83 449 487
 494 178 230 519 850 779 883 266  24 761 269 788 184 951  11 654 339  23
 615 342 499 641  20  58 128 522 199 757 319 945 293 574   4 775 613 255
 846 526 459   2 650 496 446 506 191 310 922 514 571 755 186 166 572 312
  80 544 848   6 467 198 228  61 873 585 659 594  93 325 871 786 658  17
 318 137 295 180 329 222 807 877 217  94 168 562 543 842 427 586 261 843
 565 314 233 602 150 926 433 479  40 317 129 866 657 343 950 297 456 290
 347 116 309 126 220 527 639 195 570 299 187 194 236  79 341 218 849  22
 350 847 782 604 881 193 783 563 770 844 784 605 258 226 564 311  13 262
 851 457  19 885 661  92 119 142 874  44 441 152 596  89 324 845 167 203
 598 769 438 766  53 268 546 592 655 323  82 454 593 880  16 948  54 315
 473 482 235 327 340 201 351 638 434 263 476  91 645 202 771 436 647 183
 787 486 320 332 462 867 146 485 484 245 345 865 234 241 428 481 767 588
 875 601 237 483 841  12 333 179 583  18 322 326 430 271 450 869 550 478
 257 878 197 244 762 884 872 204 576 426 776 577 772 870 468 573 335 523
 431 196 551 552 547 549 336 440 886 423 425 879 432 545 429 435 165]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0983
INFO voc_eval.py: 171: [ 84  41  60  68  70  61  81  65  50 290 196 200  52 354 350  64  34 361
  67  38  73  79 287 357 299 352 258 180  88 344 194 226 228 122 261  57
 363  46 250 288 284 301 283 178  35  72 310 239 285 123  92 355 353 293
 188  58  36 204 102 187 195 227 243 308 238  93 294 252 360 231 172  74
 249 259 233  94  49 345 211 203  91 307 356  44 348 362 286  89  90 130
 296 319  80 114 230 281 190 104  25 189  98 309 186 213 244  76 242 300
  63  75  55 251  47 201 245  42 255  51 124  45 198  87  71 220   4 321
 100 140 295 184 177  82 150 141  28 145 292  54 254 108  56  23 269 298
 235 103 349 327 121 297 276 322 332 138 210 217 330 173 181 338 110 219
 347 139 185 253 218  69 115 137 132 179  48 335 334 105 111  97 126 199
 183  77 136 273 197  37 313 142 174   6 222 325 278 131 339 164 351 257
  66 236 277 241  59   7 289 148 341 151 128 143 302 305 161 270 343  62
 155 116 202  11 240 306  40 323   2  43 326  33 127 291 324 279 358  22
 182 340  96 125 216 209 365 262  19 112 256 109 168  27 317 280  21 303
  26 160  83 260 237  13   5 328 205 175 176  39 320 154  14 101 318 232
 337 336 331  15 266  12 272 333  24 346 221 134 129  17 169 113 329 274
 342  99 135 229  16   9 314 212  53 106  10  18 133 304 119 107 191  20
  78  29 156  95   3   1 234 215 214 170 152 315 271 264 312 268 171 263
  31 144  32   0  30 282 159 265 311 192 163 162 147 117 118 359 120 146
 157  85 193 165 158 167 316 364 153  86 208   8 267 149 275 247 166 206
 225 248 223 224 246 207]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0534
INFO voc_eval.py: 171: [189 188 374 196 518 191 255  77  88 171  74 272 173  76  79 371 152  36
 498  53  85  81 193 237  52  60 226 512 383  84  40 328  50 315 172 163
 483 495 320 515 367 442 174 155 411 178 377 165 194 236  82 414 195 356
  48 360 185 169 376  38 392 408 182  39 180  73 412 351 499 153 160 151
 243 252 511  58 492 168 150  71 216 319 254 218 508 339 120  61 273  87
 221 109  42 157 259 246 444 305 501 386 314 493 361  44  41 488 267 127
 381 250 497  63 480 335 117 405 154  72 106 159 468 348 462 464 263  83
 395  57 459 210 238 288 400  90 341  65 217  43 406 429  37 503 342 276
 355 239 362 313 181 325 409 384  56 437 103 354 398 333 222 490 520 125
 417 465 316 394 212 475 220 496 158 337  24 378 450 192  46 266 470 431
 334 104 306 213 402 418  92 285 452 487 215 271 329 121 224 428  93 187
  64  55 494 343 340 111 506 324 115  70 455 223 416 344 219 500 265 507
 287 119 397 227 214 486 509 312 517 445 380 401 424 274 211 476 161 471
 244 451 184 456  51 510 469  91 399 460 338 368  47 251 175  94 308 436
 116 484 415 461 290 300 281 458 167 164 366 427 440 485 350  68 257  86
 241 108 359 105 441 326  20 225 491 379 413 426  21 294 304 203 128 393
 430 479 439  78  54 505 357 502 256 474 183  22 358 275 370 407 346 118
 123 332 249 253 489 112 477 110 453  80 396 204 369 129 353 202 454 124
 198  96  23 403  33 457 364 261 170 289 422 317 209  75 270 269 179 166
 482  67 514 504 448 467 347 438 323 134   8   5 293  17 443 404 208 114
 372 200 425  13 190 435 235   0 473 521 197 516  95 247 432 199 205  10
 420 296 140  45  89 206 207 177 421 113  62  99 148 137   9  59 186 345
 391 519 262 297 126 322 419 282   4 278  15 142 302 107 330 291  29 122
  30 423 279 284  98 283 375 463 365 513 147  31 242   3 336 349 292 410
  26 527 331 139 201  28 321 318  49 156  69 135 481  18 525 472 466 176
  14  32 298   1  19 280 478 136  11 299 522 145 390 138  66 286 387  12
 524 277 295 526  27  97 301   6  25 373 101 260 258  34 388 385  35 141
 523 264   2 102 144 234 382  16 303 268 228   7 447 363 389 352 100 311
 133 248 162 143 433 132 146 449 446 232 434 245 327 307 229 230 240 130
 231 233 131 309 310 149]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0340
INFO voc_eval.py: 171: [186  67 111 144  28 109 139 112  71  88 119 192  77  79   9 213 208 115
 182 135 196  76 103  87 199 205  96 126 171 207 218 185 124 120 142 212
  25  31  90  93 184 146 187  19 188  57  70 117  80  36  27   5  97 116
  14 166  29 211 183  78 202 127 217 110  12  48  45  13 204 136 215  34
 209  72  86 195 105 140  98  75  35  85 132 193  23 137  39 114 145  22
 173 210 194  73 191 197  20 118  68 203  81  53 123   8 149 143 164 163
  84 125 131  50  40 104 157  89 219   7  37 100 198  99  92  21 108 190
 133 122 107 158  94  91  60  26  54  47 147  17 113 141 170  82  11  38
 206   4 214 130  15 102 200  49  69 201 162 177 167 189  95  16 160  62
  46  83  10 121 178 129 101 176  74 175 216 180 220  30 168  18  32  24
  43 128 138 134 181 179 165 159 106  56 174 156 169  58 152  55   6 172
   1  44 148 150 161  59 155  66  41   2 153  61 154   0   3  51 151  64
  52  33  42  63  65]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.3285
INFO voc_eval.py: 171: [13098 17352 23214 ... 22059  7807 13483]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1570
INFO voc_eval.py: 171: [ 875 1162 1186 ...  675 1033  402]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1622
INFO voc_eval.py: 171: [ 84 304 101  85 117 218  95  93 308 265  72  88 213 433 118  92 289 432
 106 311 298 216 254  48 220 100  29 214 224 219 343 227 326 463 103 233
 259 467 397  33 247  89  52 243 246  86  55 238 473 404 334 296 492 448
 391   6 488 262  81 305 237 333  42 392 486  94 142 110 105 280 325 236
  47 306  57  14 231   2 260 222 274 240 437 360 434  36 229  68 469 256
   4 302 261 465 409  53 479  63 115 293  73   9 126  30 452 283 164 215
 230 510 252 134  58 257  20 248 328 258 217 221 313 320 107 416 386 491
 226 484 445 267 276 119  22 390 285 290 301 340  71 512  40 242  64  56
 127 498  96 485 223   1 400 116 504 411 109 327 442 470 335 244 402 447
 272 378   0  76 418 166 480 436 444 146  43 454 502 493 297 228 234 266
 449  24  12 451 138  70 435 157 278 273 505 269 441  10 323 275 232 461
 128 284 309 399 282   5  25 270  62 450 251 324 383 292 531 403 264 113
   7  51 423  67 130  41 263 136 139 457  11 279 241 111  77 466  46 250
  59  54 428  37 375 129 294 426  87  26 522 329 277 507  17 472 147 381
 519  27 159  18 288 427  50 514 478 379 499  16 316 471  21 255 361  44
 521  39 300 295 430 135 189  75 239   8 530 425 474 155 362 501 245 405
  66 368 287 225 299 380 291 123 517 476 524 342 253 529  23  69  32 163
 490 249 364 527 440  45  99 303  35 396 286 281 464  31 131 235  60  34
 120 112 453 336 481 133 387 462 483 537 532 398  61 523  19  49 516 268
 271 122  74  65 161   3  38  98 460 438  91  15 503  28 497  13 468 496
 401 407 322 511 525 205 536 140 415 455 509 369 188 459 377 168 124 149
 331 145 439 144 458 104 475 526 125 533 156 528 102  90 389 373 321 513
 534 500 508 318 141 394 443 183 431 192 376 482 307 132 535  97 520 151
 158 197 419 337 395 108 456 137 319 184 446 506 412 487 152 179 332 176
 393 518 194 371 489 413 406 347 312 187 165  82 370 330 354 338 341 185
 310  78 374 114 167  83 538 121 162 153 365 317 388 314 150 196 160 175
 203 200 186 199 429 385 339 172 198 315 363 191 366 174 202 477 367 417
 382 372 154 359 352 143 358 421 173 182 212 494 344 424 348 195 384 204
 346 193 177 422  79 190 350 207 420 148 201 349 180 351 515 345 170 357
 355  80 356 353 211 178 206 171 208 414 181 209 408 169 210 495 410]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0819
INFO voc_eval.py: 171: [ 37   1  35  94  36  93  11   0  48 103  96 106   9   6  40  39  12  14
  41  45  38  34  26  47 101  10   4  43  46  16  76  15 107 117  99  82
  89   8  23 138  64  49 100  98 111  84 144 115  22  95  80  97  44 102
   2  17  81  42   5  79  33 134 135 114 137  83  69  85 123  63   7 118
   3  86 104  60 139  25  19  77 152 126  18 133  20 122 124 105 121  73
  59  21  92  13 108  31  57 146  28 120 110 119  65  24 113 143 129  30
  55  78 128 131 130  72  58  87  32  68 140 125 150 149 127  29 141 132
 142  54 151  66  90 116  70  91 136 109  52  61 147 148  27  62 112  74
  71  53  56  67 145  75  88  51  50]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.1000
INFO voc_eval.py: 171: [375 376 374 385 391  54 419 482 378 276 478 189 505  60 242 379 493 238
 399 452 397 280 425 110 380 417  53 377  58 393 481 389 454 210 387  62
 182   8  49 302 479  89  73 271 239 447 241 450 140  64 423 494 318 304
 404 179 183  95 316 281 166 114 415 240 111  70 146 144  52 247 106 362
  87  57 102 420  63 476 489 278 526 390 321 109 180 284 149   1 251 305
 133  10 382  59 427 455 198 205  84 113 490 141 513  92 477 456 266 406
 306 340 503  66 313 142 279 365  51 484 153 508 294 194 474 487 254  97
 107 115 154 339  55  68  74 463 223 282 497 460 383 138  67  69 300 386
 173 150 185  76 145 250  88 442   2 411 308 123 310 328   0 139 334 448
 384  83 143 263 147   9 473 341 434 256 148 233 480  56 409  50 535 405
 230 348 192 286 449 483 453 467 336 361 184 125 151 367  46 244 521 152
 528 248 196 507 469   5 161 440  20 277 259 325 509 355 199 156 496 108
 307 220 226 175 485 498 499 221   3 213 438 186  33 413 323 228 127 395
 249 314  81 224 291 225 100 504  26 510 326  93  24 335 121 457 270 265
   7 188 461 190 177 119 506 292  72 495  99 283 439 437 347 429 371  65
 303 287 157 451 197 214 212 430 360 261 522 511   4 431 322 181  25 204
 116 363 163  82 500 470 298 216  86 518 343  71 381 402 492 472 388 342
 193 344 353 312 164 403 356 459 414 234 275 327 468 273 117  43 257 520
 441 217 338 203  35 187 488 426 532  11 536 502 293 218 201 104  77  61
  79 134 421 315 515 219 169  91 320   6 418 366  78 330 103 253 162 319
 357 301 231 368 517 407 435 370 514 129 519 398 232  15 424 332 311 299
 159  85  31  28  19 132 267 222 443 433 369 408 533 243 392 329  30 346
 195 118 359  38 525 534 412 227  37 491  96  27 112 122  17  47 309  14
 272 295 252 317  12 209 410  36 445 372 432 202 331  75  23 120  18 527
 529 155  90 268  34 373 394 401  22 416 324 516 297 174 501 245 207 255
 124  32  44 349  29 436  42 158 524  41  16 200 289 170 262 458  80 446
 486 422 512 333 523 172  21 176  13  45 400 215 444  94 269 354 290 178
 471 530 531 428 101 105 351 260 258 136 208 237 396 171 285 462 464  98
 211 235 358 137 191 466 135 229 274 364  48 206 246 288 264 296 352 475
 350 236 131 465 337 345 160 167 168 128  39 130 165  40 126]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0995
INFO voc_eval.py: 171: [234  92 405  99 237 313 182 224 221 400 178 297 107 236  82 212 302 104
 227 108 285 225  86 415   6 209 105  97  91 197 180 268 336 269 361 296
 398  11 374 185 233 210 310 179  89 230 339 307 364 219 211 407 288 402
 226 358  94 324 301 375 406   4 420 351 326  31 360  28 369  83 365 239
 298  18   2 283 220 401 272 289 176 422 256 175  78 330 232 344  44  21
 111  27  33  93  29 187 411 343 424 359  24 340 287 372 261 181 311 396
 188 144 329 376 286 371 366 312 238 168  50  45 320  14 414 169 409  52
 323  32 165   9 166 357 427 222 280 430 190 214 161 145 215 102 196 277
 367 417 160 242   0 106 282 152 189 167  53  75 404 403 138 172 368 370
 223 319 348 100  74 382 293 253 267 149 335 114 325 241 337  23 373 192
 109 429  56 342  47 428 142 399  12 254 177 244 200 273   3 147  90 116
  77 252 362 413 412 303 363 134  84 426  25 140 278 117  58 136 246 377
 240 322 123 164 321  87 122 397 264  38 345  72 154  51 235 101 353 258
 151  68 381 113  42 158  26 141 317 115  22 121  71 408  37 349 110 306
 183 193 217 163   7 387  30 251  96 186 143 137 257 305  16 231 162  41
 332 410 146  19  20   5 171 103 300  35 124 419  80  73 425  98 191 243
  59 229 194  63 255 380  81 281 389  39  49 315  62  69 418 416 218 155
 378  65  79   8  61  70 309 385 386 271 354 260 112 148 174 228 130 248
 153 207 334 279 139 120  55 213  10 156 128 157 318 431 208  88  76 204
 150  60 199 206 276 132 266 195  17 126 249 384 379 341 331  34 304 308
 421 259 198 247  57 393 355 299  95  46 274 129 338 328 263 350 423  64
  43  85 127 250 314 119 201  40 216 391 291 159 184  66 290 118  15 294
  36 205  48 125  54 392 292  67 383 395 170 295  13 135 388 394 270   1
 173 333 202 133 316 327 275 390 131 346 347 265 356 284 245 352 203 262]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0894
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.1178
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.240
INFO cross_voc_dataset_evaluator.py: 134: 0.034
INFO cross_voc_dataset_evaluator.py: 134: 0.118
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.317
INFO cross_voc_dataset_evaluator.py: 134: 0.112
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.085
INFO cross_voc_dataset_evaluator.py: 134: 0.140
INFO cross_voc_dataset_evaluator.py: 134: 0.098
INFO cross_voc_dataset_evaluator.py: 134: 0.053
INFO cross_voc_dataset_evaluator.py: 134: 0.034
INFO cross_voc_dataset_evaluator.py: 134: 0.329
INFO cross_voc_dataset_evaluator.py: 134: 0.157
INFO cross_voc_dataset_evaluator.py: 134: 0.162
INFO cross_voc_dataset_evaluator.py: 134: 0.082
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.100
INFO cross_voc_dataset_evaluator.py: 134: 0.089
INFO cross_voc_dataset_evaluator.py: 135: 0.118
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.904s + 0.018s (eta: 0:01:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.482s + 0.006s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.487s + 0.005s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.502s + 0.005s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.523s + 0.004s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.519s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.515s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.519s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.516s + 0.004s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.504s + 0.004s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.494s + 0.004s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.488s + 0.004s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.481s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.648s + 0.003s (eta: 0:01:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.538s + 0.006s (eta: 0:01:01)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.524s + 0.005s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.535s + 0.005s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.527s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.534s + 0.005s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.546s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.545s + 0.004s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.539s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.522s + 0.004s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.509s + 0.004s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.500s + 0.004s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.493s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.658s + 0.004s (eta: 0:01:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.546s + 0.007s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.539s + 0.006s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.537s + 0.005s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.529s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.521s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.530s + 0.005s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.533s + 0.005s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.538s + 0.006s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.521s + 0.006s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.509s + 0.005s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.497s + 0.005s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.490s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.671s + 0.020s (eta: 0:01:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.520s + 0.006s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.524s + 0.006s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.524s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.531s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.522s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.519s + 0.005s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.525s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.525s + 0.005s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.513s + 0.005s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.497s + 0.005s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.491s + 0.004s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.483s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 75.134s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 8  3  6 19 36  4 61 39 32 66 62 15 63 48 57 14  7 46 21 24 33 18 12 65
 10 17 58  0 26 23 41 67  2 20 60 38  1 30 44 13 27 56  5 52 55 31 47 51
 25 22 28 29 40  9 53 43 34 50 64 54 45 11 59 35 16 68 37 42 49]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1025
INFO voc_eval.py: 171: [457 663 227 509 674 398 650 590 644 546 649 569 673 607 572 454 669 642
 662  97 549  40  54 455 545 666 376 574 226 612 239 594 391 439 667 580
  52 425 633 502 382 388  34 241 230 571 641 393  49 643 501 651 624  64
 619 656 554 443 664 392 655  42  60 555  51 618 181  30 438 592 448 149
 617 627 386 570 652 630 564 243 647 363 231 654 538 233 498 623 639 381
 622 244 648  31 428 383 616 433  89 521 154 387 661 536 365 665 452 658
 246 550 575 561  36  70 385 506 602 670 242 659 576 601 399 279 672 396
 600 621  43  41 637 369 593 280 556 668  57  86 456 581 606 609 155  69
 620 646 503 458 440 471 451  39 657 255  78 150  85  58  59 596 660 228
 459  48 557 671 534 566 441 366 532 238 332 380 417 394 402 653 628 259
 414 225  32  16   4 632 597 645 442 568 434 558 494  63 598 318  44 626
  35  38 514 424 281 158 282 229 397 372 548 106 152  33  98 349 589 235
 395 162 551 378 495 588 475 389 316 159 610 681 519 422  45  20  61 384
 400 446 611 542  71 151 237 182 215 324 431  46 361 420 427  47  17 247
  99 522 165 437  50 496 547 147 518 245 294  91 201 573 416 635 553  87
  94 591 377 560 296 236 535 234 286 161  19 371 604 520 565 418 124 185
 469  53 426 157 515 240 328 104 220 404 683 326 232 276  84   0 342 176
  88 275 364 678 374 413  22 493 321 336 196 120 358 484 563  11 613   1
 625 252 523  81 277 449 163 221 297  79 183 478 214 278 390 323 461 584
 406 199 290 125 419 500 329 636 453 300  21 552  23 682 293 464 331 421
 533 423 508  82 295 143 614 460 298 504 202 335 269  75 109  83 114 539
 409 507 517 629   2 302  76  68 360  72 173 253 476 401 599 330  67 194
 344 195 415 537 435 370 322 477 479 292   3 408 111 450 412 634 180 301
 146 257  14 444 367 166 530  15 284  18   7 178 193 559 472 327 489 100
 132 258  80  10 679 177 499 108 577 270 675 320 473 470 107 211 265 488
 685 299 219 492 497 531 288 481 463 167 467 485 144 179 285 595 341 184
 283 359 631 505 447 168 677 430 638   5 337 171 583 287 307 462 189 333
 407 513 309 129 212 251 516  77 313 541 156 310 582 271  90 339 266 217
 142 346  37 474 351   9 373 206 334 209 308 312 325 188 264 224 482 436
 160 164 172 205 319 145 680 605 222 130 254 586 684 218  62 203 153 578
 579 197 170 375 608 432 267 540 445 204  74  92  66 350 291 403 567 483
 544 355 465 200 148 562  56  55 676  96 216 466   8 249 487 405 343 174
 429 305 379 256 141 491 115 138 213 315 615  93   6 248  13 411 468 640
 354 128  12 223 410  65 190 126 103 102 356 289 169 101 348  73 543 528
 306 603 303 352 113 175 340 186 191 123  95 357 274 311 122 304 273  26
 345  28 490 192 347 585 208 486 210 198 314 338 587 140 353  29 207 524
 512 272 137  24 317 510 480 262 268 526 263 527 135 187 136 529 105 139
 134 525 261  27 260 362 110 368  25 250 127 118 511 117 133 112 119 116
 121 131]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1001
INFO voc_eval.py: 171: [126 135 119 141 139 154 127 157 150  26  20 145 109 369 133 155 146 305
 151 491 790 287 259 129 117 377 140  15 332   5   3 118  11 283 371 148
 262 267 295 264 734 181 488 830 292 435 372 161 132 319 266 842 575 407
 291 367   0 327  60 496 581 293  18 705 789 445 714 511 370 274 375 804
  19 793 509 152 318 122   1 294 564 401 174 827 323 444 160 409 147 411
 404 131 420 164  21 725  25 716 852 723 307 499 566 124 277 233 153 387
 232 448 211 315 431 841 421 578 580 704 337 743 380  23 836 487 744 219
  71 103 707 810 414 180 419  74 173 408 334 787  90 792 423 865 726 108
 834 579 430 740 840 710 732 275 341 339 889 851 449 415  17 331 303 100
 281 260 730 739 673 290 843 500   7 439 565 397  80 523 280 298  10 838
 765 165  92 458 617 482 911 328 625 116 137 365 433 176 317 373 395 870
 125 374 729  93 416 221 669 737 138 447 101   4 394 134 321 741 114 670
 802 286  56  70 269 336 104 833 304 899 861 613 462 436  73 712 766 862
 434 379 427 282 195 112 442   2 696 446 378 896 120 144 501 279 353   8
 179 368 263 767 188 175 791 440 330 863 424 299 908 217 909 706 755 441
 698 205 347 302 426 230 218 616 829 123 359 573  69 516 391 276  22 432
 115 675 831 213 360 702 577 271 878 410 429 461 143 338 450 611 177 864
   6 869 265 515 389  98 832 288 693 486 333 136 344 296 208  63 785 835
 897 231 795 757 837 892 261  88 301 855  68 171 376 760 340 684 803  81
  12 860 229 412 775 722 567  61 326 619 898  94 184 406 749 776  99 671
  43 400 113 348 828 849 325 384 207 680 727 215  16 251 624 475 774 801
 856  67 587 691 398 479 202 297 244 390 454 105 480 521 358 678 641 854
 796 272  40 627 894 895 679 643 385 700 517 192 156 469 807 756 661 197
 106 111 270 884 654 630 170 324 345 906 902 746 166 904 128 306 310 623
 748 682 820 812 697 343  55 733 877 839 658 417  96 356 225 519 885 158
 717 586 392 874 443 881  57 223  35  86 872 198 484 526 322 167 850  64
  65 786 824  58 361 354 473 335 893 162 289 900 273 788 813 560 770 782
 823 346 800 875 666 672 639 396 731 422 284 676 258 846 285 657 681 187
 366 826 316 522 768 615 660  72 457 713  41 524 848 667 313 769  13 905
 494 362 309 212 425 754 186 847 234 193 191 570 478 751 249 130 637 845
 888 203 891 505 772  62 747 576 363 640 142 866 476 222 474  66 609 342
 665 763  42 735 886 589 349 815 456 252 399 468 250 481 653 506 563 569
  14 649 599 662 844 525 402 483 497 724 750 753 418 357 811 257  59 903
 638 825 882 355 629  28 699 738 320 243 647 527 858 163 242  89 859 451
 466 664 460 685 382 110 907 216 278 646 428 226 890 610 794 910 783 719
 784 736  34 651 237 879 901 498 256  39 880 471  79 210 626 241 694 652
 582 635 149 204 485 592 822 752  85 778 121 253 386 853 808 159 677  84
 352 572 238 329  52 351 758 708 687 571  77 312 621 797 493 235 871  27
 596 614  78 876 633 668 588 520 528 530 644 236 240 761 631 873 568 245
 477 867  45 542 805 550 645 883 604 510 254 806 711 759 492 196 311 745
 591 246  32 247 562  38  29 683 470 606 650 674 472 798 799  33 168 239
 607 248 182 721 887 214 659 268 308 634 534 220 686 514 809 224 720 868
 642 314  54   9 718 632 779 655 489 773 529 512 438 762 185 857 636 764
 532 107 628 817 255 781 502 465 536 463 504 648 620 495  53 381 518 574
 595 612 777 201 593 490 364 622 703 663 209 742 548 602  51 618 656 818
 200 228 102 559 692  97  87 227 537 688  83 535 194 508 437 561 557 583
 690 206 555 695  31 600 300  24 590  82 464 541 539 558 912 403 513 467
 545 549 771 546 554 503 540 543  47 598 531 597 183 507 601 547  37  30
 552 551 816 605 553 603 405 594 538 585 584 780 556 388 544 814 453 689
  76 190  75 819  50  95 701  44 169 533 350 715  46 413  91 178 821  48
 383 728 172 608  36  49 455 709 393 199 189 452 459]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0257
INFO voc_eval.py: 171: [397 341 348 365 392 382 385 347 362 340 388 117 368 366 376 384 467 120
  30 539 336   6 361 342 455 338 345 436 140 379 374 430 559 359  29 139
 171 372   8  59  26 546 428   3 282 402  32   0 185 309  56  50 423   1
 440 383  72 143 188 511 437  47 302 121 216 239 371 380 145 176 415   2
 475 487 242  39 201  22  46 289 150  15 551 157 456 118 126 469 354  48
 419  84 278  35 498 190 233  19 273 417 165 391 167 476 305  11  17 152
 301  54 497 356 466  55 119  61 350 261   4 358 236   7 541 355 509 346
  27 463  82  20 393 144 267 179  36 462 269 173  58 510 133 300 540 272
 483  38 470 413 155 425 205  68 370 439 198 122  23 329 481 545 213  63
 158 169 438 557 395 550 276 357 457 111 217 499 387 461 547  75 471 100
 263 221  18 244 518  45 352 478 182 411 135 280  89  42 180 485 339  87
 102  28 237  92 474 308 344 148 311 492 310  70 127 103 290  40  53  77
 477 286 519 504 542  33 315 178  98  12 563  65 442 271 245 514 508 181
  69  52 554 435 464 426 432  78  93  34 219  67 138 420 434 160 196 473
 270 552  31   9 265 153 503 197 277 243  95 260 295 257 367 106 443 228
 441 332 501 488 296 104 186 381 404 447 227 389 479 323 124 259 235 398
 548 506 561 465 211  60 489 564 166 161 151 502 164 247 427 231 163 494
 220 484 283 299 373 460 543 195  91  79 424  51  86 207 214 210 445 194
 218 208 285 249 251  41 128 390  14 378 351 109 343 472 399 500 125 266
 141  71 275  85 172 493 369 230 386 294 327 316 433 495 110 112 307 162
 256 520 253 522 444 538  16 156 254 512 246 549 544 535  90 132 331 225
 401 318  76 101 149 480 527  57 177 175 113 105 326 268 459 536 532 189
 507  74 560 449 529 558 458 446  10 422 123 317 408 131 418  94 490 414
 454 183 142 168 129 364 252 363  24 108 334 321 431 274  44  83 330 174
  43 403 192 537 313 314 486  73 405 200 400 223 396 375 107 513  80 555
 264 325 292 284 562 353 206 526  99 222 170 553 528 209 232  25 451 452
 337  64 147 410 238 116 496 288 203 146 187 556 333 250 468  49 226 293
 534  62 224  97 525 248 491 335 377 204 394 154 533 255 406 409 412 319
 306 240 193 191 450  66 215 505 521 407 517 524 241 429 287 349  21 516
 136 320 298 130 453 262  96 421  13  88 304 448 114   5 416 115 530 360
 234 281 184 324 523 322 303 212 328  81 291 137 515 229 531 159  37 312
 134 297 202 279 258 482 199]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0243
INFO voc_eval.py: 171: [63 64 62 31 61 52 33 56 59 38 22 58 27 68 24 60 57 30 50 32 13 66 29 44
 21 16  9 15  7 49 19 42 67 36 18 39 11 10 65 12 26 43 40 46 41 45  3 14
 25 23  1 28 54 47 34 48 51 35 53 17  8 55 37  6 20  5  4  0  2]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0455
INFO voc_eval.py: 171: [193 264 270 247 150 267 137 135  57  45 112 113 204 161 212 248 138 189
 141 211 186 271 278 110  58 379 143 160  56 130  63 205 196 200 119 176
 153 109 179 366 277 253 149   4 207 385  51 208 133 134  76 268   9 140
 111 158 273 152 276 147 381  46 260 128 114  53  31 132 198 240 375 275
 272   2 151 203 117 199 148 139  28 163 252  70 213 374 170 194 251  66
 173 169  44 122 347 171 413 206  64 145 129 184 315 115 298 202 257 172
  78  13 311 225  79  10 127  39 234 317 392 209  60 201 369 180 182  40
 269 283 246 177 254 218 168  54 262 165 108 106 154 244  33 238 227 142
 236 235 362   1 229 390  96 327 384 131 288  20 159 408 183  77 185 402
  71 250 187 197 125 178  16 226 181 367 395 116  85 243  91 195 167  22
 372 166  52 121 192 360 249 299  90 302   6 265  82 364 164 280 343 289
 418  37  38 261 380 307 123 348 162 258 287  72  83  89  81 155  69   8
 333  92 406 377  47 310   5   3  94 245  95  49 242 312  59 332 314 292
 393 191 188 397   0  50 351  17  11 368 352  43  65 376 319 293 303 386
 285 291 344 412 210 228  12 174  34 357 222 308 409 383   7 255 417 407
  75 223 100 107 324 361 318 338 241 404 124 396  87 231 190  84 400 126
 120 416 373 296 394 263 175 214  21 365 359 103  73 329 398 304 321 378
 274  68  55 105 232 156 371 414 266  24  98 334 313 358 389 279 306  80
 294 305 370 337 300  19  29 282 391  93 284  62 341  23 239 387 297 237
 415 230 401 259 403 388 256  15 382 339  14  74 410 405  18  86 399  88
 233  36 220 215 411  35 323  41 144 301 340 326  26  48 286  97 290 295
 330  25  30 102  42 328 320  99 224 356 336 345 325 363  67 221 342 316
 217 322 335 146 281 309 157 353  61 350 216 104  27 219 346 136 355 349
 354 331  32 118 101]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3405
INFO voc_eval.py: 171: [403 224 121 ... 762 730 753]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0372
INFO voc_eval.py: 171: [ 8 23  1 13  7 25  6  0  5 12 22 16 11  3 17  9  4 21 19 20 14  2 24 15
 10 18]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0000
INFO voc_eval.py: 171: [3616 2655 3626 ...  608  609  606]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0986
INFO voc_eval.py: 171: []
Traceback (most recent call last):
  File "tools/test_net.py", line 137, in <module>
    check_expected_results=True)
  File "/home/ubuntu/Detectron.pytorch/lib/core/test_engine.py", line 128, in run_inference
    all_results = result_getter()
  File "/home/ubuntu/Detectron.pytorch/lib/core/test_engine.py", line 108, in result_getter
    multi_gpu=multi_gpu_testing
  File "/home/ubuntu/Detectron.pytorch/lib/core/test_engine.py", line 163, in test_net_on_dataset
    dataset, all_boxes, all_segms, all_keyps, output_dir
  File "/home/ubuntu/Detectron.pytorch/lib/datasets/task_evaluation.py", line 61, in evaluate_all
    dataset, all_boxes, output_dir, use_matlab=use_matlab
  File "/home/ubuntu/Detectron.pytorch/lib/datasets/task_evaluation.py", line 101, in evaluate_boxes
    dataset, all_boxes, output_dir, use_matlab=use_matlab
  File "/home/ubuntu/Detectron.pytorch/lib/datasets/cross_voc_dataset_evaluator.py", line 48, in evaluate_boxes
    _do_python_eval(json_dataset, salt, output_dir)
  File "/home/ubuntu/Detectron.pytorch/lib/datasets/cross_voc_dataset_evaluator.py", line 125, in _do_python_eval
    use_07_metric=use_07_metric)
  File "/home/ubuntu/Detectron.pytorch/lib/datasets/voc_eval.py", line 172, in voc_eval
    BB = BB[sorted_ind, :]
IndexError: too many indices for array
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.720s + 0.004s (eta: 0:01:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.526s + 0.005s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.515s + 0.005s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.530s + 0.004s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.534s + 0.004s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.537s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.541s + 0.005s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.536s + 0.005s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.530s + 0.005s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.518s + 0.005s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.508s + 0.005s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.499s + 0.005s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.493s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.07s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.623s + 0.005s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.598s + 0.007s (eta: 0:01:08)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.577s + 0.008s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.561s + 0.007s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.556s + 0.007s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.546s + 0.007s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.547s + 0.007s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.537s + 0.007s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.529s + 0.006s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.516s + 0.006s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.503s + 0.006s (eta: 0:00:12)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.496s + 0.006s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.487s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.696s + 0.005s (eta: 0:01:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.579s + 0.005s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.537s + 0.005s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.526s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.526s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.521s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.518s + 0.005s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.515s + 0.005s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.510s + 0.005s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.504s + 0.005s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.492s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.487s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.481s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step1999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.688s + 0.004s (eta: 0:01:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.486s + 0.007s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.495s + 0.006s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.489s + 0.005s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.479s + 0.005s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.474s + 0.005s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.481s + 0.006s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.484s + 0.006s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.495s + 0.006s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.491s + 0.006s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.482s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.476s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.471s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 75.766s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [34 24 77 28 31 14 11 41 69 75 81 37 38 57 16 33 20  2 23 30 13  9 18 10
 26 82  4  5 74 73 68 71 56 40 70 15 65  8 62  3 19  1 61 35 80 59 12 36
 51 53 49 43 76 39 66 79 29 72 67 83  6 47 55 46 48 42 52 17 25 58 54  7
 21 78 64 50  0 45 27 22 63 44 32 60]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1084
INFO voc_eval.py: 171: [148 136  15 130  93  52 124 140  46 126  25 131 133  18  47  64 122 151
  14 144 137  48  59 143  57  20 129  61  51 109 128  13 152  87  55 135
 146  90 123  50 147  98  58  60  79  76   4  38 127 107 115  96 125  54
  88  65 106  85  56 112  26 134 145  95 138  92 153 110 150 139 132  74
  22 141  72 114 105 149  24   0  62  21 142  73  94  66  36  30  19  16
  17  53  75  43  89  91   8 121 100  32 117  45  12  11 118 101  99  23
  78  29   3 102  81  97   6   2   1  49 111  70   7  41 108   5  80  67
  63 113  27  10  33 120  40  42 103   9  68  31  34  44 116 119  69  77
  28  37  35 104  82  86  83  84  39  71]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1025
INFO voc_eval.py: 171: [657 662 344 659 161 323 651  62  63 646 642 639 148 632 631 127 599 113
 105 612 338 334 354 242 243 697 684 244 732 616  58 448 650 357 617 151
 743 331 847 389 298 129 593 585 633 245 624 329 586 356 753 351 641 762
 693 136 681 106 335 179 674 511 671 270 101 276 610 138 123 851 711 277
 685  96 122 666 347 330 234 355 608 121 118 112 606 192 142 377 375 494
 501 479 510 359 766 764 763 379 163 162 459 761 445 584 834 149 442 146
 283 444 605 239  79  80 653 706 630 353 326 325  89  37 745 687 614 861
 337  95  61 754 279 480 678 250  53  48 660 186 109 381 702 735 682 517
 668 272 171 253 696 663 667 582  72 407 102 235 100 408 404 844 231 390
 228  90 318 286 224 400 223 423  73 588 635 846 296 748 439 736 260 836
 470 688 451 455 440 424 409 699 410 214 394 705 728 261 611 581 193 297
 817  83 139  84 133 358 816 128 126 620  94  97 615 115 131 302 521 332
 665 343 176 738 737 376 679 267  44 374 498 293 403 290 258 299 288 268
 255 284 366 363 265 312 321 144 125 184 658 508 173 531 768 852 492 656
 655 164 154 236  82 140 589 613 536 190 497 689 227 796 421 723 422 708
 780 441 110 452 857 676 462  25 292 853 626 342 720  77 649 813 333  22
  28 661 411 124 835 392 456 117 259 246 484 252 770 222 538 759 756 143
 132 597 287 362 791 324 710 792 739 677 472 474 777 361 388 563 365 838
  74 213   8 232 254 175 862 848 294 587 119 673 860 483 150  15 352 830
 303  56 158  91 647 438 534 864 489 170 172 719 430 740 592 603 180 196
 249 435 825 183 607 266 664 625 304 530 815 805 314 539 300 316  10 491
 856 104  38 348 345  49  50 467 418 744 776 291 387 317 833  23 449  92
 636  11 729 160 425 373 691 264  20 339 405 807 396 478 784 386 328 306
 412 431 427 863 212  78 532 453 481 680  54 519 263 529 797 818 858 571
 760  21 187 814 177 524 802  57 447 686 803 522 281 382 799 514 181 783
 601 779 141 219 822 771 496 320 747 798  33 237  26  45 669 794 637  75
  18 399 168 541 709 488 594 590 596 167 240 437 417 781 107 203 315 774
 810 535 257 842 551 155 559 516  59 230 727   7  93  85 336 368  19 380
  43 829 194 130 200 750 621 206 604 628 233 859 165 221 486 269 556 477
  65 804 580 537 553 800 548 398 401 499 169  12 849 733   2 174 823 471
 808 465 273 271 391 114 609 436 322 145 812 717 725   1 402 755 634 513
 640 515  16 157 108 765 726 769 159 393 310 573 406 730 414 461 468 767
 135 850 837 595 208 103 505 311 746 520 220 778 568 205 309 415  69 543
 752 433 202 367 564 772 526   4 542 724 319 454 845 638 229  76  86 509
 561 715 346 349  66 369   6 832 241  68 500 195  87 828 574  99 714 579
 562 207 111 827 843 116 475 713 841 698 790  70 734  47 426 413 490 434
   5 789  71 185 622 695 855 546  39 555 178 787 188 238 182  31 703 821
  30 643 204 198 432 618 583 197  36 274 495 372 654 218 385  24 578 773
 600 383 576 503 692 209 785 552 550 482 540 716 191 707 570 251 156 429
 690 786  46 694 712 458 397 189 788 307 523 216 350 700  13 840 201 378
 547 469 602  42 544 518  41 560 721 806 153 718 865 683  27 487 280 623
 485  98  35 416 466 211  34 619 120 370 473 819  52 704 644 839 226  55
 701 629 278  60 751 147 428 558 722 327 395 199 557 152   9 275 572   3
 262 731 749 420  81 446 305  29 549 672 527 528 854 285 493 295 795   0
 793 820  17 627 782 248 308 476  14  88 450  67 215 591 137 289 384 460
 652 507 648  32 256 301 645 675 598 577 225 826 801 533 512 134 217 809
 775 554 824 313 419 567 831 166 811 741 443 457 670  40 360  64 502  51
 282 341 525 210 247 742 758 464 575 757 463 566 371 545 364 569 340 565
 506 504]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0095
INFO voc_eval.py: 171: [295 363 283 269 273 432 289 175 287 406 321 272 293 487 100 388 431  59
 220 249 221 280 252 424  65 485 176 381 204  99 262 365 300 254 297 168
 499  61 330 208 255 410 411 364 172 177 296  97 334  40 426  73 286 329
 171 227 206 417 427 370   6 298 434 105 382 210 263 343 327 423 195 279
 392 447 437 185  14 390  19   9  50 309  12 284 302 256 340 328 347 414
 316 425 198 490   1 258  39 146 413 174 458 420 259 268  28 274 366 196
 324 165 429  76 470 446 251  20 178 317 194 445 179   0 486 479  21 472
   2  78 415  52 216 412 419 433 308   4  11 107 223 443 299   8 342 474
 428 430   5 267 505 282 270 243 123 421 271 189 476 318 480 207  38 304
 371 260 111 375 278 150 500 240 285 332 153 166 125 137  42  47  62 250
 312 468 495  64  13 323 257 101 368 346 461  69 145 319 192 231  35 306
 450 233 350 121 333 217 213 320 453 448 241 126 504 494 120 491  81 169
 337 103 401 239 339 129 294 503 496 154 469 367 436 167  25 159  18  85
 502 456 385 160 372 144 467 399 181 173 224 315 462 356  46 374 501 247
  51 152  92 314 336 398 281 305 246 383 307  54 464  80  33 135 338 203
  84 311 188  15  48  90 455  31  86 237  93  37 444 242 277 422 394  70
 331 376 471 157  60 292 215 187 127 218 200 230 408 497 201  26 466 234
  74 452 438   3 235 112 409 124 326 303 348 313 354  67 229  55 102  58
 386  29 193  41  82  27 407 291 378 360  34 170 353  10 404 190 238 418
  23 131  44 261 163 106 219 211 110 478 156  16 115 183 191  49 222 290
 349 197 265  32 457  66 147  79 109 228 441 492 506   7 245 276 335 275
  22 473 164 134 225 451 322 253 236 114 212 214 130 117  56 361 108 377
 301 119 362 161 488  24 439  95  63 184 149 352 186 395 369 449 104  68
 199 310 226 155 162 344 489 345 484 232  98 498 142 359 483 288 379  43
  30  45 358 136 341 397  77 493  57  96 266 373  75  17 393 351 475 440
 122 116 459 463  89 244 384 325 403 454  53 140 248 389  94 481 209 132
 202 182 264 416 405 128 205 482 355  36  83 465 391  91 357 151 139 118
 148 460 442 143 477 141 396 180 400  87 402  72 380 138  88 387 133 435
  71 158 113]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0541
INFO voc_eval.py: 171: [266 154 260 228 257 248 232 205 234 263 264 213 240 165 161 225 164 215
 238 251 239 237 243 216 249 199 253 267 246 226 187  85 211  35 258 127
 196  53 217 189 153 221 192 156 252 236  82 152  78 112  40  33 193 195
  91 224 186  77  94  42  34  37  39  47  84 198 200 298 159  86 118  60
  61 194  83  32  62  10 181 229 247 250 202  88 122 147  49 244 261  30
  76 135 191 185 203 265 207  51  54 190 219  12 134   2  79  58 184 119
   1 255 242 222 220 104 235 114 245 150 110 279 295 113 212 231 183 128
 109 230 123 218 206 278 256 210 233 241 227 116 259 197 272 125 204 129
 262 281 286 188 305  16  31  36 117 254  52  50 214  28 155 166 120 139
 149  59 201 208 223 130  44 277 111 137 209 173 273  70 174   9  72  43
 136  89 288 276 151  73 282 182   3 171  71 168 133 146 180 158  15  64
  66 270  68 271  17  81 289  11 176 280 102  24 303 124 126 162  20  38
  69  90 177  96  75 106  65 145 105  23  80 160 172 300 144 304 115  63
 290  74 140  93 297  99  95 169 141 284 302  56 268  22 148   4 107 142
  21   5 292 108 274 170  57 175  14 163   0  18  48 143 132  45  92 101
  46 131   6 301   8  87  19  41  25 121  55 100  67 275  98 138 157 103
 287  27 296 283   7  97 179 178 285 293  13  26 291 299 167 294  29 269]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0185
INFO voc_eval.py: 171: [33 60 47 49 41 35 28 43 20 48 39 30 51 52 34 40  2  5 54 61 18 26 64 14
 58 27 38 65 15 29 57 22 44 55 56 12  1 23 19 59 46 68 10  0 50 37 67 16
 42 11 45 63 24 62 53  6 36  7  9 66 32  3 13 31 21  8 25  4 17]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2278
INFO voc_eval.py: 171: [ 436  263  446 ... 1092 1083 1087]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0273
INFO voc_eval.py: 171: [27 28 13 20 29  5 22 14 12 15 10  4 31  3  2 26 19 17  7 16  0  1 21  6
 11 18  8  9 23 25 30 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [ 867  761  944 ... 1220 1757 1763]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0297
INFO voc_eval.py: 171: [ 22  19   4   8  14   5  12  78  35  73  70  27  81  17  33 112  88  25
  36   6  91  86  21  41 111  48  55  65   2  77 101  20  97 108 100  58
  71 116  69 104  29  45  66  61  49  76  62  83  87   1 117  59  18  11
  67  57 114  84 110  74 109  13  16  31  94  52  15  38  54  95  82  28
  47   0  10  56   3  26 106  85  23  37  24  30 115  60  90  93 118  98
  80 107 119  64   9  51 103  92  40  75  79   7  68  99  63  72 120  34
  39  96 102 113  32 105  43  53  44  89  46  50  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0328
INFO voc_eval.py: 171: [588 291 626 584 544 568 554 587 246 240 235 231 574 579 224 218 216 649
 656 575 248 667 671 570 561 711 564 709 565 698 572 695 691 690 684 683
 678 254 694 256 259 258 313 639 631 630 583 613 612 611 609 603 602 598
 384 389 589 582 257 301 299 560 264 581 647 275 276 278 279 287 288 289
 290 294 296 297 300 558 255 484 550 728 538 493 548 734 498 496 547 541
 555 551 625 348 658 324 325 634 664 573 622 632 668 623 423 677 628 673
 674 666 203 576 637 481 648 266 241 268 270 226 284 221 286 483 178 214
 209  23 577 202 642 196 193 308 191 311 312 539 685 480 604 686 100 382
 713 585 715 108 376 415 113 391  73 416 607 567 562 557 693 731 617  69
 696 556 819 586 387 412 204 245 717 295  95 201 212  75  93 263 215 727
 390 272 230  31 228 227 236 559  29 281 223 644 222 237 595 578 722 721
 687 596 197 662 369 198 610 699 571 601 735 627 569 618 682 688 619 697
 659 606  18 641 491 814 188 110  17 655 665 638 600 636 804 401 482 430
 261  28 285  22 597 621 614 394 318 420 640  36 772 773 633  40 424 298
 635  79 243 194 225 681 742 220 219 545 217 680 676 672 670 663 654 189
  64 566 229 477 103 109 114 590 319 725 724 340 629 399 823 421 192 395
 187 624 117 365 594 599 563  63 689 535 605 353 616 615 692  86 195 669
 643 645 646 580 242 406  21 213 518 478 770 383 283 495 774 651 199 730
 497  68 537  70 608 234 271 741 592 239 111 593 106 381 433 542 650 653
 207 292 309 479 316 293 434 315 180 811 205 104  27  99  98  76 652 190
 249 591  41 408 657 118 771 736 210 232 492  71 211 342 200 679 723 310
  61 675 244 150 427 720 719 282 528 750 620 710 490 660 176 661 116 425
 166 102 181 546 265 107 531 267 549 417 472 494 515 749 233  26 260 159
 775 303 759 326 125 334  56 206 343  16  46 744  49 718  84  74 336 540
 167 405 468 486 738 208 251 269 779 489 508 525 794 183  57 467 552 520
 726 543  94 368 358 422 330 756 758 777 414 553 743 536  52 788 409 400
 827  81 815  88 714 809 799 499 375 238 158 501 737 485 175 732 273  66
 753 119 476 157 397  87 712 380 791 787  20 307 130  13 351 782 748 707
 388 177 357 149 790 708 818 413  39  37  54 366 459   9 122  96 410 136
  77  33 768 133  59 527 403 444 360 474 785 419 165 404  85 441 396 796
 816 523 321  60 778  24 280 534 826 488  25 514 179 247  45 370  72 339
 795 323 455 751 306 783 132 475 471 521 164 314 765 151 729 426 513 473
 392 354 141 305 356 530 152 766 767  78  34 522 747 824 462  65 350 716
 185 418 429 128 131 277  47 274 347 374 442 755 250 182 262 184 364 458
 333 706  58 320 407  53 345 359 739 733 451 101 511 168 466 411 798 322
 822 142 398  42 160 123 487  67  62 532 377 143 754 803 344 156 115 793
 169  92 402 449 355  32 454 139 517  82 820 432 126 456 821  12 780 252
 446 813 806 812 703 302 526 435 134  89 817 781 337 161 170   6 745 304
 752  14 137   0 329 362 378 428 757 120 138 253 148  50 810 463 705 338
  51  11 346 461 393 464 805 510  91 763 702 135 327 439 385 335 431  43
 174 808 761 445 776  30 154 112 450 361 452 317 437  38 807 386 500 341
 331   7  44 367 465 144  48 502 504 509 328 503 105 760 447 373 453 519
  80 171   8  55 764  90 332 162 438 379  83 127 363 825 792  10 460 457
 371 140  35 762 155 704 769 746 349 372 147  19  15  97 529 448 701   1
 800 352 797 153 124 512 740 786   3 784 163 129 146 145 186 121 524   2
 507 516 440 789   4   5 700 443 436 469 505 533 173 802 506 172 801 470]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0094
INFO voc_eval.py: 171: [ 51  33  52  39  56  35  53  12  37 122  21 123  46 127  31  43  36  17
  29 128 131  11  34  30 117 126  24  22  19  94  10  45   6  18  40 116
 118 120  26  32  76  48  15  54  14  73  71  41 110  13  95  61  74  93
  75  69  72 115   0  27   4  59  57  83  16   5   3  58  80   7  44  78
  66 103  23  92 108  99  81   9  86  49 121  70  85  20 101   8  68 119
 130  91  25  62  84  28  79  97 112  63  82 106 125 129  50 113  98 104
  77  64 111  96  47  38 100 109  42  55 124   2  90 102 105  89  67  65
 107  87 114  60   1  88]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0015
INFO voc_eval.py: 171: [ 24 105 106  12  81  30 111  83  99  91  43   1  34 100  22  57 135  29
 200  25  95  84  18   2  92  15  14  13 190  42  89  39 198  37  36 103
   8 137  82 233 189  35  48 101 229 147 184 143  51  96  33  46 131 204
   7   6   5 239 219 222 236  62 133  90 224 197  88   4 121  93 145  20
  50 102 129 187 169 164 176  19 231  54 194 181  60  97 203 242 217 124
  86 199 220 165 141 128 114 130 171 193 162 246 170  66  65   3  58 122
  32  49  98 140 232 109  52 207 146 182 244  87 123 230 218 221 238 118
 245  10  38  27 228  64 188 234 142 127  21 120  94 125 168 119 213 110
 150 195  59  73 191  44 196 226 227  26 211 178  17 179  28 241 134  40
 116 126 166 117 155 202 175  68 160 205 138  23 132  55 158 154 206 210
 235 108 115 209  47 185 173 248 223 148 104  56 208 214 253  45 172 139
  67 225 192  41 177 167 243 237 251 157 161 144 107 250 180 247  85 153
 216 183  53 163 186  61  16  80 249 240   0  31 201  11 215 252  70   9
 258  78  79 174 212 159 136 151 255 257 156 152 149  72 254  75  63  77
  71 113 112 256 259  74  69  76 260]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0324
INFO voc_eval.py: 171: [64 70 24 67 66 81 35 17 48 58 34  8 33 73 46 74 22 32 85 38 75 26 37 50
 19 72 62 57 14 18 49 41 84 60 44 28 68 21 43 69 55 27 47  1 53 83  3 39
 25 82 15 16  4 76 54 78 40  2 23 59 12 10 42 80 52 45  5 36 77 56 65 86
 63 79 51 31  0 71 61 30 29 20  9  6 13 11  7]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2424
INFO voc_eval.py: 171: [30699 16449 17246 ...  5679  5716  5668]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1358
INFO voc_eval.py: 171: [438 273 126 424 154 326 197  38 243 483 127 239 322 425 368 303  68 104
  80 359 319  87 282 342 327 320 407 455  62  63  60  33 128 411 122  81
  83  84 155  66 242 412 436 272 317 156 474 459 423   5 352 377  61 251
 433 431 130  77 415  42  72  96 143 219 427 325  92 354  73 416 277 316
 238  14 241 463 371 485  54  51 413  91 475 103 121 168 467 340 345 249
  35  88 441  75 124 323 398 402  69 214 469  74 477 217 362 360 376 306
 102 466 434  10  44 426 203 364 324 202  78 105 370 448 489 213 108 318
 201 404 161 349 321 113 405 372 200 150  45  71  30  50  53 176 111  56
 265 307 347 446 146 478  67 403 457 158  82 233  55  15  58 280 304 496
 374  52 420 101 443  86 153 299  79  98 300  85 428 484 421 301 482 490
  89 215 109 131 132 114 141  57  65 430 152  70 225  76  46 487 184 199
 440  59 394  64 274 432 369  90 164 409 494 358 237 112 481 204 410   8
   0 140 244 308  22 435 157 120 232 271 472 275 341 452 454 123 375 221
 408 262 468 418 451  28 437  16 344 385 134 314 224 346 309 261 256 449
 458 366 196 139 205 447 192 166 165 118 310 236 445  40 110   9 373  94
 230 276  29 312 332 235  37 223 206 178 257 211 246  36 138 281 414 417
 151 291 389 252 117  31 106 227 442   2 207 259 357 115 293 315 380 229
 145 388 495 228 218 183 296 125 264 397 142 220 198 247 278 283 144  32
 406 479 255 491 379   4 333  47 210 190 137 328 269  39 177  48 343 248
 129 486  18 439 119 378  99 338  43  41 148 268 488 279 493 387 100  95
 258 159 116 313 336 209 393 240 476 401 334  21 419 267 169 173 294 329
 222   3 136 186 160 363 285  24   1  49 348 208 270 163 174 167 311 180
 212 170 234 133  27 250 172 399 226 471  93  97 400 383 193 260 470 305
 107 361 175 396 422 335 188 339 337 330 382 171 302 456 392 185 480 386
 195 444  34 492  11 331 390 355 147 297 473 191 263 135 292 284 289 187
 381 216  23 254 384 286 162 182 231  25 194 181 253 245 287 365 462  17
  13 179 395   6 149 351  12 353 189 350 266 391  19 367 295 288 298 453
  20 356   7 460  26 464 450 429 461 290 465]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.1091
INFO voc_eval.py: 171: [124 138 143 116 170 121 247 125 150 214 152 329 212 252 172  21 185 120
  68 168  58  67 253 224 208  50  20 223 169 222 162 255  62 146 127  52
  59 195   7 309 148   8  23 219  49 250 198  82 207 330 118 191 119 141
  10 176 180 165 178  57 117  53  56  70 129 137 220 216  18  63  41  54
 234 251 269 132 302 151 133 239 194   3 298 123 128  51  25  86 110 215
 106 188   4 192 324  16 164  73 134  47 126 235 268 267 319 327 217 109
 334 289 104 160 135  30 155 230 322 296 333 167  69 211  80 315 201  22
 103 285 225 205 181 288 142  45  60 331 231 328  31 147 144 193 213 199
 175 249  76 131 189 177 122  79 159 114  75  17 284 318 325 113 158 149
 136  85 190  11 200 153  46  28 182 320  38 145 307 273 173 163 297 254
  99 183 312 228 210 105 203 112 262 139 100  97 258   0 111  98 171 281
   9 130  64 237  88 313 174 283 277  83 263 218  55  24 202 287 323 244
  13 161 272  91 227 221  32 326  78 232 154  65 196 140 115 332 206   2
  33 248 259 290   6 157  71  14 233 304  74   1 197  92 314 179  39  72
 184 209 294 187 308 186 257 229  87 166 204 226  48 156  81 293 301 317
  90 256 295 291  29 316 299 265  37  36  96   5 266 286  27  19  77  84
  34 271 321  66  43 270  61 280  95 245  26 108 101 292  15 275 303  12
 236  94 310 279 306 261  44 107 274  89 240 246 242 238  35  40  42  93
 305 300 311 102 243 260 282 241 278 264 276]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0476
INFO voc_eval.py: 171: [64 71 62 55  4 51  0 69 61 68 21 23  1 57 43 73 46 53 70 24 60 32 25 58
 67 59 30 54 17  5 42 49 38 56 11 19 66 50 65 13 86 81 87 31 74 12  3 47
 91  9 29 41 75  7 26 85 83 18 28 27 45 15 63 34 77 82 79 48 10 78 14  6
 44 40  8 39  2 52 89 35 84 76 20 22 88 36 80 72 90 33 16 37]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0413
INFO voc_eval.py: 171: [ 94  96 198  93  95 100 101 102 209 105 197 148 154 121  73 195 190   9
 196  98 192 151 194  60 204 160 152 168 118 202  99 166  65 106 169 123
 142 149 150 117 111  68 128 131  58 147  70 162 155  12 153 165 172  61
 210 199  27  80 207 185 167 203 108  67  82 189  63  17  28  57 206  53
 193 164  15   6 182  59  66 114 110  10 132  16  81 200 146  13 177  30
  41 138  25   8  62 134 103 130  56 156  44   7 183 161  33 171 144 159
  54   2 140 126 174  49 104  50 124 158  97 113 188  76 115  71 179 187
 201 119  22 116   3 133 109  21 139  23 178  40   1  64 208  72  26  35
 191  11 212 205 127 125 213  19  48 163 186  37  77 141  75  36 143  29
  18  46  39  38 129  74 180 112 120 170  78 157  47  69 211  79 107 135
  52 175  32 137  55  51 145 136  14  24  89   4 184  31   0 176  43   5
 122  83  20  34  84 214 181  91  85 173  87  86  45  88  90  92  42]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1277
INFO voc_eval.py: 171: [ 99 141 121  91  62 156  93  63 180  97 123   1  83  92  24 117  76  38
 133  90 132 171  81  74 137  33  48 165  43  44 169 111  55  98   9  68
  69  82 112 125 120  37  22  19  66 185  86 131 192  75 170 147   2 163
 136  47 161 153  28  27 144  87  49 189 191 160 118  61  78 195  96 184
 159  13 152 150 100 110 142 130 198  59  46  41  29  60  52 194 113  65
  40 182  70 157  51 175 179 174  53  95 146 107  84  80 124 127 187  31
 101  12  73  79 190 196 148 139   3 176  71 162  14  42 103  20  26 166
 177  58  45   4  89  72 102   7 135  56  36 134 119  85   8 128   0 193
 186 197  32  23   5  67  39 167 164 155  54 104 105 158  64  77  50  11
 183   6 108  21  57 140  88 116 168  30 109  94 143  15 151 188 172 126
  16 138  35 145 122 181  25 154 149  34 115  10 178  17 114 106 173 129
  18]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0273
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0715
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.108
INFO cross_voc_dataset_evaluator.py: 134: 0.103
INFO cross_voc_dataset_evaluator.py: 134: 0.010
INFO cross_voc_dataset_evaluator.py: 134: 0.054
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.228
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.030
INFO cross_voc_dataset_evaluator.py: 134: 0.033
INFO cross_voc_dataset_evaluator.py: 134: 0.009
INFO cross_voc_dataset_evaluator.py: 134: 0.002
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.242
INFO cross_voc_dataset_evaluator.py: 134: 0.136
INFO cross_voc_dataset_evaluator.py: 134: 0.109
INFO cross_voc_dataset_evaluator.py: 134: 0.048
INFO cross_voc_dataset_evaluator.py: 134: 0.041
INFO cross_voc_dataset_evaluator.py: 134: 0.128
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 135: 0.072
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.620s + 0.005s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.541s + 0.005s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.522s + 0.004s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.530s + 0.004s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.539s + 0.004s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.534s + 0.005s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.531s + 0.005s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.527s + 0.005s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.517s + 0.005s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.502s + 0.005s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.491s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.486s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.489s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.780s + 0.003s (eta: 0:01:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.548s + 0.004s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.525s + 0.006s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.495s + 0.006s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.497s + 0.006s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.510s + 0.006s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.517s + 0.006s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.518s + 0.006s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.509s + 0.006s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.499s + 0.006s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.492s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.484s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.481s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.478s + 0.003s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.557s + 0.004s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.505s + 0.007s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.499s + 0.006s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.509s + 0.006s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.505s + 0.006s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.502s + 0.006s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.500s + 0.006s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.501s + 0.006s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.488s + 0.006s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.480s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.475s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.473s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.613s + 0.041s (eta: 0:01:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.463s + 0.014s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.494s + 0.010s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.506s + 0.010s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.506s + 0.009s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.501s + 0.008s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.505s + 0.007s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.501s + 0.007s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.497s + 0.007s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.484s + 0.007s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.475s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.468s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.469s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 75.413s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [107  41  40  38  19  95  42  29  93  22  32  68  86  21  34  39  28  94
 106  16  92   7   6  11   9  17  31  91  85  75   4  66  74  36  14  33
  30  24  82 104  70  90  44  73  84  10   5  48  59  89 105  23 101  61
  46  57  26  63  12  13  25  15  54  67  88  98  53  97  56  45  81   8
  35  50  58  43  47  18 102  77  87  64  51  37  62 103  49 100  69  65
  79   3  27  80   0   2   1  99  96  71  60  76  52  72  83  55  20  78]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0776
INFO voc_eval.py: 171: [ 12 133 138  15  40 110 116  39 113 112 119  84 117  41 122 131 124 135
  55  53  11  17 115  52  44  51  79  46 128  10  80  16  42 136   5 114
  43  57  75  49 125  13  72 121 109  74  94 104  64  81 120 130  33  67
  86  96  97  48  92  47  20  98  37 123 100  19 111  87  78 132 126  93
 118 134  85 127  63  30 137   1   4  23 129  82  62  91  66   3  58  18
  14 105  27  56 106   7  70  34  83   8  88  24  50  68  90  65 101   2
  77  36  26  35  61   9  59 103  54 107  95 102  21   0  99  29 108  38
  25  60  28  89  69  76  71  73  31  22  32  45   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.1349
INFO voc_eval.py: 171: [768  74 365 380 733 388 391 693 265 732 690 430 436 748 448 749 450 174
 751 752 846 720 774 714 378 116 377 771 301 309 312 316 323 145 325 143
 141 782 136 135 366 127 124 367 268 371 374 375 681 497 447 502 535 727
 763 568 181 726 554 670 759 552  39 789 767 979 965 968 680 797 964  47
 784 758 770 723 805 785 745 118 569 185 659 379 113 125 130 137 132 713
 712 709 708 741 794 319 149 148 165 630 198 764 699 140 139 189 658 717
 356 146 747 106 385 455 951 525 850 687 686 684  68 683 689 180 481 243
 483 484  59 485 489 682 961 901 110 532 163 756 692 264 865 534 542 422
 396 261 772 392 428 966 863 862  35 438 103 267 721 211 213 255 730 262
 216 322 354 655 623 800 657 578 570 567 822 674 531 517 838 839 491 490
 482 480 461 427 864 424 413 691 399 395 368 362 716 411 645  94  30  86
  96  54 107 737  42 114  45 122 164 123 129 967 973 962  64 744  72 179
 138 134 133 441 950 432 462 251  87 776 433 443 398 414 321 329 333 334
 703 779 154  88 131 372 746 115 284 871 946  95 290  60 778 247 664 219
 187 662 762 660 558  14 819 543 801 593 527 204 594 625 493 242 735 192
 736 840 637 980 237  43 977 361 351 982 696 617 355 315 600 765 869 109
 754 152 467 843 506 460 860 444 434 516 811 835 429  37  90 418 830 412
 868 563 582  58 529 847 384 909 891 286 176 288 206 257 246 289 184 715
 296 167 215  40 359 508 505 874 845 357 722 478  32 415 440 442 941 259
 446 944 405 397  70 254 476 280 281 402 383  92 910 629 975 541 330 661
 743 588 586 579 590 153 556 877 564 409 349 631 389 477 802 201 972  80
 451 928 258 731 401 104 469 902 244 423 766 676 679 318 603 504 150 338
 496 183 808  48 454 266 861 799 249 463  71 620 252 608 930 589 666 842
 905 571 724 761 826 172 677 734 287  98  93 331 119 326 880 947 710 943
 194 373 320 151 417 560 452 279  65 688 685 890  20  22 544 105 875 310
 343 339 882 638 884 628 376 495 678 537 823 332 595  41 416 675 904 298
 472 957 960 313 668 622 263 203 404 935 814 260 572 669 559 914 974 205
 795 626 121  25 293 671 592 156 810 583 812 815 889 209  38 833  82 866
  89  97 949 175 829 955  23 599 160 294 231 470 912 100  27 292 256 382
 636  53 788 464  91 849 302 566 851 888 479 308 245 777 363 706 896 970
 813 144  17 918 915 698 867 942 963 157  78 449 540 170 208 921 522 169
 639 837 627 166 236 926 953 271 406  81 426 345 573  19 892 878 510 390
 609 612 528 487 887 656 958 883 836 606 879 501 933 873 786 328 707 239
 672 643 393  18 521 370 632 488 524 885 420 959 347 212 936 228 474 539
 117 102  28 911 621 718 327 235 171 498 387 314 307 803 834 816 581 899
 804 221 750 948 821 954  56 437 728 807 725 188 101 920 694 984 945 518
  10 969  24 336 615 618 898 824  76 798 931 640  83 195 832 644 466 158
 494 520 230 291 360 633 580 787  26 893 276 894 624 250 410 742 306 848
 613 227 652 903 173 553 841 872 783 701 753 468 193 197 649 587 126 907
  15 856 147 925 232   5 607 335 561   7 344 120 297 407 665 596 465 272
  31 341  50  44 642 598 155 278 818 916 729 547 604 551 769  63 906 757
 453 806 182 793 780 353 514 956 295 760 796 394 500 983 704   4 976 421
 142  79 218 282 895 241 196 507 616 475 191 305 927 886   0 214 178 591
 439 403 386 557 240 458 831 565  84 740  57  61 311 738 234 932 229 177
   6 364 917 809 908 303  29  73 159 601 602 971  16 435 705 978 555 695
 337 162  36 844 346 934 400 108 775 611 790 538 425 277 825 304 852 253
 273 270 226 225 575 913 650 352 546 584 431  49 112 562 817 202 456 459
 492 574 634 222 223 369 224 700  67 128  85 597 515 300 233 511 317 739
 111 648 929 924 486 719  34 549  51 419   9 605 711 859  55 275 870 533
 512 653 781 857 773 755 358 820 523   3 473 900 220 550 340 530 348  33
 939 646 526  13 210 791 190 855 938 509 503 940  52 585 408 981 519 641
 269 952 207 248  75  77 922  21  69 536 217 654  11 876 897  12 673 853
 651 445 350 200  46 161 199 499 285   8 342  99 667 858 274 937   2 168
 471 647 324 702 513 457 619 881 919 381 697  66 923 663 186 283 854 827
 792 299   1 548 576 614 577 828 238 545 635  62 610]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0126
INFO voc_eval.py: 171: [399 336 481 112 477  68 549 304 113  71 319 450 358 360 186 398 400  67
 452 471 321 320 310 249 470 407 272 309 464 408 302 428  66 241 347 110
 534 324 117 276 228 209 463 279 183 426  20 280 224 219 273 119 277 380
 291 490 334 473 363  10 454 401 533 487 457 448 290 371 216 365 352 348
 250 305 286  86 554  26  21 315 306 307 332 230  30 372 478 499 185 293
 296 301 551 330 361 423 436 459   4 123 191  16  93 114 176 338 215  19
 405 194   9 136 344 491 135 182 431 314 275 325 271  89 374 345 204 232
 208 364 412 353 356  40 460 486  25 178 493 269  13   1 141  23  27 492
 335 288 525 222 214 264 384 235 461   6  46 283 469 284   8  50 105 515
 524  48 518 532 190 480 243 455  99 416 168 472 148 543 366 139 339 268
 317 299 147 540   7 349 270 402 125 278 218 354 212 546 138 223 179 475
  45 247 513 369 308 466 446 251 421 258  62  33  92 337 506 362  43 495
 406 311 375  12  55 341 262 177 327 107 357 187 417 456 510 118 552 257
 343 329 101 539 504 520 154 390  94   3  90 298 173  37 303  51 553  44
 260 203 377 246 115 359 498 199 432 229  69 121 331 128 144 451 195 234
 508 221 289 535  24 396 292 206 388  41 192 509  28 328 403 231  96 433
 550  36 526 376  32 516   0  58 205  97 263 150 548 519 410 501 435  75
  31 482  11 409 541 387  15 184 180 514 108 274 355 392 368 102 132 106
 256 503 449 507 378 326 316 248 124 252 181 512  73 429  72 163 547 462
   5 395 120 397 370 414 295  47 281 104 437 140 143 259 538  74 346 220
 111  39 226 207 151 497 312 542 382  49 200 159 287 109 297 386 233 318
 266 350  65 211  76 244 420 172  54 164 340 127  34 254 465 496 145 458
  87 485 440 500  57 385  95 255 122 521  35 367  88  38 529  53 225 528
 394 415 201 149 425 427  18 381 240 544 413 165 545  17 134 174  63 236
 379 530 537 404  70  42 300  22 342   2 483 142 239 213 536 285  60 441
 166 227 522 294 453 531 447 156  29 411 130 210 511 100  64 253  84 488
 202 351 391 422 238 445 527 267 424 476 170 245 103 146 217 502  14 158
 333  78 161 323 484 155 322  81 467 479 474 133  82 152  61 126 160 237
 167 373 505  79  59 261 442  98 242 393 188 175 419 129 157 489 265 282
  85 197 313 389 193 383  56 131 162 444 494  77 430 438 169 443 468  83
 523 517 198  52 439 116 189  91 196 418 434  80 137 153 171]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1058
INFO voc_eval.py: 171: [246 149 240 236 214 242 230 188 140 245 248 216 183 133 186 145 221 198
 233 146 208 249 222 226 215 202 180 232 235 168  71 193  69 207 191 231
 211 227  77 174 138  52 220 176  37 241 173  39 181 200 171 115  35  41
  70  73 280  61 132  43  36  85   9  75 166  34 107 194  86 229 170  74
 135  57 204  47  32 184 206 213 219  13  79   1 189  54 169   2  64 136
 228 175  76 195 185  55  33 237 244 264  97 247 234 101 223  51 279 167
 111 205  30 225 179 100 210 262  58 239 209 130 148  44 108 192 105 187
 199 224 178 197 144 271 263  93  68 217 203  38  50 182  66 253 104 288
 114 117  18 243  63 109 106 267  82  98 116 137  67 156 102 172 218 255
   7 212 201 119  89  56 196 177 238 190 287 260 154 134 129 258  45  99
 158 150 164 110   3 163 259  53  26 141 251  19  16  87 122 272  49  72
 165  65 285 252  88  62  80 121  22 120  92 128 113 118  11  25  40  60
  59 112  12  94 155  46 278 266 143 268  24 125 277 289  84 153  83 103
 162   6 265   5  10  15  23 250   0 281 160 147  95  91 124  21   4  27
  48 126 123 142 282  42   8 131 139 151  96  90  14 256 127  81 270 257
 157  78 274 254  29  17 269  20 152 283  28 284 273 161 286 276 261 275
 159  31]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0207
INFO voc_eval.py: 171: [68 39 56 50 53 42 22 43 57 45 49 62 41 59  6  4 52 24 20 33 40 38 31 16
 19 69 25 30 74 37 46 36 32  7 67 63 60 58  2 51 61 65 28 54 64 27 29 35
 48 12  1 26 23  8 66 44 73 72 13 70 55 21  0 34 17  3 11 71  5  9 14 10
 18 15 75 47]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2159
INFO voc_eval.py: 171: [ 282  459 1069 ... 1299 1108 1106]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0245
INFO voc_eval.py: 171: [48 50 29 41 46 31 14 32 49 13 16 12  8 33 36 47  3 40 18  2 42 15 35 17
 30 28 34  9 43 39  5 11 44  1  7 38 20 45  4  0 10 37  6 22 26 25 27 19
 21 23 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [1067 1053 1661 ... 1117 1468 2101]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0169
INFO voc_eval.py: 171: [22  7 16 19 12  6  9 27 11 28 60 29 14  2 54 51  5 43 15 38 67 17 33 41
 37 61  1 58  3 36 23 46 44 48 47 50  8  4  0 55 24 13 45 18 20 66 59 34
 49 10 64 53 40 42 21 57 72 30 56 62 52 69 70 35 68 32 71 31 63 25 65 26
 39]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0421
INFO voc_eval.py: 171: [463 520 244 511 509 492 491 490 487 485 257 481 471 468 464 458 455 454
 531 535 240 581 579 573 211 218 219 608 188 309 315 319 185 180 321 178
 356 365 626 625 174 392 173 171 351 192 199 256 221 224 227 215 228 213
 230 233 195 235 400 242 206 246 203 252 255 201 238 409 348 622 537 540
 542 543 546 551 557 558 569 570 574 582 585 586 587 595 596 597 598 605
 607 532 411 530 523 452 457 459 460 462 164 465 467 474 478 479 483 484
 486 500 501 512 514 516 517 522 525 140 609  78  83  89 312 510 576 197
 507 505 303 503 190 502 499  14  20 183 182 181 317 177 583 584 578 202
 519 524 548 549 541 243 236 538 231 536 534 169 559 560 561 562 269  26
 528 527 568 526 268 338 545 469 476 612 475   9 147  52 472 153  58 589
 136 395 398 406 449 412  73 155 619 352 444 161 158 477 482 489 280 566
  72  62 565  71 564 322 234  67 225 397 493 264  82 618 394 358 466 186
 498 504 353 506 159 160 294  79 572 204 205  84 207 339  48 615  21 446
 544 617 258 518  23 533 456 515 513 363 362 316  51  50 333 259 364 276
 156  13 575 137 138 189 663 175 144 571 563 226  17 145 547 162 151  88
 152 154 163 553  76 391 157 580 590  57 318 496 495 347 324 480 473 360
  44 167 591  86 399 241 552 554 261 427 529 653 594 620 567  63  70  81
 470 675 170 149 443 336 665 556  45 232 508 539 296 555 453 666 401 431
 550 451 328  42 494 497 652  10 692 384 404 277 383 448 593 191 673 385
 671 669 176 521 592 631  74 134 660 611 396 588 150 434 300 613 272 488
 210 165 212 187 350 302 577 326 408 307 410 179 283 661 216 331 146 680
 428  49 345  60 193 217 445  94 690 651 254 166 388  11   8 306 628 354
 636 168  27 172 447 415 282 450 627 407 670 314 623 461 135 290 355 141
 439 440  46 614 120 604 337 196 330  41  35  15 662 209 624 658  75  54
 686  66 119 239 403 402 393  69 198 648 634 674  77 341 251 684 610 359
  32 184 313 413 602 657 208  39 100 148 639 297 123 421 334 229 125 357
 117 432 677 129 616 621 655 630  59 603 346  38 343 667 223  55  56 442
 340   6 649 124  25   2 121 288 335 329 606 390 102  19 682 638 106  85
 275 281 301 429 113 267 292 237  24 260  64 637 382 645 436 320 441 349
 664 433 633 250 387 249 694 654  47 405 103 435 222  87 139  12 687 266
 295 644 220 373   4 323  99 423 332 426 389 342 371  18 369  40 245 305
 632 132 109  22  53  68  36 678 278 430 143 681 247 101 691 683 344 126
  43 214 263 265 285 646  80  93 104 111 656 194 142 689 437 641 110 284
  34 327 643 685 679 286 271 635 375  30 368 200  28 310 248 695 697 114
 253 361   7  16 128 293 374 325 279 122 381  65 659  97 601 287 118 289
   5  90 270 688 380 308  31 131  37 107   3  98 262 299 693 108 438 378
  33  61 133 116 377 672 376 291 304 372 105 696 112 642  29 274   1 640
 311   0 273 668 298 367 379 416 676 629 424 115  92 127 647 650 425 419
  96  95 370  91 366 417 599 420 414 422 600 418 130 386]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0061
INFO voc_eval.py: 171: [ 49  35  56  52  22  53  37 117  23  54 119  38  11  10 110  34  33  32
 116  20 122 126 125  39  16  45 112  21  48 113 111  46  31  13  12  26
  29  25   6  86  51  87  15  24  89 109  27  88  19  78  41   9  28  75
 120 114  40   8 106  14 124  43   5 105  17  80  79 100  76  60  85  47
 104  90  68 101  99  42 118  91  71   7 123  92  18  77   3  55  36  64
  67   4  70 115 121  72  50  57  95  44  30  69  74  94  93  98  97 107
 108  83 103  81 102  61  59  96  58  66  65  73  62  63  82   0  84   1
   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0056
INFO voc_eval.py: 171: [110 108 143  33  83  36  82  22  31  40  42 113  51 114 116  39  89 117
 101   5  93   3   2  10 105  44   4 100  53  12  95  48  32 154  29 109
 227  25  58  87 115  16 213 206 202  50 194   6 112 156 199 137 235  86
  14 145 208  88  20 205  96  84 224  90 191 189   9 146  21 214 160 141
 139 129  56 177 242 131  57 232 184 102 207  94 147 247 215  63  27 230
 197  23 181  99   8 237 249 135  92 123 196  43  74 151 157 107 226   7
 178 220 225  37 239 218 210  35 134 133 190 174 241  45  61 118  68  59
 142  85 136  17 149 155 128 238 245 140 185 243 125  97 234  71  13  15
  24 162  18 221 153  54 216 209 167 158  91 212 111  60 132 229 127  30
  47 204 246 193 106 176  52  38 219 223 211 169 198 183  46 152 217 119
 130  26 250 240   0 180 188  81 186 253 126 166 222 124 261  28 161  78
 192 201  70  67 144 244 203  11 150 171 148 252 195 256  62 175  66  41
 248 165 182 170  72 200 254  55 233 179  49 236 172 251 168  34  69 228
 163 173   1  80  75 103  79  19 187 259 138 255 257 231  73  77 104 120
  76 159  98 121 164  64 122 263 260  65 262 258]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0163
INFO voc_eval.py: 171: [55 56 50 52 29 17 38 67 23 60 31 73 37 26 44 41 61 30 36  7 32 43 74 70
 62 19 24 59 57 18 33 72 51 21 45 12  2 49 58 71 28 34 14  3 13 35 65 39
 63 40  0 16 54 75 46 42 27  5 68 66 53 11  1 20 69 15 48  9 64 25 22 47
  4  6  8 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2020
INFO voc_eval.py: 171: [ 2246 34949  9719 ...  5590  5638  5647]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0602
INFO voc_eval.py: 171: [239 246 334 427 330 428 287 460  88  65  76 328  62 369 245 326 438 139
  64 280 412  91  86 415 459 278 385 414 149 470  80 434  42 122 426  66
 433 153 359  61 333  19 284  89 138  79 243  52 242 420 490 309 200 362
 221 335 218  38 325 471  74 480  35 487  84 350 123  53 324 152 347  13
  47  71 222 482 331 206  46 374  83 147  99  81 494 365 375 353 479  60
 366 404 332 475 225 378 329  77 327   7 429 307 137 418 204 455 286 358
 409 118  68  67 115  73 447 371 145 437 217  58  70 417 124 405 289  45
 435  54 244  36 380 456 450 485  78  56 311 219 236  85 497 465 133  55
  51  82 125 377  50 397  75 258 406 401  90  63 203 493 146 143 402 237
 310 436  57 452  59  69 364  22  72  87 382 441 461 316 208 209 361 312
  18 227 116 119 383 495 102 104 444 113 432 489 323 148 413 107 396 424
 453 387 265 205 410 120 416 101 319 314  16 308  32 264 162 279 165 352
 488 238 111 348 313  39 259 170 351 477 199 254 446 231 159 376 224  37
 232 411 391 226 109  30 132 422  11 228 215 478 320 269  31  20 167 150
 281 117  97 103   2 390 185  14 457 454 389 430 288 106 466 340 315 301
 379   1 304  43 250 255 486 274 285 384 135 198 425 130 188 134 151 172
 105 282 271 178 277 201 108 229 191  40 273 256 114 223 464 251  28 131
 317 247 373 345 386 462  44 211 140 142 230 213 263 283 442 363 275  94
  96 448 128 241 112 257 398 234 195  33 492 306 270 481 190 186  48 240
 174 343 110  93  41 252 129 168 202 207 368 166 341 491 321 249 408 449
   0 500 212 121 100 155 183 407 216  98 342 336 393 126 349 499 467 144
  15  92 419 157 169 210 367 194   4 381 421   3 476 423 163 173  95 468
 305 136 322 164  34 272 276 318 160 291 260 395 262 196 161 189 439 220
 339 344   6  21 193 346 171 337 267 214 261 370  12  10 403 400 484 158
 338 156 483  17 300 445 498 496 296 290 266 154 197 399 372 127 388 177
 187 235 179 233 141   5 463 184  49 268 176 357   9 253 394 297 472 175
  24 248 356  27 292 180 182 360 181 355 392 443  25 192  26   8  29  23
 298 303 473 293 451 440 354 469 474 431 294 458 299 302 295]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0800
INFO voc_eval.py: 171: [ 73  68  78  86 111  76  67 119  93 115  79  91  16  66  63 153  88  89
  24 163 100  61 217 167 226  36 138 135 157  65  64 152  39 148  92 170
 132  80 212 169  22 103 144  44  40  33 106  23 158  32 116  31  62 206
  38 159 117  35 202 141   5  30  74  13  71  69  96 120 224 181 123 219
 121 113  19 197 147  81 196  77  43 195 134   7  83 131  26 176  94 124
 168 151 207  45  97  84 164 127   1  47 104  28 178  29  41 112 154  72
 102 177 155  70 150 205  42 145  82  37 221  34 133 223 165 128  52  57
  56 225  60 142 222 108  10 185 161  21 166 175 143  49 126 130 146 110
 137  15 193 187 172   8 183  17 118  53 101  27  99 149  75  48 209  90
 156  58 125 140  85  50 192 179  46  87 107 171 114  11   6 136 162   2
 105 218 184 129 109  18   3   9 160 139  95  98 198 190 122  55 173  20
 180  54 204  51 216   0 199   4 211 220  12 201 213  59 200 215 188 174
  25  14 189 214 208 210 203 194 182 191 186]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0336
INFO voc_eval.py: 171: [ 77  80  63  25  62  28  64  75   8   0  73  79  36  67   1  54  34  83
  53  24  29  72 121  89 105  12  71  35  57  76  13   4   6 107  87 108
  10 119  11  82  81  23  59  97  18  50 109  32   5  49 110  47  65  74
 102  40  58 118  98  21 114   9  88  90 115  44  70  15  14  55  16  33
  26  20  45 117 101  42  39 104  52  94  66  96  99  41  46   3  95 103
  37  31 111 106   2  92 100  19   7 113  61  68  30  78 112 122  17  93
  38  60 120 116  69  91  56  48  86  51  22  27  85  43  84]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0392
INFO voc_eval.py: 171: [122 255 265 261  77  91  98 256 276 193 268  28 195 121 123  16 277 135
 128 149 129 130 201 132 125 257  89  86 127 137 152 262 215 269  84  81
 186 204 192 196 160 134 243 157 208 280  21  24 216 272 213 140  79 198
 194 159 258   8 221 138  61  83  75 212  41  31 202 197 249  22  40  12
 267 151 177 239 263 207 229 210  76  72 143 273  93 154   9 109 164  85
 285 155 105  15 248 278 252 191 107 170 183  87 187 266  17 227  42 108
 275  11 289  47 133  97 259 163 100  27 103 124  58  95 131 176 156  74
  99  63  92 184 274 146 214 253 110 190 270 139  29 219 203   3 282 179
 235  88 106 254 234 174 180  14 205  69 246 148 199 271 161 104  80 223
 245  33   1  71 244 126 173 283  82  50  68 211  49 162 290 224   2 225
  39 240  96  26 153  51 286  34  36  23  52  43 226  38  19  53   4  66
 189  37  46 166  32  55 288 145   0 158 242  25  10  78 250 218  59 241
 111 175   7  45 209 178 117  18 169 264  54 165  20 287 182 188  48 206
 172  35 144 181 102 220 232 118 112 168 142 200 147  73 281 101 141 171
 228 150 260  70  90  65 247 236 217  94  30  13 136 251   6 291 167 113
 222 230 115 185 237 231 238   5 279 116 284 233  44  60 114  57 120  56
 119  64  62  67]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1276
INFO voc_eval.py: 171: [148 210 192 122 104 223 215 196 190 188  61 150 147 132 128  83  86  91
  92  95 107 144 224 102   6 270   7 267 145 189 143 141 195  78 157 205
 151 111  19 211  62 288 177   1  99  26  89  66  23 262 263 287 108 100
 135  80 180 197 203 231 241 181 244  56 185 204 120 126 112 207 208 233
 225 285 117  24 268  49 129 154 106  60  96 238 172 149 295 245 119  17
 137 136 200  57 239 206 212 213 109 291 277 152 118 296   9  81  90 171
 193 229 124 187  50 218  29 265  88 110 280 165 114 138 116 243 174  18
 125 286   0 139  47 123  22 236 247 275 290 222  68 240 168  13  64  69
 101 131 169 237  93 253 289 230 127 164  51  76 261  94  58 214 260  16
 220 242 219 266 191  21  79 271 221  12  82 184  46 272 201 294 292  32
 256  97 281  37 103  15  77  75  38  27  98  85  54 284 199  63 179 167
 140  30 115 182  84  55 232 121 161 279  48  67 146 216 160  11 228 252
 113   4 183 156 155 166  31 105 153  73  52  70  44  71   3  10 282 254
 255  40 248  42 162  59  45 176 276  41  14 234  20  53  87  28 269 264
 198 209   2 293  34 283 202   8 235   5 194  74 217 142 274  65 178 158
 273 246 257  35 226  39  72  25 251  43 159 249 130 170 175 163 227 278
  36  33 186 173 250 258 259 133 134]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0241
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0646
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.078
INFO cross_voc_dataset_evaluator.py: 134: 0.135
INFO cross_voc_dataset_evaluator.py: 134: 0.013
INFO cross_voc_dataset_evaluator.py: 134: 0.106
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.216
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.017
INFO cross_voc_dataset_evaluator.py: 134: 0.042
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.016
INFO cross_voc_dataset_evaluator.py: 134: 0.202
INFO cross_voc_dataset_evaluator.py: 134: 0.060
INFO cross_voc_dataset_evaluator.py: 134: 0.080
INFO cross_voc_dataset_evaluator.py: 134: 0.034
INFO cross_voc_dataset_evaluator.py: 134: 0.039
INFO cross_voc_dataset_evaluator.py: 134: 0.128
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 135: 0.065
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.685s + 0.004s (eta: 0:01:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.469s + 0.004s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.486s + 0.004s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.487s + 0.004s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.507s + 0.005s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.508s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.512s + 0.005s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.509s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.494s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.485s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.477s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.479s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.482s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.693s + 0.004s (eta: 0:01:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.505s + 0.006s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.505s + 0.005s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.514s + 0.007s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.524s + 0.006s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.521s + 0.006s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.530s + 0.006s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.519s + 0.006s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.504s + 0.006s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.496s + 0.006s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.486s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.487s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.487s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.888s + 0.004s (eta: 0:01:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.576s + 0.007s (eta: 0:01:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.544s + 0.006s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.538s + 0.006s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.523s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.515s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.507s + 0.005s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.506s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.493s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.480s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.472s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.471s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.478s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step2999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.817s + 0.008s (eta: 0:01:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.510s + 0.005s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.528s + 0.005s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.518s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.502s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.498s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.501s + 0.005s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.498s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.483s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.473s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.464s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.460s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.465s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 76.139s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 40 107  82  43  41  18  15  87  34  30  68  93  29 108  33  17  21   7
  91  12  10   8   6  90  66  32  72   4  37  89  31  14  28  84  23  71
  26  78  42 104   5  70  85  19  16  80  59  81 103  47  24 102 106   9
  45  57  64  20  98  54  38  67  61  86  53  50  56  44  77  58  46  27
  13  83  88  99  48  51  65  75 100  36   0  49  25  73  62  94  60 105
 109  69   3   1   2  79  11  92  97  22  74  95 101  39  52  96  63  55
  76  35]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1314
INFO voc_eval.py: 171: [148 136  19  16 125 122 130  48 132  94 144  47 147  49 138  63  62 126
 150  21 142 131  52  58 129  88  61  15  75  20  17 151   6  54  64  89
  14  50  44  36  56 124 128  51  82  77  60  76 127 106 135 145 134  57
 121 115 108  68  91 133  55  96 112 110 137 139 104  81  78 141 152  23
  22  71 149   3 103  97 146 143 123  33   1  26 109 140  95  18  93 101
 113  85  29  65  41  10 117  40  67 114 116  80  59  74  99  46  92  27
  42  70   8  83  13 105  66 120  30 107  43   4   7  24 111  32   0   2
   5  45  37  35  39 119  69  28  79  86 118  84  31 100  87  53  38 102
   9  98  25  72  73  11  12  90  34]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0684
INFO voc_eval.py: 171: [827 852 422 ... 908 545  66]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0511
INFO voc_eval.py: 171: [426 515 370 511  78  73 386  80 385 589 477 349 333 122 427 121 430 262
 197 425 482 433  74 505 336 503 332 354 348 494 345 493 573 259 455 295
 119  25 391 485 371 401 201 429 356 451 297 126 248 293 222 591 325 593
 326 302  10 300 339 520 393 324  75 492 241  98 312 506 407 476 298 229
 223 127 525  27 395 254 308   5 572  23 337 360 379 512 246 328 536 380
 358 198 462  29 321 432 450 205 132 357 145 389 362 188  79 316 364 390
 562  21 322 124 219 292 296 144   8 487  20 363 193 440 103 351 221 408
 335 281 457  42  99  55 486 524  28  13  26 110 490 392 491  87 353 521
 523 190 526 441 381 151 311 438   1 301 228 236 557 271 514 288 338 578
 583 257   6  46 397   9 412 483 502 323 571 534  52 374 305 159 585 340
 306 507 148 373 231 226 484 289 539 529 528 195 359  64 133 134 375 369
 157 527 299 265   4 378  50 330 540 290 367 150  68 445 235 260 551 244
 547 273 428 365  65 200 383 509 101 481  14 234 544 331  35 388  34 454
 552 346 543 554 238 217   7 592 434 277 278 169 105 249 128 185 189 541
  51 548 563  45  33 350  39 154 405  37 415 435 347 444 431 108  76 538
 161 327 387  57  24 210 587 458 382 574 320 594  16  30  36 125 203 258
 129  63  81 109 463 436 352 215 233 251   0 112 403 414 417 280 553 218
 114  61  32 313 247 166 402 579 396  82 516 555 590 196 459 146 256 140
 173  95 275  69 245 272 398 567 549 368 550 558 545  17 268  12 478 479
 191 276 115 546 116 413 294 400 102 341  47 420 130 291 376 424 263 155
 530 588  83 461 149 120  41  40 465 580 329 192 163 153  49 315 361  11
 232 177 303 576 409   3 394 237 106 250 136 216  72 372 342 176 565 519
 443 439 464 309 220 118  97 317  58  96 411 421 184 537 584 568  56 131
 285 467 270 466 266 406 224 449 416 104 152 240 533  19  18 366 160  44
 180 211 186 586  71  62 437 318 418  22 252 577 469 475 559 123 307  92
 242 239 575 468 447 560 564 446  48 517 212  66 314 183 472 172 269 111
  77 377 227 522 569 355 535 141 113 171 448  60 570 510 179 284  85 214
 168 474 513 532  89 518 225 384  88 508 243 230 344 156 209 542 286 283
 135 495 143 182 581 399 343 453 261 107 410 419 471 582 170 204 165 187
 208   2 253 422 206 279 319  93 181 255  15 442  70 213 137 334 404 480
 207  94 470 460 473 138 489 199  67 531 304 164 456 496  84 264 566  91
 100 194  43 556 274 174  53 167 267 423  31 488 561 310 452 287 498  59
  38 499 202 282 497  54 500  90 147 501 158 139  86 504 162 175 117 178
 142]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1046
INFO voc_eval.py: 171: [258 159 261 228 143 249 150 197 196 193 230 154 245 254 257 226 224 255
 240 155 209 221 246 236 214  79 184 243 253 234 202 180  85  78  40 149
 242  81 244  37 206 207  87 185 241 227 188 124 237  42 205 293 250  60
 191  50  48  43 142  45 106 215  80  10  51 146 229 176 220  44  92  68
 235  86 217 112  83 200  65 181 276  15   2 145 238 259   3  55 222 183
  74 225 232  62 194 292 212 108 274 260  38 104  84 248 239 189 266 141
 265 195 211 115 116 179 256  76  59  26 186  35  66 286 252 300 121 151
 190 199 203 113 282 101  94 278 123  39  41 247  57 178 231  77 107 110
 192  95 233  46 216 223  52 147  63 122 218 127 269 182 170 198  16 111
 103 126 201 117 109  90  19 144 273 299 187 140 208 167 160   1 213 219
 174 166 272 210 204 114 161  31 105 251 102 296 128 177 263  18 131 138
 284  13  53 173  64 264   9 152  58  99  93  82  96 271 135  88 130  27
 139  61   7 120  30 290 118  75  69 164  47 301  25  12  70 119 156 288
 279 175  28 153 125  89   5 134 289  54 262  24   4  11  91   0  32 148
 158   6  71 277  21 291 132  67 270 133 136 162 268  98  49 157  29   8
 165  56 100  97  72 287 298 267  73  34 137 129  20 295 281 163  17  14
 297  23 172 283  33 168  22 275 294 285 280 171 169  36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0206
INFO voc_eval.py: 171: [69 58 37 51 53 40 59 46 41 60 24 42 50  6 62  3 34 19 54 39 38 32 18 15
 31 47 64 22 26 67 61 63 33  4 21 28 30 52 72 65 55 66 45 48  1 49  8 11
 73 44  7 68 57 25 12 27  0 43 70 23 20 35 56  2 16  5  9 13 29 10 17 71
 14 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2163
INFO voc_eval.py: 171: [ 861  482  486 ... 1341 1340 1147]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0239
INFO voc_eval.py: 171: [51 53 48 44 31 34 18 32 52 17 16 47 35  9 41 36 37 20 46 19  6 33 49  8
 12  3 39 30  2 15 14 22 43  7  0  5 38  1 40 11  4 24 45 27 13 21 23 50
 29 10 26 28 42 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [1196  738 1763 ... 1250 1239 2349]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0152
INFO voc_eval.py: 171: [18  5 15  8 17 23  4  6 25 24  9  2 14  1 39 19 11 34 33 36 28 46  3 32
 12 40 44 41  0 13 20 47 45 16  7 10 31 42 49 38 43 37 27 51 30 50 26 29
 21 48 22 35]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0439
INFO voc_eval.py: 171: [492 293 292 630 523 278 520 626 580 294 518 494 267 619 264 241 261 499
 498 506 509 525 576 229 553 563 549 503 567 577 542 532 253 530 528 447
 526 527 435 531 199 201 204 212 218 207 217 209 215 197 510 664 195 554
 552 545 544 533 161 524 196 521 181 517 187 516 192 514 194 180 220 501
 230 290 291 670 672 454 446 434 433 429 399 388 381 378 377 338 374 367
 343 365 348 355 668 283 277 275 232 233 502 236 237 555 500 242 245 249
 223 250 497 255 256 257 259 260 496 265 269 490 252 556 353  60 572 622
 575 639 579  98 638 581 593  15 628 629 582 589 636 583  90 632 587 635
 584  87  22 571 598 574 607 650 599 649 616 657 666 608 603 651 564 614
 601 602 560 561 615 558 203 610 210  78 206 631  54 248 234 592 591 620
 597 239 623 627  82  58 286  30  70 488 221 244  71 618 508 243 198 190
 586 522 578 323  83 570 392 640 386 539 540 541 385 568 653 655 548 565
 346 562 430 432 573 179 585 611  89 450 185 448  92 311 437 307 613 172
 173 175 352 396 661 660 634 262 280  24 617 665 281  26  17 270  12 621
 390 495 397 395 489 700 557 176 512 188 184 588 537 569 183  96 546 213
 590 216 101 710 505 225 529 177 227 609 166 174  50 171 551 547 519 251
  56 559 606 698 420 422  97 419 543 713 350 349 357 550 364 719 718 717
 692 716 566 382 536 535 393 109 315 612 699 604  65 504 493 491 624 507
 481 285  76 314 211 436 637 513 471 258  86 515 178  80 302 182 511 600
 596 595 439 594 375 485 633 534 538 475 625 480 371 708 663 193 709 162
 160 156 202  49 205 152  51 208  67 304 739  14 605 470 443  73 674  84
 421 696  94 167 451 214 104 186 656 337 191 361 231 189 652 418  52 425
 363 452  61 453 235 359 335 342 671 372  27  10 238 200 389 387  40 482
  19 705 724 133 469 721 159 274 449 704 456 487 486 138 276 240  11  18
 341 736  55 368  64 658 669 707 645 648  35  38  47 148 114 733 219 730
 263 720 659 347 295 391 646 728 321 139 273 360  43 715 484 441 455 383
 442 222  41  81 228 369 345 247 379 327 473 116 680  85 141 142 303  63
  69 407 464 703 714 474 147 329 129 331 431 100 654  25 667 351 308 370
 476 695 146 445 477  95 679 163  23   8   4 662 282 689 384 694 427 316
 121 362 683 254 288 325 697 408 468 647 313 117 336 726 113 712 483  74
 322 687 287  79 738  34 376 424 673 289 124 330 168 682 702 465 417  48
 144  16 151  62 157 226  31 725   6 145 279 128 105 380  21  53 115  37
  93 169 681 319 340 701 676 426 366 440 732 675  99 118 126 153 678 400
 102 472 737 268 143 130 354 108 428 438 246 299 409 706 398 444  45 224
  20 317  57 305 266 309  91 686 135 320 731 300 411 122   3 111 410 131
 688 734 333 729 344 373 112 356 127 727  44 324   9 644 358  42  29 123
 296 155 394 120   7 272  28 690 405 406  75  33  36   5 416  39 685 742
 334 478  77  68 284 677 479  59  88 110 735  46 415 740 741 328 723 106
 306 684 412 404 297 298 132 339  72  66  32 140 125 103 165   0 413 693
 134 312 466 402 722 136 459  13   1 461 467 711 310 318 326 150 332 301
 414 643 691 271 149 107 164 158 137 403   2 170 119 458 462 463 460 401
 457 642 154 641 423]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0065
INFO voc_eval.py: 171: [ 58  46  62  33  48  44  64 151  34  22  66  47  43 149 143  26 152  45
  30 148 156  56  21  49 159  32 145  65 158  23 107  41 144  39 110 114
  28  17  42  31  59  36 154 112  35  25 108 111  51  94  19  29 109  40
 113  38  90 153  52 134  20 157  70 142 105  53 136  16  18  55  24  93
 155  96 118 146 106  95 135  50 138  73 115 150  91 160  82  87  92  27
 133  67  74 127  61  88  13  54 128  15  72 131  78  89 130  37 124 120
 140  57  98 119 121 117 116 125 101 123 147  80  75 141  81 137  71  68
  97 126  60 104 139  83 122  69 132  99   4  79  63  12 100 129  14  76
 103 102  77   8  84  10   9   7   3  86   1  85  11   5   6   0   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0070
INFO voc_eval.py: 171: [114  33 105  80  30  81  40  41  25 111  92  21  46 112  47 108   2 116
  10  94 140  58   5 207  98 150 212 154 226 160  56  55 106  42  50  89
  16  32  87   3   4 142  22   7  86  13 138 152   6   8 217  90 200  84
 136  18  20 223  82 109 193 202 196 210 235 144 229 185 132 191 179 130
 155 246  29  88 166 231 248 110  44 242 218  54 145  99  96 237 104 141
 121 143 222 157  24  15 198 208 197   9 148 227  76 224  12 177 175 115
  37  66 206  48  53 192 219  14  62 238 135 153 128 187 241  83 102  26
 127  95 151 126  71  91 167  35  64 243 161  59 133 113 244 245  51 216
  19 183  28  43 129 107  61  31 214 213  93 215 176 134  34  38  79 209
 228 103 205 156 125 165 221  11 146 139  57 131 263  52 194 211 182 162
  63 169  36 188 180 225 249   0  72  39  23 189 239 149 172  70 186 163
  85 252  27 124 240 195  45 199 123  65 230  73 178 204 255 170 236 100
 203 256 173 122 168 232 158  77 250 247  68 174 261  74 251 201 181 159
  49  78  67  60  17 184 234 147  75 137  97 190 117   1 253 101 118 254
 220 233 262 164 119 260 171 120  69 259 258 264 257]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0164
INFO voc_eval.py: 171: [63 41 57 64 33 18 60 75 26 83 69 35 40 48 38 30 45 67 47 77 31 34  7 68
 36 22 80 27 81 70 37 49 58  2 55 19 39 15 59 82 79  3 66 56 32 72 43 65
 42 62  1 14  0 44 74 53 23 17 73 46 50 78 25 54 13  5 20 28 61 76 16 21
 12 71 51  8 29 24 52  4 11 10  6  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.2020
INFO voc_eval.py: 171: [15074  2218  7776 ...  5548  5585  5582]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0517
INFO voc_eval.py: 171: [258 451 351 253 450 347 484 299  69 293  80 346 463 257 342  65 146 392
 505 456  93  94 297 439 495  82 437  72 405 260  68 153  62 127 449 438
  44  92 457 157 295 256 230  15 515 380  52 144 211  86 344  41 341 442
 233 461 340 370  13 218 252 511 393 493 504 444 384  56 228 465  87 128
  77 152  37 368 507 348 395 156 324  48 396 294 106  73 375 236  63  90
   8 519 216  95  54  70 398 452 298 149 320 145 349 503 343 345 321 499
 460 150 382 387 472  57  61  71  74  84 391 379 458 526 433 350 248 302
  89 509 231  38  78 120 402 474 140  47  53 429  88  58  40  83  85 480
 420  98 520  55 213 459 291 399 440 273  79  75 129 130  91 525 518 327
 250  21 125 431  66 151  76  59 479 496 426  60 111 220 427 238  12 408
 112 325 333 334 404 467 292 485 219 448 385 235  81  18 410 249 478 278
 109 110 514 473 441 113 447 422 251 455 171  67 217 124 164 167 513 102
 177 372 272 277 161 434 243  64 322  34 502  39 210 221 237 469 397 107
 446 261 371 415 239  10 436 268 435 500 329 282  20 336  32 139 358 414
 245 196 154 413   2  46 316 481 131 173 314 483  33 300 366 100 488 185
 406 339 287 330 179 159 114 142 445 264 359 269 301 489 118 453 332 400
 148   1 121 510 136 234 296 141 137 265 240 199 201 242 115 270 212 224
  42 487 363 284  30 147 226 209 119 285  96 522   4 138 117  45 409 274
 288 227 223 254 206 255 476 172 326 225 319 403 516 283 181 468 271 197
 192 386 174 123 202 361  49 421 101 266 290 134 506  43 337 517 524 374
 263 108  35 116 389  23 132 126 122 338 105   0 178 475 490 417  16 432
 352 214 205 388 222 175 170 430 318 491 367 335 365 169 373 103 369 143
 303   3 498  99 104 165 160 419 443 215 166 276 207 200 204  36  22 289
 407 331 286 135 168 357 247 232 279 158 364   7 462  11 187 353 186 313
 183 180 208 362 512  97 163 390 423 521 508 471 523 328 425 275 310 355
 133 428 323 198 244 189 246 354  19 280 356 411 182 195 424   5 162 259
 360 176 191 155 378 193 486  17 412 267 190 309  51 306 492  50 418 262
 281 241 307   9 194 229  28  27 394  29   6 401 377 188 184 383  14 381
  31  24 203 416  25 470 497  26 477 317 312 376 464 494 305 501 308 454
 315 482 311 304 466]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0851
INFO voc_eval.py: 171: [ 64  57  48 115  52  67  56  75  96  73  84 107  55  83 136  14  68  59
  15  72 190 111  30 141 105 148 100 195  98  80  53 187  63 125 145 139
  49  33  61 127 130 133  85 173  27 176  69  99  77  95  76  25  31  18
 188  29 182 101 160  50  24  74 137  88  58 191  92  19  54  26  37  35
  60 108  97  36  62 154  79   2 147 116  10  91 157 102 123 109 156 184
   9 114 177 124  66  71  20 132  13 138 146 164  34 121 142  28 134 129
  32 117  87 113  38 174 118 162 165 131 186 192 172 152 194  45 170 126
 175   6 158  21 103 112 135  90  23 106 193   0  81  51 179 128 143 140
  82 120 166  46  70  86 163 178  42 151 149  43  39 122 119  11 104  22
  65 144  16 150   4   7 167   8  78  47 110  93  89  94  12 185   1 155
  40 181 161 169 189  44 159 180 168  41  17 171 153   3 183   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0421
INFO voc_eval.py: 171: [ 90  86  83  71  70  88  73   8  31  28  79  74  15   0   1  38  56 136
  57  40  27  85  33 115  39 117  13  11  10  80  98   6 114   4  64  78
  97  89  52  66  92 108 112   5  72  21  55 120  59  82  45 119   9  53
  35  65 113  77  25  14  24  48 118  99 127 130  49  37 131  20  36  29
  47  91  46 116 105  17  44 111 121 126 109   3 106 107 123  58  67  51
  16 137  32   2 124  12  87 135  69 110 103 125  81  19  60  34   7  42
  23 134 132  76  18  41  84 104 100  50 122  63 102  75 129  61  94 133
  68  62 101 128  22  96  54  26  30  95  93  43]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0325
INFO voc_eval.py: 171: [136 281 238  28 285 287 176 107 299 165 207 275 273 150 147 144 143 142
 140 139 137  91 210 135 222  87  93 279 280  18  97 216 278 217  84 221
 209 149 265  86 227 296 231 232 235 226 151 277   9  67 167 168 110 172
 301 175 203  21  25 270 290  82  45  90  44 155 170  31  12 249 193 213
  88 233 224 223  83 215 212 284  22 262 286 288 260 169 297 211 228 154
 269 118 307  10  80 295 220 115 204 103   8 111  46 186 179 298 201 141
 117  19 274 164  15 163 246 106  32 119  66 145 146  51  70 104  50 138
 206 178 171  33 101 192 113 272 219 120  13 159 229 153 255 191 195  81
  85 214 253  96 291 294 268 161 266  27 276 199 293 304 267 190 311  34
 241  55 116  78 303 282 202  53  42  43  77 114  23 177  92  56  36  58
  24 105 251 242  79   3  30 261 243  62  57  49  68  74 234  60 205   1
 312 244  37  61 162  41 308 181   4  40 271 310 158  11 174 263 109 194
  89 264 122  26  16 133 200  48 198 157   7 152 185 208  52 189  20 254
 180 160 230   2   0  75 309  17  29  39 184 237 121 257 102 112 250 256
 108 173 129 187 100 248  64 302  14 283 156 166  94  35 258 183 123 196
 240 236 188 289 252 300 292 182  63  47  95 218 148 125  59 124 126 225
 134  99  54 197 259 306  38  98 305   6   5 245  72 131 247 132  65 239
  71 127 128  69 130  76  73]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.1543
INFO voc_eval.py: 171: [165  94 100 101 108 114 115 117 124 142 167 209 219 170 256 254   6   7
 243 223 220 318  95 304 300 106 120 126 128 136 148 163 239 168 169  76
 181 202 238 213 216 232 241  73   0  16 156  93  99  24 111 207 212  74
  15 157 273 222  67 118  62 275 237 259 255 322 147 296 329 107 302 198
 129 175 180 135 276 159 150  18  68 225 226 319 228 294 231  26 125  14
 235 158 138  36 178 152 113 134 271   9 326 233  64 214 266 203  71 131
 310 199 313 245 190 297  12 141  60 251  61 221 174  23  19 102 262 272
 146 263 265 288 270 250 197 252  79  82 325 133  33   2 309 320 109  58
 278 323  11 112 145 143 195  87 127 204 306  90 218 103 274  70 171 242
 258 189 162  46 160 321 122 305 144 173  92 211  59 217  91  96 298 253
 227 210  42  88  32 123 292  27  55 327 164  97 139 314  13 116  34 194
 293 268 179 286 205 201 166   3 187 317  56 132 299 312 192  83  47 290
 260  81 247 289 161 176 269  78 184   4 240  10 267  65 130 191 234  48
  69 291 110  31 264 229  49  77 177 308 151 315  72   5 105 279   1 303
 295  45  21  66 261 330  75 224  54 119 172  28 248  22  86   8 316  57
 307 206 249 155 287  84 277  37 246  17  98 183 185 208 284 121  35 324
  41 282 257 301 137  29 328  51 140 188  89 230 104  25  80 149  20  85
 311 281 283 186 285 280 200  43  52  53 215  63 244  44 196  38  30 236
  40 182  39  50 153 193 154]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0271
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0673
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.131
INFO cross_voc_dataset_evaluator.py: 134: 0.068
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.105
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.216
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.015
INFO cross_voc_dataset_evaluator.py: 134: 0.044
INFO cross_voc_dataset_evaluator.py: 134: 0.007
INFO cross_voc_dataset_evaluator.py: 134: 0.007
INFO cross_voc_dataset_evaluator.py: 134: 0.016
INFO cross_voc_dataset_evaluator.py: 134: 0.202
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.085
INFO cross_voc_dataset_evaluator.py: 134: 0.042
INFO cross_voc_dataset_evaluator.py: 134: 0.032
INFO cross_voc_dataset_evaluator.py: 134: 0.154
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 135: 0.067
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.788s + 0.004s (eta: 0:01:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.525s + 0.005s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.517s + 0.005s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.519s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.535s + 0.005s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.519s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.514s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.504s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.489s + 0.004s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.482s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.481s + 0.004s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.485s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.491s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.930s + 0.003s (eta: 0:01:55)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.524s + 0.005s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.542s + 0.007s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.553s + 0.008s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.539s + 0.007s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.543s + 0.007s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.539s + 0.006s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.517s + 0.006s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.502s + 0.006s (eta: 0:00:22)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.491s + 0.006s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.494s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.501s + 0.006s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.500s + 0.005s (eta: 0:00:02)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.832s + 0.004s (eta: 0:01:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.551s + 0.007s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.513s + 0.006s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.511s + 0.006s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.496s + 0.007s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.498s + 0.006s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.500s + 0.006s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.486s + 0.005s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.474s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.468s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.461s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.465s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.472s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.681s + 0.003s (eta: 0:01:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.511s + 0.004s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.514s + 0.004s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.517s + 0.004s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.523s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.515s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.514s + 0.005s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.496s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.485s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.476s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.469s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.476s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.478s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 76.960s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [121  22  92  19  45  43  33  17  35  37 103  39  42  95  74  12  14  21
 119  41  26   9   8   7  23 100  72  34  10   5  27  79  32  98  18  99
  94  80  31  44  40 117  87  93  76  20  24  49  62 116  90  89 112 107
 118  11  61  48  25  68  97  52  15 114  73  57  30  56  91  60  47 108
  86  29  16  96 111  50  64 101  38   0  70 106  51  77 113  54  65  85
 120  28 102   4  88 115  83  13   1   2  75  63  46  59  67  69  71 110
  53  84  66 109  82  55 105 104  58  81  36   3  78   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0856
INFO voc_eval.py: 171: [155 138 135  17  49 146  20 141  50 132 131 126  23 144  98 150 153 158
  91  51  64  54  65  16  60  63 136  21  77  46 157   5  92  94 134  56
  66  15  18  37  53  52  58 152 111 129 133  78  62  59  70 118 113 140
  88  95 100 114  57 127 139 142 110 143 145  79 159 151 130  22  85   2
 117  73 154   4  24 108 148 101 156 149  35 147  27 128  19  99  42  87
 137   1  11 121  41  97 105  31  69 109 122  67  48  82  43  61 104  72
   8  76  96  86  29  34  44  14 124  47  68 116  32 119 120 112  38  26
 115  36 102 123  71  89   3  80   0  30  81  39  45  33 125  55 106 107
   7  40  74 103  75  93  84  12  13  10  25  28  90  83   9   6]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0621
INFO voc_eval.py: 171: [859 429 430 ...  71 927 455]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0097
INFO voc_eval.py: 171: [542 441 355 539  79  82 127  87 497 391 390 618 126 447  80 440 530 448
 442 532 504 200 343 336 348 398 260 517 291 368 124 351 513 444 601 264
 296 132 412 374   9 507 620 301 302 473 245  81 205 460  27  16 357 621
 519 298 325 324 327 226 307 533 106  86 496 421 338  11 602 331 385 549
 402 234 299 411 233 134 554 405 414 345 488 446 364 201 334 329 151 397
 365 330 340 258 589 538 568 243 250  30 232 196 321 139 320 190 129 150
 110 514 369 362 360 510 457  12  23 281 208   8 359 295  44 410 246 394
 383 553 297 223 506 308 105 458 289 339  29 387  26 512 511 472 225 192
  22 341 454 353 612 112 400  57 271   3 550  94 607   4 424  48 560 585
 544 406 541 157 239 529 314 312 552  10 257 361 342 502 199 306 515  67
 535  55 265 231 155 380 377 167 600 305 326 290 300 229 420 262 140 556
 557 564 503 462 407 142 555 378 166   6 371 570 238 571 449 275 443 379
  52 370 388 156 386  70 615 236 372  36 579 580 584  35 349 536  74 203
 501 107 450 188   7 613  15 333 278 279  28 207 399  34  50 114 575  38
 241 479  85 311 428 384 574 569 350 578 622 184  31  40  46 191 220  84
 160  25 135 259  37 572 445 461 115 130 354  66 415 328 213 453 163 248
  17 562  69 358 487 219 581   0 282 413 255 152 404 136  33 396 417 603
  88 431 118 427 109 543  68 403 287 119 409 595 382 608 373 173 482 485
  76 175 401 252 240 619 247 582 147 121 277 276 101  47 268 137 272 253
  14 498 583  18 577 586 344 426 499 293 558 475 125 108  42 436 576 194
 389 162 593  13 294 480  41 616 609 471 318 273   2 332  51 468 214 455
 170 604 159 303 141  78 375 484 451 195 317 187 381 422 367 337 376 596
 548 144 178 116  64 237 221 309 288 418 437 123 566 478  77  59 352 425
  56 185 138 158 319 103 251 224  21 466 452 614  20 567 102 286 430  45
 189 561 483  89 111  65 263 169 181  97 433 494 242 516 617  62 128 322
 592 227 267 489  24 588 416 244 605 285 464 392 174  49 590 177  96 316
  83 269 215 551 356 393 186 537 540 598 120 597  61 284 565 408 149 206
 419 249 176 292 117 534 545  72 230 217   1 481 491 573 547 518 432 490
  93 347 456 172 197 183 161  91 599 611 235   5 610 469 438 346 212 323
  19 256 113 228 270 335 218 423 143 280 171 222 500 463  60 146 254  99
 210 164 486 216 211 395  43 492 467  73 476  53 100 474 304 261  32 509
 559 520 606 459 470 435 165 274 439 104 202  90  39  63 193 153 131 522
 198 508  98 587 591 526 310 283 363 594 204 366  54 546 525 315 313 495
 266 154 493 434 145  75  58 527 521  71 524 505 429 477 465 531 180 528
 523  95  92 168 179 563 122 209 148 182 133]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.0584
INFO voc_eval.py: 171: [270 272 167 262 149 238 206 158 243 205  54 268 266  81 256 211 162 161
 236 267 237 208 251 229 219  80 224 257 248 233 193 191 210 254 255 242
 217 156 247  87 264  83  39  38 253 252 249  90 258 241  82 148 216 207
 194 199  61 110  48 307 244  49  47  45 185 260  42 203  43  10 226 232
 153  95 225 292 198 151 315   4 271  66  40 239  53  68  89  41 235   2
  26 240  86 222 269  59  14  76 113  78 145 306 282 122  24 123 220 277
 112 259 107 190 301 250  88 195 200 201 202 209 157 294 187 204 215 126
  58  56  62 116  69 105 263  36  79 227 287 164  97 114 197 261 312 212
 111  98 179 234  50  51 106 108  44 127 278 245 246 276 115 192 121 213
 280 150 129  15 214  92 231  17 144 132 313 228   1 175 283 178 183 172
  31 274 104 223 109 196 130 311   3 218 140 221 189  18 275 118 188 182
 299 137 230 102 265 273   9  65  55  57 159 177  96  84 143  64 125  28
 302  23  29 135  99 133 316   7 120  93  77 173  46 134 124  71 295 184
 289 166  27 155 303 296 154  12 128  25 165  30  91   0 139  20  52   5
   6 304  85 163  73  11 142 291 285 160 169 138 146 170 288 281  70  60
 141 279   8  33 101  67 176  63 117  72 310 152  19  13 103  94 100 168
 300  35 147 309 136 131  75 314 119  32 171 298  74  16  21  34  22 308
 284 290 293 286 181 305 297 180 174 186  37]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0224
INFO voc_eval.py: 171: [71 40 53 55 62 43 44 54 64 27  4 61 42 66 35  2 19 49 41 34 69 18 15 68
 32 67 48 21 72 29 33  3 77 30 22 70 46  5  1 31 51 23 10  7 76  6 73 47
 52 58 59 11 56 25 65 57 60 74 38 45 26 63 24 20  0 36 16 37  8 12  9 75
 14 39 17 28 13 50]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1427
INFO voc_eval.py: 171: [ 491  315  313 ... 1656 1406 1404]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0247
INFO voc_eval.py: 171: [47 44 48 40 33 30 31 20 19 49 43 18 34 11 36 38 35  3 42 22 21  9 32 45
  6 14 12  2 37 39  8 29  0 23 10  1 17 16  5  7 41  4 26 24 15 46 28 13
 27 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0364
INFO voc_eval.py: 171: [1288 1449 1272 ... 1362 2520 1758]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0166
INFO voc_eval.py: 171: [22  6 18 11 29 20  5  8 31  2 30 12 17  0 13 49 60 43 23 41 36  4 47 15
 40 27 16 24 52 53 58  3  1 54 66 25 61  9 51 59 19 14 38 48 56 50 10 45
  7 37 35 21 65 57 55 68 64 34 67 39 28 63 62 44 42 46 26 33 32]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0418
INFO voc_eval.py: 171: [587 306 537 291 307 310 658 581 668 281 612 672 592 674 274 478 239 242
 606 602 259 618 264 266 539 681 680 267 268 269 270 466 666 540 615 541
 560 562 565 566 552 567 568 569 548 572 661 542 543 617 391 638 623 621
 620 404 405 216 217 408 430 649 249 699 639 251 683 328 390 416 691 418
 692 425 218 693 250 421 616 214 213 624 223 643 371 186 202 642 228 627
 628 192 201 193 194 368 362 631 629 373 206 669 670 227 237 380 637 211
 675 626 210 647 378 636 171 174 673 232 671 208 207 377 409 258 613 290
 531 530 528 287 579 657 580 535 664 584 280 589 278 591 614 655 654 583
 488 536  34 296 564 563 561 559 557 556 303 538 554 551  23 305 549 547
  28 545 544 553  71 640 480 255 712 598 707  97 708 597 100 262 112 103
 604 710 220 263 601 462 596 477 593 594 611 273  80 463 260 271 468 608
 467  86 714 323 302 648 650 253 245 225 247 261 226 653 244 659 641 656
 279 233 234 335 224 644 660 104  67 697 427 428 578 607 605 603 106 343
 695 585 599  93 461  90  85 469 595 479 485  69 586 529 527 414 571 198
 197 189 188 187 184 570 176 419 677 172  40 619 298 534 678  37  65  24
 625 510 588 590 555 533 507 574 582 635 320 634 633 374 375 334 646 385
 389 410 422 424 609 652 450 451 453 600 318 288 576  18 110  77  33 238
 667 119  88 229  92 183 241 221  99 190 195 196 766 205  29 203 706 200
 185 246 111 679 744 762 162 760 293 763 575 294 745 265 747 737 702 297
 703  61 740 630 397 546 682 676 622 524 704  22 550 756 177 755 610 167
 191 169 209 632 786 577 513 663 665 532 746 272  60 573 248 470 651  32
 222 356 204 108 219 452 401 716  17 411 743  51 140 508 449  81 454 698
 429  96 109  62 487 694 558 525 366 212 387 662 361 275 256 254 199 157
 645 170 700 490 713 153  72 125 458 768 360 486 754 520  26 311 154 475
 407 151 398 782  63 483 417  42 393 751 158 482 277  84 276 257 484 215
 772  52 308 126 523 149 394  19  66 322 388 526  45 383 319 436 289 489
  75 117 365 359 286  49 481 732 331  56 711 778 400 155 688 689 690 420
 522 701 753 764 285  98  94 761 777 235 336  58 509 512 340 734 406 384
 500  79 395 721  39 750  74 159 160 124 161 376 372 353 350 101 348  30
 230  83 474 709 438 516 403 514 741 396 131 127 386 413 720  16 696 300
 173  11 345 148 771 725 730 231 784 742 252 465 342 240 330 152 518 134
 135 506 739 705 476 759 301 459 352 724 521 441  91 770 139  41 501 517
 304 791 114  27 299 402 105 341 120 715 446 180 168 415 163 123 775  59
  48  13 129 457  25 292 748 718 182 137 738 783  44 364  64 316 464 321
 749 412 138 733 132 717 426 437 472 392   9  55 752 143 113  31 115  43
  50 442 511 282 326 145 332  20 723 107 456  68 337 236 780  54 136 133
 243 471 776  38 179 473 379 333 399 460 339 367 440 382 790 774 312 370
  53 313 773 687 423  10 317 351 789 130 156 729 118  36 779 358 433 284
  14 357  12 142 102 165  87  57  15 381 787 121 788  89 324 722 329  47
 439 144 519 315  35 727 116 349 447   0  70 141 314 434   3 515 448 719
 128 147 503 769 492  78  46 122 765 146 736 363 757 435 496 728  21 369
 504  82 181 327  76 443 767  73 726 785 354 686 445 325 346 347 758 309
 338 432 344 498   2 781 355 283 493 178   6 499 444 295   4 491 164 175
 150 166   5   1   8   7 731  95 685 431 735 505 684 494 495 497 502 455]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0060
INFO voc_eval.py: 171: [ 60  41  66  44  38  29 157  17  63  40  30  64 158 148 154 156  23  21
 166  45 162  39 153  27  16 164  46  28  53  62 150  18 149 106 114 110
  26  50  35  25 160 112 111  51 108  32  48  20 109 113  92  31  36 159
  56  24  43 107  14  12  88  34  55  10 161  57 138 163  42 144  47 104
  13 118 140 165 117  94  37  93  15  89  91  19  67 155 151  72 120  54
  90 139 105  22 142 132  83  58 131 137  33  85  82  59 135  87   9 116
 152 136 134  96  77  75 121 143 128 122 123  69 119  78  11  95 125  99
  65  71  61 141 126  70 101 145 115  49 147 146  68  79 103  52 127  97
 130  98 129 133 124   4 102   8  73  76  84 100   5  86  74  81   3   2
  80   7   6   1   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0090
INFO voc_eval.py: 171: [109 118  92  28  81  40 221  45 113  48 111 110  51  52 105 102  90  83
 155  24  38   4  18   1  13  54 101   6   7  96  47 164  42   2  15  88
   3 159 144 215 139 140  32  82  23 142  19 234  84 225 236  87 244  91
  86  99  21 200   5  22 207 209 156 141 135 130 106 220 116 202  20 228
 161  95 196 191  89 184   9 254  29  36  41  56 245 229  10 257 203 256
 148 146 117 250 138 240 227 226 150 217 124  39  77  17 158  14  37 205
 182 183  97 198 137 119  65 235  46  57 171 157 211  85 114 247 249 132
  62 252 201  26 131 129 128 185 166 233 143 251  34 134  53 188 199  58
  43  72  64 112  61  93 232 108  33 136 223  35 160 231 168 219 218 253
 194 238  12  31 241  98 127  80 115 169 167 133 152 153 212 216 214 224
 165  55 267 222 193   0  63 172 177  59 190 174 237  25 248  44  74 230
 276 192 126  60 204 125  94 151  71  69 145  11  66 103 154 175 258  16
 213 173 268  73 149 239 260   8 163 179 206 210 246 187 255 120 266 180
 189 262  67 259 100  50 181  78  75 186 122 273 197  27  79  30 107 261
 195 263  49 265 243 104 242 170 178 162 121  76 208 147 176 264 274 123
 271 270 269  68  70 272 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0182
INFO voc_eval.py: 171: [67 43 74 69 35 20 61 88 17 42 68 40 37 49 32 50 84 48 36 38 33 23 66 56
 86 81  7 27  2 39 60 41 16 87 57 85  3 75 73 14 78 34 63 72 46 58 65 45
 29 64  0 44 15 24  1 47 80 79 53 82 19 51 26 62 21 55 28 22  5 76 13 59
 70 12 83 30  8 71 52 25 77 31 18  4 54 11 10  6  9]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.1886
INFO voc_eval.py: 171: [16880 12935 13508 ...  5492  5433  5476]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0506
INFO voc_eval.py: 171: [470 268 472 274 366 364 501 314  73 310 502  83 273 363 157  71 477  97
 524 412 278 424 359 276  72 460 514 311  86  78  96 478  68 457 458 155
 164 168 469  50 131 462  58 486 400 367 358 245 247 313 272 535  22  91
 225 484 522 361 403  47 464 234  43 389 357 342 411 532  79  19  63  59
 487 269  54 510 243 365  10 387  90 163  74 527 414 416 251 167 492  98
 113 539 337  94 312 362 394  75 521  61 156 160 336 121 530 518 227 401
 407 419 360 161  82  85 493 473 315  77  89 398  44 134 264 318  67 481
 309 410 421 453 152  81 246  65 449 129 544  53  93 494 498  64  62 541
  92  88  87  46  60 489 438  76 265  69  70  84 290 480 186 543 228 344
 420 537 459 509  26 137 138 125 162  95 474 252 450  18 341  80  66 446
 347 427 468 249 238 109 348 423 317 429 230  25 499 128 404 124 447 122
 115 497 266 114 533 283 296 267 174 467 461 233 231 534 442 391 257 289
 142 293 277 338 476 193  36 390 235  45 250 108 253 491 285 350 434 418
  16 466 126 517 455 520 140 345 165 203 525 454 120 210  52 490 171 151
 383 300 339 386 232 433 170   7 431 195 482 375 260 507 316 500 147 154
   2 332 191 456 356 378 415 425 465 105  37 330 286 248  40 531 287   0
 214 127 256 153 159 166 226 216 149 306  48  34 302 117 130 158 222 239
 304 118 380 185 206  51 303 512 540 343 116 270 150 241 536 237 197   6
 292 428 242 271 196 145 188 546  39 301  13 211 217 288 132 136 405 495
  49 240 334 426 352 280 545  55 441 180 224 538 381 281 308  35 526 393
 547 282 139 496 384 385 183 141 354  28 119 100 408 406 133 219 435  41
 187 417 112 349 368  38 335 184 483 107 182 229 452 236 106 177 110 146
 175 392 178 440 388 346 101 111 294 218  21 513 221 463   4 215 382  27
 451 307 143 319 172 479  42 179 202 297 262 374 305   9 144 372 376 199
   5 542   3 169 223 148  99 377 194  17 471 340 327 444   1 369 355 192
 443 103  24 259 529 373 409 445 213 123 102 504 207 205 298 200  56 351
 371 295 519 135 328 263 448 523 379  23 508 258 353 528 370 204 209 275
 189 104 430 397 208 516 505 325  31 284  30 176  57 173 181 190 322 506
 255  32 244 439 299  33 323 432 279 396 254 413   8 261 291 422 201 503
  12 198  14 402 220 437 436  11 212 333 399 395 485 515  15  20  29 321
 324 475 511 331 488 326 320 329]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0938
INFO voc_eval.py: 171: [ 62  56  47  73 100  67  53  57  78  88  75  71  55  82 119  13  66 135
  72  54  58  16 182 140  98  31 146 113 188  50 125  86 138  48 143  32
 116 130 134 122 103  77 171 127  28  93  17 109 108  26  33 173  65  49
 181  36  94  51 170 136  76 175 104  30 183  83 158 185  19  27  59 105
  69  91  74  25  52  92  60  37  80  35   2 145  95 155  90  14 123 179
 154  12 107   9 151  70  97 153  64 132 137  22 117 128 101 161 141  34
  29 111 144 178  24 129  44  38 169 162 167 186 159 187 131 124 172 160
 165 102 133   0 110  21 112 126  84  68 156  89  96  79 168   6 121 139
 174 166  45  87  11 115  63  81  41 114  42 149  99 148  85 120  61 150
   3 147  15 142  23  10 118   7   1 106  18  46 180  39  43 176 177  40
 152 164 157 163   4   5 184  20   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0308
INFO voc_eval.py: 171: [ 97  99  92  30  67  33  79  72  81  94   0   7  42 153  40  15  82   1
  29  87 131 133  61  35  39  78  95   5 101 128  10  12  70 113   9 127
  88   4 129  54 125  96  86  14  55 112 134  77  24  91 137  73  58   8
  47 119 130  85  64  16 135  37  27 142  13  51  26 144  52  49 114 120
  50 147  31  38 141 148 152 146  23 124  18 154  63 138  46 123 121 139
 100  71   2 143  11   3  98  34  53 126  43 122  76  20  80 149  74  25
 151  93 115  84  90 103  89 132 136  17  62  19  36 150  68   6 145  66
 102  60  65  83 117 118  41 116  59 140  69  75  32  21  28  57  56 105
 111 104  48 107 108 109 110  22  44 106  45]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0277
INFO voc_eval.py: 171: [ 30 308 151 153  91 164 117 305 184 304 226  94 196  95 191 306  99 288
 103 107 307 299 109 257 246 231 185 236 172 165 163 121 162 160 159 155
 239 230 243 161 312  96  22 323  23  28 327 314 315 215 303 167 245  71
  90  11  14  32 158 174 250 233 190 316 252 285 319 247 255  48 271  89
 254 284  97 251  27 113 293 291 122 302 309 189  47 313 237 335 188 235
 154 200 116 195 127  24  98 294 207 206 321 244  10  12  87 170 322 125
 223 157 266  34  75  49 112 197  18 324 216 181 232 227 234 192  53 212
 300 229  16 156  92  78  55 131  35 104 199  67 298 277 177 129 290 168
 249 248 276 329  38  44  21  88 132 328 338 214  93 179 260 208 301  66
 224  59 198 241  26 261  85   8  73 222  37  56 183 126  46 120 310 238
 123  58 263 193  62 286 217 152  61 182  70  52  33 339 262 337  13  83
 297 334  86  64  43 242 213 211 187 221  40  31 220 205 279  65 287 108
   4 111 114  68   0 118 280  20  29 272 274 110 201 176 228 124  69   7
 128 320 273 134  42 330 119 278 194  39 289 267 100 270  17 218 210 209
 178  54 282 169 336  19  50 101 171 133 275  80 258 148 204   2 186 105
 219 311  15 175   1   3   9 141 150 137 332 203  63 106 136 318  51 326
 333 295 317 173 256 202  41 325 240 138 264  72 259 283 331 102  74  60
 139 180 166  57 149 265  76 143 225 296 292  84  25 281  36 253 146 115
  45 268 147 269 142   5 144   6 140  81 145  79 130 135  82  77]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0671
INFO voc_eval.py: 171: [186 280 282 138 161 245 243 128 238 126 157 177 122 234 103 266 121 120
 227 114   7 111   2 181 263 262 182 259 229  84 231  82 223 171 257 218
 279 104 359   8 119 127   4 132 341  23 137 361 148 285 136 340  75  74
 113  68 108 150 102 170 169 145  85 160 178 184 117 258 331  16  26 250
 244 360 237 193 236 305 306 213 212  20  19 124 301 140 319 329 135 337
 195 255 253 248  77 232 217   5 367  70 185 278 125 350 351 364 297 143
 289 153 277 241 216 205 302 226 368 115  81  66  33 100  14  91 304 283
 332 276 268  88 264  52 221 211 204 194 362  21   0 116 309  98 123 209
  65 188  67  72  76 174 347 246 159 303 284 296 155 156 154 175  18  97
  38 215 342 147 235 172 358 141 107  71  99 265  25 333  50 339 320 109
 134 249  63 105 191 365 299 254  15  11 176  13 149 208  94 106 323 357
 163 219 317  35 152  51 273 260 322 292  34  31 187  73 352 356  90 133
 131 349 142 222 288 272  54 139  86 328  10 129  56 207 199  48 252   9
 346 300  24 173 190 335 220 298 327  83  95   1 180 308  62  22 325 197
 353  78 330 242 192 318  12 290 355 168 338 130 110  93   3 239 189 198
  17 200 144 274 261 314 354 201 343 366  46  59 225 312  28  96 275 270
   6 310 307 183  37  36 206 321 251 101 271  58  32 224 295  57  69 118
  87  49 203 286 151  45 363 112  89 233  43 316 348  80 326 256 334  27
 287 313  92 267 315  64 179  53 214 336  47 162 311 293  44 281 158  55
  29 344 345 146 196  60 291 247 269  61  41  40 210  30  39 294 202 166
 167  79 240 228 230 324  42 164 165]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0249
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0509
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.086
INFO cross_voc_dataset_evaluator.py: 134: 0.062
INFO cross_voc_dataset_evaluator.py: 134: 0.010
INFO cross_voc_dataset_evaluator.py: 134: 0.058
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.143
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.036
INFO cross_voc_dataset_evaluator.py: 134: 0.017
INFO cross_voc_dataset_evaluator.py: 134: 0.042
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.009
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.189
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.094
INFO cross_voc_dataset_evaluator.py: 134: 0.031
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 135: 0.051
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.842s + 0.004s (eta: 0:01:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.562s + 0.009s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.560s + 0.007s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.536s + 0.006s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.542s + 0.007s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.530s + 0.007s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.508s + 0.006s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.493s + 0.006s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.480s + 0.006s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.471s + 0.006s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.481s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.487s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.494s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.815s + 0.004s (eta: 0:01:41)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.527s + 0.007s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.515s + 0.006s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.507s + 0.007s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.528s + 0.007s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.527s + 0.007s (eta: 0:00:39)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.517s + 0.006s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.503s + 0.006s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.485s + 0.006s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.477s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.477s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.481s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.481s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.622s + 0.003s (eta: 0:01:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.513s + 0.007s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.535s + 0.009s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.537s + 0.008s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.517s + 0.008s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.508s + 0.008s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.491s + 0.007s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.480s + 0.007s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.473s + 0.006s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.470s + 0.006s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.473s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.476s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.475s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step3999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.802s + 0.003s (eta: 0:01:39)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.508s + 0.004s (eta: 0:00:58)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.488s + 0.004s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.481s + 0.004s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.484s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.479s + 0.004s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.473s + 0.004s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.457s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.452s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.445s + 0.004s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.441s + 0.004s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.444s + 0.004s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.449s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 75.558s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 25 125  45  27  31  23  42 100  48  44  98  41  40  81  26 126  21 107
  32  18   9  16  11  13  12  10  38   7  28  84 105  79 108  22  15  87
  85  36  47  99  46  92  29 121 103 102  24  69  53 115  37  96 122 117
  95 112  14  68  52  30  57  34  75  19 104 123 120 116  64  35  80 114
  66  20  50  91  62  97  43 111  54 124  56  72 118  77 101  33  73  17
  90  60   2  70 106  89  71  58 119  88  65  49  82  55  76  78   6  59
   4   3  86  83  74 127  51  67 110   8   1  61 109   0  39 113  94  63
  93   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1301
INFO voc_eval.py: 171: [161 148  54  55  22  19 140 143  69 147 102  70 152 162  23 131 136  56
 132 155  61  59  65 139 138  68  52  24  20  18  83  96  98   5  42 158
  58  71  17 156  63 137  57  82  84  67  62 115  64  97  75 118   3 119
 141 123  50 114 104 134  77 145 146 144 133 100  86 122  92 157  30 150
  21 151 160 154 113 105  93 135 159 149  32  47  46 103 153  11  74  72
 142 101  53 126  34   6   1  88 127 112 109  66  48  78  29   9  49  90
  81 108  51  73 129  38  16  99  41  36  33 121 125 124  40   4  31 116
  94  27 128  76  37 120  85 106  87  12  43  35 130   2   0  79  44   8
  25 110  80 111  60  91  95 107 117  15  26  39  10  45  13  14  28  89
   7]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0651
INFO voc_eval.py: 171: [426 876 776 ...  83 707 706]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0182
INFO voc_eval.py: 171: [447  83 547 371 544  89 392 623 446 125 454 393 127 506 534  81  82 448
 533 202 405 521 514 345 455 343 353 467 300 450 420  18 294 381 303 321
 131 516 268 304 624 625 606 258 379 123  26 299 422 133 419 554 154 242
 538 411 412 556 330  12 504 328  10 232 244 368 208 366 364 231 269 225
 283 453 464 335 509 110 310 484 592 358 524 607 403 388 339  31 301 542
 341 199 574 128 363 375 137 498 203 193 338 112 352 331 515 230 408 152
 241 249 337  14 329 262 194 389 399 400 406 415 326 274 324 461  29 113
 518 298 557  60 517 617 519 292  23  27 342 354 367 390   9 223 553 312
 548 612 535 237 227 344 267 545  48 205  44   4   6  95 486  69  11 360
 429 560 382 315 563 456 172 384 589 317 510 138 340 605 307 260  87 308
 414 159  52 413 235 539 469 264 620 576 373 351   5 618 228  28 370 457
 293 376 156 569 302 139 280  37  38 391  40 168 426 581 449 586 349  76
  56 204 129 327 233  35 559 160  53 282 107 470 263  85 198 541 387 190
  30 583 573 407 488 195   7  57 334 285 350  16  42  32 434  24  88 220
 116 224 451  51 572 130 579 256  39 209 402 214 314 511 163 626 423  17
 578 575 505 115 460 134 585 346 421 222 165 219 153 417 437 410 409 496
 404 386 374 356 600 425   0 238 119 567 432  77 546  86 135 361 610 582
  49  71 290 490 401 109 276  36  15 149 121 622 481  58 246 257 279 270
 175  46 497 281   2 122 101  21 332 596 178 151 431  13 124 587 613 584
 561  45 234 580 176  33 377 187 621 383 599 442 458 297 108  54 372 323
 171 478 166 565 380 552 305 475 189 196 243 320 608 333 141 278 427 619
 493 164 385  80 179 512  67 118 296 588 570 197 221 158 287 311 186 229
  79 443 357  62 144 191 378   1  59 291 252  65 103 459 555 136  20 513
 492 157  22 430 487  50 463 473 322 105 181 571 288 248  68 522 436 265
 595 439 507 564 503 255 126 111 286 173 452 240 239 215 495   8 309 396
  66 207 355 394 180  25 609  98 471 520 185 140  43 395 591 319 271  75
 540 602 491 543 480  99 424 188  84 148 590 397 362 601 416 537 295 182
 568 177 462 250 489 523 438 120 551  19 611 192 117 200 226 217 499 577
 216 445 348 174   3 247 616  94 365 549 213 614  63 161 325 604 218 476
 347 273 558 143  74 465 147 167 485 603  92 211 494  70 170 428 398 251
  34 212 615 597 477 210 500 114 259 306  55 479 474 104 102  61 562 254
 468  41 418 508 530 162 277 359 525 441 594 266 598 313 502  90 369 550
  73 527 284 106  72 593 440 316 155 501 206 146  47 336 318 201  91 236
  64 531 142 275 245 532 253 435  78 100 482 483 289 261 528 466 526 536
 472 566 433 272 444 169 529  96 183  93 145  97 184 132 150]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1030
INFO voc_eval.py: 171: [277 169 279 267 246 155 165 163 208 243 275  59  85 244 211 263 215 173
 166 240 245 213 232 259  84 109 264 270 254 202 229 224 281  40 217  43
 225 265 261 262  87  90  93 271 260  86 257 221 316 154 192 198 200 204
  46 272  52 252 255 113 212 251  65 248 158 239 268  10  53  51  49 280
 157 230 241 300 222 323 206 209 235  47 278 233  97   5 207  44  92  28
 234   3  45 247  73  11 116  57 141  82 291  74 152 317  89 216  27 125
 124 199 326  83 119  64 311 115 167 111 258  81 274 286 114 223 273  60
 303 132 193 236  41 197 214  91 266  76 205  38 162 319  66 159  67 218
 296 285 210 201 117 149 249  48 256 136 228 219  68 203 189 100 287  54
  55 134 250 269 121  42 118 289 242 103  18 238 156 325  96   1  16   2
  63 175 196 292 276 185 220 237  36 182 108 127 322 226 190 283 253 195
 137 147  72 112 227  19 123 231 178 144 284 106 135 282   9 131 164  58
  61 312 151  26 184  30 327  70 140  34 129  31  98  21  88  78  50 104
 122   7  95 128 110 306 139 298 305  33 314 170 310 191 160  13  12  99
  35  29 133 130 313 102 101 324   0 107 171 150 146 301  71   6  23  94
 168  14 161 294  56 290 288 297 143   8 177 153 321 148  75  69 105  22
  77 183  32 320 309 142 138 174 181 308  80 120  62  79  37 126  24  17
 172 145  25 318 293 299 295 188 187 304  15  20 186 315   4 302 194 307
 179 176 180  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0206
INFO voc_eval.py: 171: [65 75 43 61 67 46 47 56 26 66 55 45 69  5  6 76 58 38 25 44 28 37 52 71
 18 22 72 64 27 70 35 50 33  7  3 36 81 24 57 73 32  2 51 21 54 30 10 13
 79 74 49 62 68 29 14 63 59 77 48 41  0  8  9 60 34 80 40 39 19  4 31 12
 15 78 17 11 42 23 20 16  1 53]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2768
INFO voc_eval.py: 171: [ 328  923  331 ... 1229 1249 1233]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0230
INFO voc_eval.py: 171: [53 55 48 35 44 32 17 33 16 18 15 37  9 54  4 39 38 42 20 49 46 34 10 19
  7 40 47 12  1 50 41  0 29 30 23  8  3  2 45 26 14  6  5 13 52 22 51 36
 21 31 11 27 43 28 24 25]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [ 540 1156 1157 ... 1819 2609 1827]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0182
INFO voc_eval.py: 171: [27  8 20 15 35  7 25 17 38 37  3 14 16 22 26  0 69 49 61 48 43 56 29 33
  6 55 45 59 30 19  4  5 58 28  1 10 66 74 24 18 31 70 68 52 12 46 63 64
 53 60 57  9 23  2 42 21 44 75 11 13 67 62 65 41 73 72 76 34 47 77 71 51
 54 32 39 50 40 36]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0396
INFO voc_eval.py: 171: [645 512 548 254 628 253 546 545 544 543 542 252 247 538 682 537 256 257
 514 258 280 518 292 293 294 651 557 523 650 643 642 267 517 261 634 516
 654 231 565 519 454 571 443 580 603 229 593 453 527 576 589 177 664 260
 198 197 262 663 264 265 192 279 184 268 183 275 182 191 228 203 236 239
 524 583 223 525 520 244 245 246 199 220 250 251 216 215 213 211 209 207
 206 204 662 201 578 288 392 614 613 579 398 403 406 612 411 611 606 605
 601 600 581 441 598 585 588 590 591 592 594 391 595 464 582 456 597 447
 445 596 284 388 384 290 652 646 165 644 641 314 640 639 637 636 515 513
 631 511 629 509 617 618 375 506 620 364 615 363 361 621 508 353 346 626
 362 528 241 162 572  87 556 683  18 533  92 555 534 554 567 532  23 676
  31 105 678 570 536 569 680 541 540 539 100 531  90 568 530 561  62 670
  76 574  72 559 604 350 408 632 248 410 552 365 630 622 625  60 328 547
 112 627 356 507 352 563  81  34 307 616 648 706  85  39 396  75 564 562
 703 321  93 286 505  95 282 163  97  99 577 401 610  57 442 176 667 178
 179  19 448 575 174 212 208 185 187 455 188 463 602 194 210 224 219 225
 166 232 234  25 440 586 666 238 370 186 190  79 154 584 573 624 360  54
 710 173 712 175 510 713 306 320 711 486 180 181 587 529  59  69 305 429
 558 404 405 673 674 101 227 677 551 230 550 609 608  30 432 496 431 430
 380 277 649 218 732 196  83 730 566  89 729 285 214 727 281 489 623 722
 502 402 755 607 721 503  14 374  17  53 449 619 635 549  98 653 200 195
 193 553 669  77 255 638 233 560 647 526 633 675 160 217 167 393 107  86
 462 399 148 485 427 202 521 522 686  13 145  29  55 134 159 372 709 599
 161  45 189 390 535 344 237 345 351 718 433 665 409 118 497 684 428 451
 242 143 142  28 434 461 205  56  63  66  68 504  74 381 735 466 417 149
 297 278  35 459 259 147 720  20 465 672 460 748 457 671 349 728  52 371
  47 308 304  44  41 726 719 484 437 681 243 120 263  58 343  65 322 240
 743 738 661 385 151 501 389 499 295 400 274 659 660 698 383 493 378 152
  73 369 377  84 717 700  15 104  71 117 744 458 476 226 122 357  88 488
 368 419 150 283 140 707 336 126   7 340  12 708 373 446 326 679 144  26
 330 359 131 750 121 491 129 696 102 725 379 318 335 691 439 114 106 289
 338 452 668 164 287 415 423 483 273 498 477 386 222  24 221  27 693 692
 737 291 714 492 327 272 110  94 435 332 153  82 715 397  37 699 235 170
 689 490 119 757 348 133 132  43 716   9 436 127 688 172 123 747  40 754
 749 137 444 407 249   5 266 740 319 376 139 158  50 424 302  21  38  46
  49 323 324  16 269 128 450 124 394 412 103 500 315  22 438  96 311 704
 487 736 422 111 169 745 425 687  48 685 337 355 116  36 303 367 387 125
 382 276 299 298 694 138 341 690 325 658 741 753 495 395 742 746 366  10
  51 271 414  33 420 756 317  11  91 309   8   6 136 739 731 156 723 494
 115 358 478 469   2  78  80 472  67  61 316 301 733  32 313 113  70 135
 702  42 734   0 480 724 171 109 752 334 347 108 312 300  64 421 416 354
 333 130 474 339   4 751   1 695 141 329 413 342 475 168 470 310 657 331
 467 296 418 697 705 146 701   3 270 426 155 482 468 157 481 471 656 479
 473 655]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0059
INFO voc_eval.py: 171: [ 59  40  22  42  43  64  23  36 153  15 154  63 144  37 152  34 159  18
  41 150  28  26 161 104  16 146  54  19  25 148  44 109  61 111 145 105
  48  17 107  46 149  35  11 106 156  30  49  20  24 112  91  27 157  51
 130  21  53  33  87 110  55 108 158  13  57 102  45  29 114 138 160  12
 155  38 117  93  90  88  92 139  14  52  39 151  89  67 115 147   9  65
  56  31  58 103 127 137 129  83 136 135  84 132 134  81  86 118  75   8
 116  95 133 119  73  94 124 121 128 120  76  47  50 122 100  98  60  10
  70  32 101 125 123  62  69 140 143 113 142  78  66  97  96 141  82 131
 126  68   3  71   7  74  80  99   4  77  85  72  79   1   6   0   5   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0095
INFO voc_eval.py: 171: [ 43 108 109 216 110 111 115 218 160  52  51  33  50 215  48 138  40 154
 103  21  89 100 235  98  13   2  11  82  90  18  80   4   5  83 137 102
  81  53  91  93 105  49  44 139  87  86  16  26  27  14 155 158   7 146
 151 191 196 207 200 257   6  99   3  88 202 238  85 170  42 244 142  47
 213 243 253 129 134  25  24  23 222 227 117 225 156 144 188 140 185  34
  54 241  17  30 205  12 204  19 106 123 228   8 250 148  39 231 260 113
 211 219 157  41  38  59 136 182 183  76 186 104 246 143  84 212  95 193
 131 167 252 249  64 118 198 172  29  63 223 236 130 199 141 126  45 254
 189 125  37  92 248 114  58  55  71 220 233 159  62 153 168 132 232 224
  10  96 128 267 149 165 221  65 166 127  36 239  57 194 255 217 237 230
  60 177 229  79 247   0 195 226  22 135  46 174  28  31 203  73 234 192
 145   9 133 201 150 278  61 124  70 181 163  68 116 175  15 214 270 164
 173 184 209 206 262 266 240 279 179  72 119 264 152 187 245 190  97  20
 256 263 180 121  35 242 261 171 161   1  32 197 265  74  77 258 251  56
 107 101  94 269  78 120 208 259 147 162 178 169 210 112  75 268 176  66
  69 122 275 276 271 272 273  67 274 277]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0381
INFO voc_eval.py: 171: [73 48 81 76 38 21 95 46 44 67 19 40 34 55 75 90 52 24 39 41 36 92 72 61
 43 87  3 42  8 28 65 45 17  4 93 91 15 62 80 82 84 50 69 53 31 79 37 71
 25 86 49 51 70  1 63  0 20 16 27 57 88 60 85 22 68 23 30 94 14  5 64 47
  2 32 13 66 54 89 77  9 83 26 56 74 33 78 29 58 18  6 35 59 12 11  7 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.1881
INFO voc_eval.py: 171: [15585 29228 16735 ...  7846  5461  5456]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0854
INFO voc_eval.py: 171: [294 512 514 515 301 397 395 345 394 544  82 300 342 543 391 106 462  80
  95 565 343 398 399 519  87 306 304 169 449 559 177  81 346  96 511 105
 180 168 501 520 499 392 529 527 498  23 504  65 150 271 400 576  75 299
 347 567 393 424  66 375  54 435  52  99  21 247 390 273 176  88 104 439
 447 295 535 506  48 530 277 553  56  12 269 575 257  70 452 171 100 173
 396 164 454 422 344  83  91  76 457 121  84 348 581 429  68 370 534 572
 374 133 574 289 564  74 558 523 258 516  49 437  97 341 445  85 446 587
 434 127 545 371 490  93  72 272 322  98 101 582  89 494  69  78  67  86
 460  94 532 136 174  77 522 459 278  51 552  79  71 476 292 199 165 172
 250  28 528 517 153  90 103 585 317  19 102 500 579 492  73 145 175 251
 380 464  92 352 487 275  59 441 571 139 252  61 461 118 510 466  27 383
 541 538 539 126 134 502 509 312 135 187 488 141 256 291 327 480 254 293
 518 426 204 320 283 372 381 151 303 325  41 425 378 117 276 259 124  50
 216 508 377 147 227 562 496 471 456 418 333 421 373 495 279 590 149 184
 211 138 231 131 384 210 350  55 125  20 566 182 524 321 158 163 415 408
 349 206 469 540 255 577 286  45 366 453  39 364   7 411 351 470 197 389
 140 282 236 463 123 114  42 550   1 170 274 248 119 507 166 319   0 335
 497 130 376 178 160 262  15 237 213  53 128 224 260 263 337 142 156 555
 244 578 296 323 162 334  44 154   6 588 387 416 589 584 465 308 442 419
 536 368  35 332 268 309 238 586 265 580 503 451 379   2 146  40 232 297
 143 428 195 420 338 569 340  60 246 479 570 120 137 313 537 201 444 240
  30  46 129 472 109 455 157 190 525 417 382 369 427 266 219  22 401  43
 261 215 493 196 194 188  26 191 167 116 192 477 198 185 115 423 155 339
 573 239 491 235 324 243 218 148 110 505 521 556 353 328  10 161  47 583
 547   3 225 407 310 152 159 318 336 311 223 287  18 409 249 483 207 229
 253 245 214 485 410 412  25 183 481 405 546 513 360 448 205 209 298 107
 388 285   5 329 402 533 267 202 112  31 486 561 144 228 385 226 234 568
 406 284 551 326 290 302 111   8 363 386 203   4 230 489 549 450 404 212
 186 468  33 403 359 443  34 560 189 108 432  62  29 193 113 316  37 356
 122 200 526 413  36 264 270  63  57 132 331 478 357   9 222 431 280 307
 221 288 467 208 414 305 217  13 179 281 474 330  14 181 242  11  64 314
 473 220 438  58 315 484 241 430 233 436 458 367  16 557 482 475  32  17
  38 440  24 433 355 548 554 358 531 365 542 361 354 362 563]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0870
INFO voc_eval.py: 171: [ 65  59  50 118  76  69  60  56  92  83  74  84  85  58  13  68  77  57
 140  62  16 186  72 105 145 151 127 100  33  51  53  93 142 194 128 148
 130  34 134  79 138 103 114 176 117  25  67  81  32 178  54  28  52  36
  96 187 175  30 141  89 180  75  19 189  71 163  99 191 109  98  29  64
  63  55  27  82  23  39  97  38 150 129   3 101  37  10 185   9 160  70
 159 113  61  14 102  73  66 156 154  35 158  78 136 122 143 115 132  24
 146 111  21 167 184  88  31 149 124 133 174  40 192  46 108 172 135   1
 166 120 193 164  95 116 177 170 165  20 106  49 137  91 161 144  11 121
 126 173  86   7 179  47 119 171 104  43  44 131  94 125   4 139 153  41
  15 152 155  80 123  12 112 147  87 107  22  90   8 110   2  17 188  42
 190  48 182 181  45  26   5   6 169 157 162 168 183   0  18]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0380
INFO voc_eval.py: 171: [ 82 105 107  34  86 100 101  45 111  72  33  32  78 109  30 148   0 173
  16  42   1 146  94   7  74  11  79   5 104  36 143 144  65  64  15   6
  43 125  84   9  57 138  12  76 102 149   8  95  92  14 145 124 158 134
  73  61  17  54  91 163  68 152  51  50  39  25 170 164 141  87  31  41
  26  23  21  40 161  53 165   4 139 154  58 126 174  52 169 108  99 175
 135   2  46  20 117 155 137  13 157  18  96 106 168  35 110   3  90 140
  22  48  55  66  97  85 150  81  71  49 147  75  98 151 166 116 172 114
  38  89  63  19 162 167  70  69 131 136 127  83 133  77 128 130 118  88
  62 156 129 132 159 103 142 160  60  37 113 153  27  80  93 112  10  59
 120 171  24  44 123  28 119  67  29 121 115  56 122  47]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0211
INFO voc_eval.py: 171: [161  98 174  35 163  26 125 199 326 335 333 111 181 176 101 173 172 171
 170 169 164 102 103 353 128 113 107 165 201 203 332 309 334 277 266 263
 260 256 252 200 251 245 340 234 341 342 343 210 207 206 249 358 330  36
  31  28 319 168 307 184 308  77 366  11 129  97 177 315  52 275 202 352
 321 331 104 337 293 262  32 349  20 344 267 258  40 265 264 235 274 118
 271 121 339 105  96  16  13  59  51 351 214  82 224  27 255  94 246 119
 205 198 196 254 110 222 167 288 124  19  33 137 242 132 141 248 138 355
 345  75 112 208 215  60 327 231  73 253 166 188 298  50 136  99 297  39
 212 179  74  44 131  83  43 360 361  87  58 100 348  22 233 329 223 189
 373 243 328  95 282 211  55 143  25 127 325 240  65 257  45 359 283 311
  10 336 134  66  56 312  62  92  69 370 197 300 261 367  80 236  14 303
 285 232  67 162 284  90 195 227  37  38  15 144 310  53  49  63  71  72
 193 273 238 294 295  76 120 221 116  34 194  48  85 239  42  93 130 296
 122 117 247  23 217 362   2 301   9 350 292 187 108   6 369 226 126 313
 289  21 216 225 209 237 106 160   5 190 279 180  61   4  57 305 278 178
 250 338 220 114 346  17 268 135  18 364  70 192 146  12 157 204 152 287
 219  81 185 316 147  68 115 356 357  30 322 365 145 153 276 175 259 323
  54 183 371 304  24 218  47 306 109  64  29 291 244 363 317 286 281 290
 123 191 148 149 230 318 159 186 372 270 314 182  78 347 269 241 228 140
 158  79 320  41   3 368 272   1 154 324 213  86   8   7 354  46 229 155
 299 156 302   0 139 280 142  88  84 151 150 133  91  89]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0498
INFO voc_eval.py: 171: [196 265 296 299  91 253 248 171 135 243 131 282 110 126 129 120 190 200
   4 115 116 272   9 233 234 117 174 245 244 185 238 187  78  82 159 251
 145  85 111 386 112   6 383 366 364 123 127  19 275 136  22  25  92 295
 137 144 301 134 169  79  80 160  73 186 191 154 194 240 286 384   5   7
 361 355 326 321  18  20 306  26 293 271 109 254 263 198 205 226 257 255
 264 227 389 199 218 353 339 270 246 320 300 294 259 391 148 149 277 229
 197 392  14  84  74  99   2  68  54  88   0 217 206 140  33  69 375 373
 189 153 356  81 225 325 164  96 324 385 258 316 122  17 230 106 170 285
 289 304 315 370 329 323 195  67 124 236 341 165 223 130  72 357 363 167
  50 108 287 133  65 249  49 119 150 114  31 260 156  32 166 113 362  10
 381 279 142 105 307 345 155  12 292 318  15 100  97 337 262 220 175 310
 267 231 138 232  47 163 158 203 390  36  29  77 184 349 382 368  59 343
 221  21  57  11 372 201 193 235 380 261 209 376 351 274 352 338 188 139
  94 288  43 303 354 319 211 172  90 103 210   1 182 317 379 101  86 328
  16  13  24 344 308 239 141 322 359 256 237 350 377 118  46  60 311 273
 250 222 360 280   3 330 346 204 183  52 365  35 342 213 388 241 334 284
  48 247  34 290 208 327 291  93 378 202  40 332   8 192 336 331 107 371
 104 312 132 347  30  66  76 157  44 348 216  87 125  95  98  53 302 161
  62 335  58 387 121  56 313  39 305  23 358 309  42 367 152 168 207 102
  63 333  27 228 180 369 281  61 298  83 147  38 173 214  28 374 266 224
  89  75 143 252  51 278  71 128 297 151 340  37 146 269 162 215 242  70
 178 314  41 276 268 212 283  55 219  64  45 177 179 176 181]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0267
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0645
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.065
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.103
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.277
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.040
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.009
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.188
INFO cross_voc_dataset_evaluator.py: 134: 0.085
INFO cross_voc_dataset_evaluator.py: 134: 0.087
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.050
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 135: 0.064
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.607s + 0.004s (eta: 0:01:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.489s + 0.005s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.509s + 0.007s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.510s + 0.006s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.523s + 0.007s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.513s + 0.006s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.497s + 0.006s (eta: 0:00:32)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.485s + 0.006s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.472s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.475s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.480s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.484s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.488s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.725s + 0.004s (eta: 0:01:30)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.520s + 0.005s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.522s + 0.005s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.526s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.535s + 0.005s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.519s + 0.005s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.514s + 0.005s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.496s + 0.005s (eta: 0:00:27)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.485s + 0.004s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.492s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.495s + 0.004s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.499s + 0.005s (eta: 0:00:07)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.497s + 0.005s (eta: 0:00:02)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.875s + 0.017s (eta: 0:01:50)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.554s + 0.005s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.514s + 0.005s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.512s + 0.005s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.510s + 0.005s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.497s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.475s + 0.005s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.466s + 0.005s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.459s + 0.005s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.460s + 0.005s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.466s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.469s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.475s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.703s + 0.003s (eta: 0:01:27)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.500s + 0.004s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.511s + 0.005s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.507s + 0.005s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.513s + 0.005s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.505s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.487s + 0.005s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.474s + 0.005s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.468s + 0.005s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.466s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.465s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.470s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.474s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 75.751s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 25 125  27  99  31  46  44  23  42  48  81  98  21  41  26 126  39  16
  18  13  11   9 107  32  12  10   7  38  84  22  15 108 105  28  79  47
  85  87 100  45  36  92 121  29 102 103  24  69 115  53  96  37 122 112
  95 117  14  52  68  30  57  34  19  75 104 123 116  80 120  35  64  66
  50 114  20  91  62  97 111  43  54  56 124  72  77 118  33 101  73  17
   2  90  60  89 106  70  71  58 119  88  65  49  55  76  82  78   6  59
 127   4   3  83  74  86  51  67   1   8 110 109  61   0  40 113  94  63
  93   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1301
INFO voc_eval.py: 171: [163 150 145 142  23  19  55  56  24 138  57 154 149 164 157  70  71 133
  99 103 134  62  66  69 140 141  84  97  60  53  25  18  21  20   5  72
 160  59  17  43  68  85  83 158  58  64 139  98  65  76  63 124 116 119
   3 120  87 143 105 148 101  78 115 146  94 147 136  51 135 159 123 156
 114  95 162 106 152  31 153  22 161 137  33 151  48  47 104  73 155  75
 127  35 144  11   1 102  54   6 113 110  89 128  67  49  30  79   9  92
  82  50 109  52  74 131  39  16 100  42  37 122  34   4  41 125 117  32
 126  77  28  38 130 121  86 107  88  12  44 132  36   2   0  80   8  45
  26 129 111  81 112  61  93  96 108 118  90  15  27  40  10  46  13  14
  29  91   7]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0470
INFO voc_eval.py: 171: [ 229  830 1133 ...   83  714  713]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0506
INFO voc_eval.py: 171: [452  83 551 626  89 509 376 548 397  82 537 538 458  81 451 398 125 127
 453 459 524 347 301 202 410 517 348 629 455 519 471 507 333 382 385 123
 132 269 357 324 425  18 610 258 233 306 630 302  25 305  10 296 417 154
  11 245 542 391 512 367 369 489 371 243 427 416 560 331 558 424 313 208
 546 134 226 232 270 457 502 338 303 361 335 110 527 340 341 518  31 407
 284 342 611 231 468 596 250 413 128 113 138 152 263 411  14 578 465 329
 332 203 199 344 193 522 378 521  30 242 364 355 392 327 300 561 420 224
  23  63 275 621 386 403 194 112 294  26 520 404   8 345 356 204 370 393
 315  44 557 552   6 549 433 616 267  95 307 238 236 311 539  69 343 228
  48 568 318 346 490 363 319 384 260  12   4 388 159 609 139 264 418 229
  52 564 624   5 543 140 419 310 593 354 460 172 473 373 304  28 156 513
 295  87 281 395 585 129 622  56 375 563 573 379 580 461  40  35  38  29
  37 234 590  53 454  76 474 168 205 353 396 431 330 283 286 587  16 107
  24 262  32 160 577 412 191 337 545 198  58 209  85 352 130 221 317 225
 256   7 438 583 195 492  42  88 116 456 576 437 428 514 408 508 631 163
 115 215  39 135  51 153 223 464 579  17 426 582 390 165 349 604 220 588
 377 239 415 500 119 442 414  86 494 359 435 586 368 430 422 136  49 406
 409   0 571 277 121 550 321 614 292 109  77  71  15 149 482 257 282  36
 280 175 271  57  46  21 600 178 122 486 627 101 247 334 589   2 565 151
 617 603 235 380 591 387 124 108  13 447 569 383 171 481 556 625 374 176
 584 462  45 308 187 326  33 299 190 323 478 166  54 244 197 336  80 623
 389 612 141 279 164 574 497 592 196 515 289 118 222 287  67 179 314 298
  79  59 144 360   1 186 448 158 230 381  61 559 628 192 496 293 463 103
 516 434  22  64  50 253 137 491  20 180 467 157 477 325 575 444 290 599
 249 105  68 525 441 265 506 255 510 567 216 173 189 288 498 241 126 240
 111 312   9  66 613 399 358 207 182  27 475  98 268 523 185  43 322 595
 400  75 272 544 606 501 484 495 547  84 188 148  99 594 429 605 365 401
 421 541 297 493 572 183 443 466 615 555 120 526 177  19 227 117 200 218
 450 217 620 503 581 351 248 174 553   3 214 608 366  62 618  94 479 161
 328 219 350 394 440 274 143 212 469 562 167 147  74 607 488 432  70  92
 499 213 619 251 170 480  34 601 402 210  55 114 259 309 483 104  60 566
 102 472 405  41 534 278 511 528 423 162 362 446 602 598 316 211 266 530
  90 554 372 505 131  73  72 285 106 155 597 146 445 504 206 339 320  47
 201  91  65 535 237 536 276 142 439 246 254  78 485 252 100 531 487 261
 470 529 291 436 476 540 273 449 570 533 532 169  96 181  93 145  97 184
 133 150]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1026
INFO voc_eval.py: 171: [168 275 154 277 265 241 243 207 162 164 172 210 165  60 261 242 214 273
  85 257 232 109 238 212 244  84 253 263 267 228 222 201 280  43  40 259
 262 224 260 216  90  87  86 258 255 250  93 220 315 269  66 266 246 249
 113 153 191 268 197 199  53 203 211  46 237  49  10 157  52 279  54 234
 299 205 208  47 229   5 240 156 221 276 322 231  44  28 316   3 235 206
 245 233  92  72 290  45  82  97 141 116 325  11 215  58 151  27  74  89
 256  65 272 198  80 161  83 310 166 125 124 302 285 115 114 111 119 200
 196  62 271 213 317 223  81 295 132  41  38  91 192 264 204 252 209  67
 286 117 217 284 158 188 227 202 218  79 148 288  42  48 136  56  68 247
 100 254  55 270 103 118 120 239 155 248 134   1 324  18 174   2  16  64
 291  96 184 274 282  36 236 219 181 321 127 195 189 108 225 251 278  19
 146 194 137 112 230 226 283 177 123 135 144 106 281 131   9  59  63 311
 163  30 150 326 183  70  26  35 140  98  88 129  31 121  78  50  61   6
  21 104  95 128 110 305 297 139  33  75 304 309 314  76 169  13  12 190
 159 130 133  29  99 312  71  34   0 170 323 102 149 145 101 107  23 300
  94  14   7 167 293 296 287 289 160  57 143  51 152  73   8 176 147 319
 105  77  22 182 320 308  32 142 138 180 173  69 307 122  37  24 126 171
  17  25 318 292 298 294 187  15 186 303  20 313 185   4 301 193 306 178
  39 175 179]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0201
INFO voc_eval.py: 171: [68 64 78 44 47 70 58 48 57 69 27 46  5 73  4 39 60 29 45 75 38 53 24 76
 36 17 21 67 28 74 50 33 72  2  6 83 37 59 23 77 32  1 51 55 19  9 12 25
 81 52 65 71 13 30 61 66 79 49  0 42 26  8 62  7 63 35 40 41 82  3 18 31
 11 56 14 80 10 16 43 22 20 34 15 54]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2617
INFO voc_eval.py: 171: [ 920  319  508 ... 1227 1247 1231]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0189
INFO voc_eval.py: 171: [48 50 43 28 31 39 17 29 18 16 15 33  9 49 35  4 34 38 20 41 44 30 10 19
  7 36 42  1 12 45 23  0 37 25 22  8  3 26  2 40 14  6  5 21 13 47 46 32
 27 11 24]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [1331 1333 1335 ... 1388 2604 1822]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0169
INFO voc_eval.py: 171: [28  8 20 36 14  7 25 17 39 38  3 13 16 21 27  0 70 49 50 62 44 30 57 34
  6 56 60 31 46 19  5  4 59 29  1 10 67 76 22 32 71 12 18 69 53 64 47 65
 54 61  9 58 23  2 43 45 77 15 11 24 26 68 63 66 74 42 73 78 35 79 48 72
 52 55 33 40 51 41 75 37]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0434
INFO voc_eval.py: 171: [574 299 602 522 298 692 234 235 655 579 612 584 285 252 387 589 258 464
 463 259 260 262 453 263 264 598 273 267 661 660 526 523 528 638 566 644
 557 555 554 553 552 551 550 527 547 546 664 652 529 300 533 653 536 639
 641 320 654 297 647 651 295 274 646 281 284 650 649 313 289 656 271 253
 268 188 195 196 200 201 202 204 206 207 209 210 211 213 215 217 218 223
 266 257 256 662 636 251 270 250 245 241 240 233 227 224 247 632 370 630
 518 516 576 577 578 580 582 586 587 588 590 591 592 474 466 519 521 524
 525 570 568 565 564 563 549 548 594 545 542 541 540 538 535 534 530 543
 457 455 451 615 616 372 371 187 369 621 394 364 623 361 624 355 625 627
 628 622 631 398 402 597 599 600 601 603 604 605 401 606 608 421 610 416
 413 408 613 607 186 384  92 672 673  19 674  33  95 169  74 680  23 693
  65  78 181 102 166 106 107 688 690 686  77 658 411 717 287 420  71 442
 450  63 452 327 677 458 254 418 611  83 286 326 635 640 335 643 626 114
  99 358 360 311  98  96 374 619  87 293 614  59 713 573 406 465 595 473
 517  36 222 221  26 214 212 170  20 556 178 561 197 180 571 192 191 572
 189 182 183 228 229 167 676 244 585 583 158 515 238 637 236 741 743 720
 596 559 560 506 414 415  81 103  62 575 633 723 722 721 459 593  94 567
 368 620 496 439  32 440  56 738  85 441 520  86 581 390 539  89 185 379
 618 732 740  15 512 184 683 242 312 177 216 179 220 291 687 199 194 190
 659 684 412 243 237 663 261 499 537 383 617 609 219 531 679 685  31  18
 164 733 657 232 513  79 151 171 642 205 198 645 558 101  55 629 634 569
 562 203 766 648 419 427 400 109 730  57 163 193 731 137 437 409 472 495
  76 675 696 443 165  14 544  47 120 149 146 727 359 745 382 461 403 354
 353 391 532  58  70 494 152 265 145 249 153 469 471 444  66 438  68 248
 694  30  22 737  37 476 314 208 283 303  90 507 514  88 246 104  49 467
 357  54 749 470 708 759 475 155 310 729 681 509  67 511 269 682 691 380
 122 739  46  43 447 669 754 755 671 670 328 378  75 280 410 710 399 395
  61 393 301 468 351 119 503  16 388 365 386 231 746 377  73 156 429 486
 728  27  93 343  91 128 124 498 337  28 381 154  60 333 718 456 143   8
 134 719 689 288  13 347 123 342 148 132 367 736 449 501 761 706 701 324
 115 678 389 108 425 294 433 168 487 462 345 493 230 226 296 508 225 748
 396 703  29 279 724 334  25 702 112 502 339 278  97 157 709 725 699 768
 500 239 174 121  84 445  39 407 352 135 136 176  41  10 726 255 129 760
  45 125 765 698 758   6 454 446 140 292 750 272  52 417 434  40 385 142
 308  51 325 329 331  48 126 162 404  21  17 130 275 460 422 321 510 448
 105 100  24 714 317 497 747 113 432 757  50 173 344 435 697 695 118 363
 309 397 376  38 127 392 282 348 141 764 305 332 304 131 704 752 668 700
 424 505 373 753  53 756 405  11  35 767 277 430 323   7  12 290 315 751
 139   9 742 117 504 734   2 160 366 488 479 482  82  69 744  34 322  80
 116 319 307 138  64  44  72 712 490 735   0 175 763 111 431 110 426 341
 318 356 350 306 340 362 484 375  42   4 133 346   1 762 144 423 705 336
 349 172 316 485 480 667 338 428 477 330 302 707 716 150 711   3 436 276
   5 159 492 147 715 478 161 491 481 666 489 483 665]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0063
INFO voc_eval.py: 171: [ 62  23  43  44  60  37  24 156  65  42 154 155  38 146  16  19  41 161
  35  27 152 105 114  29 163 148  26  63  20  17  45 150 112  55 147 106
 113 110  49  18  47 108 151  36  11 107 158  92  31  51  21  25  28 159
  52 111  34  22 132  54 109  89  56 160 103  13  58  46  88  30  12 140
 162 157 116  39  53 119  90  14  94  93 141  91  40 153  57  32  68 149
   9  66  59 117 131 104 129 136 139  85 138 137  86  84 134 120  77   8
  96 118 123  75 121 135  95 126  50  48 122 130  78  64  15 101 124  99
 127  61  10  71 145  33 125  70 144 142 115 102  83  80  67  98  69  97
 133 143 128   3  73   7  82  72  76 100   4  79  87  74  81   1   6   0
   5   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0112
INFO voc_eval.py: 171: [107 108  41 160  31 154 215  36 217 218  80  81  47  49  50  51 138 109
 110  88  26 102 114 235  97  13  16   4   6  19  99   2  79  89 137   5
  52 139   7   8  48  90  92 104  23  42 101 146  14 151 170 155  82  84
 158  24  86  98 238 243 244  87 222  85   3 191 196 200 202 257 207  20
  21  22 213 253 142  46 134 129 227  40 140 224 156 185 116 144 188  54
  29  32 241  37 250 205  17 204  12 123 148 112 228 260 105  44 231   9
 157  25  83 136  76 186 171  39  58  38 211  63 117 182 219 183 103 249
  62 131 118 193 143 212 246 252 198 167  94 199 223  28 130  43 126 236
 194 141 125 254 189  35 220 233  91  70  53 113  61 248  56 159  34 153
 168  11 232 132 128  95 225 165 149 221 268 166  64 127  57 216 255 230
 239 237  59 177 229  78 247 226   0 195 135 174  45 203 234  27  72 192
 133 201 145  10  60 279 124 150 163  69  67 115 175 181  15 271 214 164
 173 184 209 206 262 280 267 240 179  71 152 119 190 187 245 264  96 256
  18 180 263 121 242  33 172   1 261 161 197  30 266  73  74 258 251  93
  55 106 100 208 270 120  77 259 162 147 169 178 111 210 265  75 176  65
 269 122  68 276 277 272 273 274  66 275 278]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0260
INFO voc_eval.py: 171: [74 49 82 77 22 38 47 68 45 19 56 35 40 76 53 91 87 25 39 42 93 37 44  3
 62 88 73 43 29  8 46 66 17 94  4 92 15 63 81 51 83 85 70 54 52 32 26 50
 71 72 80  1  0 64 16 28 89 21 58 61 41 86 23 69 24 31 95  5 14 48 65  2
 33 13 55 67 78 90  9 84 57 27 75 20 34 79 30 59 18  6 36 60 12 11  7 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.1879
INFO voc_eval.py: 171: [12832  2159  7580 ...  5460  5455  5451]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0478
INFO voc_eval.py: 171: [293 512 300 515 397 514 345 395 544 542 342 394  82 299 391 398 519  80
 462 105 563  94 343  87 167 178  95 558 399 303 449 305 104 499 166 511
  75 501  81 520 175 530 392 346  23 148 498  65  52 298 400 347 575 270
 527 504 566 574  66 506 246  21 439  98  54 435 425 375 390 393 272  88
 256 268 174 103 344 169 447 276 454 294  56  48 529 535 554  70  11 162
 422 171 396 370  68  99 452  91  76  83 348 374  96 534 119 429 523  84
 571 573 580 457 516  85 257 557  49 437 446 341 133 564 445  74 288  72
 127 434  97 371 490 545 271  93 586  89 581 321  78  92 494  86  67 100
  69 460  71  79 522  90 459  51  77 172 277 135 249 532 476  27 552 517
 198 528 170 584 291 143 102 151 184 500 163  73 173 101 464 492 250 578
 316  19 487 274 380  59  61 461 441 251 383 570 466 510 134 538 138 116
  28 120 509 541 539 125 311 140 254 186 255 502 480 326 488 203 426 372
 290 518 292 381 378 282 149 319  41 302 215  50 424 275 324 258 377 123
 145 508 496 373 418 421 456 562 471 589 137 131 226 333 180 182 147 124
 278 230 210 524  20 495 205 540 209 320 469 350 565 156  55 384 408 349
 470 253 576 415  45 351 161 366 453 363 285 329 168  42   7 113 411  39
 139   1 550 126 463 122 236 117 273 196 389 281 376 336 176 164 318   0
 130 507 334 247 212 497 158 223  15 261 259 154 128  53 243 262 338 555
 335 587 577 160 322 295 152 387   6  44 416 588 307 536 419 465 264 442
 579 583 503 237 308 368 379  35 267 585 332 296   2 238  40 231 420 144
 451 142 568 245 428 339 194 235  60 479 569 136 312 472  46 537  30 118
 444 200 129 239 108 155 382 417 525 455 427 369 189 260 265 187  26  22
 193 218 195 493 214  43 190 401 197 165 115 191 477 423 114 572 153 183
 340 521 323 109 217 242 146 491 352 505  47 159 556  10 546 327   3 407
 224 582 309 157 317 337 150 310  18 286 222 483 248 228 412 409 206 485
 252  25 244 410 213 181 547 405 297 481 106 448 207 513 204 533 359 388
 284   5 561 328 402 266 201  31 227 111 486 234 225 141 385 567 551 283
 406   8 386 325 289 301 110 362 202 489 211 404 549 229   4 450  33 443
 403 185 358 468  34 560 107 188  29 432  62 112 192 355 315 121  37 413
 526 199  36 263  63 269  57 132 478 331 356   9 221 431 279 220 287 306
 467 208 414 304  13 216 474 177 280 330 179  14 241  12  64 219 473 313
 438  58 484 314 240 430 232 436 458 559 367  16 233 482 475  32  17  38
 440  24 433 354 548 553 357 531 543 364 353 360 361 365]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0878
INFO voc_eval.py: 171: [ 64  56  72 117  77  69  60  57  93  84  76  86  85  59  12  78  67  58
 142  17  52 187 106 124  73 147 127 153 100  34  53  51  94 195 144 128
 150 131  35 136  81 140 178  26 116 113 103  66  82 180  33  54 188  37
  29  98  31 177 143  20  90 190 182  75  71 165  99  55 109  62  30 192
  63  83  24  28  40  97  11 152  39 129   3 101 186  10  38 162  70 161
 112  15  61 102  65  74 158  79 156  36 160 138  25 145 122 114 134 110
 169  22 148  89 185  32 151 135 123   1  41 130 176  47 193 108 174 194
 168 137 119 166  96 115 179 172  21 167 105  50 139  92 146  13 163 120
 126  48 175  87 118 181   7 173 104  44  45  68  95 133   4 125  16  42
 141 155 132 154 157  80  14 121 111 107 149  88  23  91   8   2  18 189
  43 191  49   5 184  27  46 183   6 171 159 164 170   0   9  19]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0378
INFO voc_eval.py: 171: [ 96  78 100 108  82  95  68 105  34  33  32   0  74  30  16  44   1   7
 141 143  11  58  41  90 169  99  75  61  62  70 103  36 138 139  12   8
  42  79   6  10  15 133   9  72  97  91 120  14  88 144 140 152  69 129
  57 119  17 158  87  64 147  52  38  26  49  48 160 166  31  83  22  39
 136  40  27   4 156  24 134  51 159  54 149 122 170 102  50 165 161 171
   2  13  21  45 130  92 101 132 164  18 153 151 112   3  35  23  86 104
 137  93  63  46 146  53  81  67  47  77 142  19  71  94   5 162 148 111
  37 109 168 163  85  65  20  66  60 126 121 131 128 157 123  80  98 113
  73 127 154 125  59  56 135 155  84 150 145 124  28 107  89  55 106  76
 115 118 167  25  29 114  43 116 110 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0226
INFO voc_eval.py: 171: [331  35  26  97 159 124 198 172 161 333 325 174 168 255 102 101 251 250
 100 248 351 169 244 233 170 209 206 205 167 202 171 200 199 162 163 180
 330 262 259 356 328 332 308 112 110 337 128 338 106  28 276  36 265 339
 340  31 329 103 364 320 318 314 306 175 307  76 347 183  11 261  51 350
  32 129 341  96 266 335 166 274  20 257 292 201 104  95 349 120  58 254
  13 336  16 273 271  27 264 263  40  81 116 234 223 213  50 253 138 245
 241 118 136 221 287 204 123 197 195 165  33 109  93  18 343  59 111 214
 247 230 353 132  74 140 342 187 296 177  39  98 164 207 135 211  72 252
  73  49  44  82 131 297  22  86 359 346 358  57  43 232  99 326 371 189
 210  94 281 222 327 242 239  54 127 142  64  25 256 324 179  65 310  10
 282 357  91  68  55 311  45  61 334 260 133 368 196 231 235 365 299 284
  14 302  79  89 226 143  66  15 160 283 194 309  38  70  71 192  37  63
  48  52 125 272 294 193 220 119 237  92  75 293  47 238 295  34  23 115
 121  84 117 130  42 246   2 216 300 360 348   9 186   6 291 225 312 126
 107 367 288  21 224 215 188 236 105 208   5 158 278  60 178   4 277  56
 304 219 249 113 267 176 344  17  19 134  69 156  12 191 362 145 184 203
 218 150  67 286  80 315 114 355 354 321 144 146 151  30 363 258 275 322
 182 173  53 369 217  24 303  62 305 108  46  29 280 290 361 316 285 243
 190 289 147 122 317 148 229 157 185 370 269 313 181 345  77 268 227 155
 240 139  78 319 270   3 153 323  41   1 366   8   7 352 212  85 228 152
 298 154 279 301   0 137 141  87  83 149  90  88]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0548
INFO voc_eval.py: 171: [284 197  91 255 297 300 250 245 112 137 236 274 133 131 128 234 268 121
 119 364 176   9 118 191 173 117   4 203  86 161 188  83  79 208 247 246
 186 257 252 240 114 125 129   6  20  22 138 302  25 365 382 145 296  93
 146 385 113 277 139  74 187 151  80 156  81 172 162 148 136 192 195  69
 383   5   7 359 354 325 321  18  19 320 307  26 294 272 287 111 265 199
 261 206 259 256 242 267 229 228 388 198 200 220 352 271 248 301 295 390
 279 150 338 231 391  70  54   2 100  75  89  14  85   0 141 324 323  17
 155 166  32  82 260 190 232 227 219 316 355 207 374  97 124 384 372 237
 304 167 126 315 171 108 369 290 196 322 225 132 328  68 340 286  36 110
  73 169  51 288 356 362 135 262 251 361  50 152 122 380  66  10 116 158
 115 168 107 143  12 344 308 269 281 293 157 318 101  98 204 223 209 233
 311 177  15 336 264  34 235 389 276  31  48 160 164  60 381  78  29 371
 367  58 185 342  11 348  21 201 379 263 194 238 212 350 375 351 289  44
  95 337 189 175 140 224 105  92 353 305 214 319 273 213 183   1 317  13
  37 102 378  87  16 343  24 309 358 120 327 241 142 349 239  47 376 258
  61 275 312 253 329 360 184 363 345 282 205   3  53 341 243 285 216 333
 387  49 249  94  33 292 211  41 326 291 330 377 331   8 109 266 193 335
 134 313 106 222 370  30 346  77  67 159  45 347 218  96 127  55  88  99
 163 303  59 334  62 386 202 123  57  40 357 310  23 306 366  43  35 154
 170  64 210 332 103  27 230 104 181 283 368  63 299  84  39 149 174  28
 373  90 217 226  52 280  76 130  72 254 144 153 339 298  38 147 165 244
 179  71  42 314 278 270  56 215  65  46 221 180 178 182]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0260
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0622
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.047
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.103
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.262
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.017
INFO cross_voc_dataset_evaluator.py: 134: 0.043
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.011
INFO cross_voc_dataset_evaluator.py: 134: 0.026
INFO cross_voc_dataset_evaluator.py: 134: 0.188
INFO cross_voc_dataset_evaluator.py: 134: 0.048
INFO cross_voc_dataset_evaluator.py: 134: 0.088
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.026
INFO cross_voc_dataset_evaluator.py: 135: 0.062
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.694s + 0.006s (eta: 0:01:26)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.480s + 0.004s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.492s + 0.005s (eta: 0:00:51)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.516s + 0.005s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.520s + 0.005s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.495s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.481s + 0.005s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.474s + 0.005s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.472s + 0.005s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.472s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.474s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.480s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.484s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.643s + 0.004s (eta: 0:01:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.525s + 0.007s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.549s + 0.007s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.526s + 0.007s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.518s + 0.007s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.496s + 0.006s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.487s + 0.006s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.474s + 0.006s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.473s + 0.006s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.482s + 0.006s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.486s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.490s + 0.006s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.488s + 0.006s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.674s + 0.027s (eta: 0:01:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.542s + 0.008s (eta: 0:01:02)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.535s + 0.008s (eta: 0:00:56)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.550s + 0.007s (eta: 0:00:52)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.532s + 0.006s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.504s + 0.006s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.490s + 0.005s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.485s + 0.005s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.480s + 0.005s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.479s + 0.005s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.488s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.486s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.489s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step4999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.611s + 0.003s (eta: 0:01:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.514s + 0.005s (eta: 0:00:59)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.526s + 0.005s (eta: 0:00:55)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.523s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.506s + 0.005s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.481s + 0.005s (eta: 0:00:35)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.475s + 0.005s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.461s + 0.005s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.452s + 0.005s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.460s + 0.005s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.457s + 0.005s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.461s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.464s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 75.246s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [ 25 125 107  23  27  31  45  42  48  46  21  39  98  26 126  81  18  41
  16  32 106   9  11  13 108  10  38   7  12  22  15 104  79  84  28  47
  85  87  44  99  36  29  92 101 102 121  24 115  69  53  37 122  96  95
 112 117  14  68  52  30  34  57  19  75 123 103 116 120  35  80  50  66
  64 114 111  91  62  20  97  43  54  72  56 124 118  77  73 100  33   2
  17  90  89  60 105  71  70  58  88 119  49  65  55  82  76  78 127   6
  59   4   3  83  74  86   1  67  51   8 110   0 109  61 113  40  94  63
  93   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.1320
INFO voc_eval.py: 171: [163 145 150  56  57  20 142  24  72 154  71 138 157 164 149 100 104  58
 133 134  25  67  61  70  54  63  22  98 140 141  26  85  44  19  21  18
 160  60  59   5  73  65  86 117  84  69 139 158  64  99  77 143  66 120
 125   3 106 121  88 116  95 147 135 148  79 136 102 146  52 159 124 153
 152  23  32 156 107  96 115 162 137 161  34 151  49  48  76 105  74 155
  12 128 144 103  36   1 129   6  55  68  90 114 111  31  50  80   9  51
  94  83 110  75  53 131  40  17  43 101  38 123  35   4 127 126  33  42
 118  78 130  29  39 122  87 108  89  45  13  37 132   2   0  81   8  46
  27  82 112 113  62  93  97 109 119  91  16  28  10  41  47  14  15  30
  92   7  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0605
INFO voc_eval.py: 171: [ 157  871 1127 ...  589  715  714]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0089
INFO voc_eval.py: 171: [456 556 380  84  83 553 514  89 401 402 632 457 124 543 126 542  82 462
 455 304 635 351 201 352 463 521 616 299 529 414 459 309 308 385 389 371
 523 512  18 263 327 475 258  10 122 337 429 361 636  25 131 230 305 547
 315 153 431 428 563 565 551 421 420 133 273 306 335  11 249 107 493 246
 395 347 231 225 517 372 375 207 461 522 137 506 472 151 262 336 339 342
 617 344 345 601 532 229 365  31 382 417 287 411 368 331 333 415 408 359
 192 127 243 348 198 202 390 396 266 469  14 527  30 583 526 111 110  63
 360 223  23 233 566 407 193 278  26 423 297 303 525 627   9  95 323  44
 397 203 374 562 557 317  70   6 494 554 437 349 622  12 464 573 367 569
 310 346 392  48 138 388  52 615 271 171 321 598 227 158 544 350 548 320
 239   4 377 379 267 358 628   5  28 630 307 139 422 155 313 298  87 518
 424 383 477  56 356 465  37 568 585 334 268 128 204 399 590 578 284 244
 595 435  86 458  29  77 550 232 478 286  40  35  38 167 400 593 197 159
  53 341 208  16   7 265 289 496  24  42 256 442 582  58 416  32 220 190
  88 105 194 114 260 460 355 129 589  39 214 162 224 579 319 637  51 584
 113 468 134 412 586 441 513 432 236 353 592  17 219 222 152 430 610 446
 419 394 381 363 504 434   0  90 234 240 164 117 576 555  78 498 439  49
 426 418 413 410 135 591 295 373 620 116 324  73  36 524 119 148  15 280
 248 633 174 486 490 283 285 274 261 100  57 177 606   2 121  46 150  21
 623 596 594 123 338 570 451 609  13 631  33 588  45 175 384 391 106 466
 235 302 330 311  54 574 187 485 387 561 170 482 165 378 196 282 245 189
 618 326  81 340 120 292 501 140 629 163 519 393 178  68 118 195 597 580
 221 602 452  80 316 290 157 301 364 634  62 500 386  59 467 143 184 228
   1 101  64 564 296 191 254 438 471 180  20 328 136  22 495 481 520  50
 156 581 104 447 445 251 293 605  69 511 269 530 291 516 257 502 572 215
 108 172 188 241 329 314   8 125 242  67 619 238 182 403 206 362 479  27
 528  97  43 600 404 272 185 612 488 325 505 275  76  85 549 552 499 186
 599  98 433 611 147 369 405 546 425 448 497 300 470 181 621 560 531 577
  19 115 176 217 259 454 226 199 626 587 216 507 250 357 213 173 614 332
   3 558 218 624 483 370 160 354  61  94 398 211 277 444 473 567 146 613
 142 166  75 492 212  71 436  92 503 625 109 252 508 484 169  55 209 264
  34 406 312 487 607 112 103  60 476 571 102  41 409 281 539 515 161 533
 450 366 427 210 608 604 318 535 376 559 270 510  74  72 288 603 130 154
 449 509 145 343 205 322  47 200  91 540  66 237 541 279 443 141 247 255
 489  79 536 253 491  99 474 534 294 440 545 480 453 276 575  65 537 538
 168 179  93 144  96 183 132 149]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1035
INFO voc_eval.py: 171: [274 167 152 276 206 242 161 163 237 264  85 211  60 208 241 260 213 272
 171 164  84 243 229 249 256 108 266 222 228 262 279 240 252 200 261 215
 224 258  41 259  44 221 268 314  86  87  90  93 248 257 255 196 245 198
 202 265 267 253  66 156 112 153 190 210  47  53 275 238 235 278  50  52
  10  54 220 207  48 155 232 204 172   5 298 320 244 289  11  45  46  92
 205 231 233  72 230 315  82  28   3  97 151  89 324 214 165  74  27  58
 138 115 122 301 123 119 284 197 254 113 110 309 160  91  83  80 271  65
 114 316 195 294 223 191 199 209  42 212 263 270  62  81 203  38 251 216
 116  79 157 227 285  67 187 283  56  49  68 133 100  55 287  43 246 217
 201 117 147 247 131 323 269 236 103 154  18   1 239  64  35   2 180  16
 290 173  96 273 194 219 281 183 234 319 125 218 225  19 107 188 118 250
 111 134 144 226 277 193 121 176 105 280 282 132 141 129   9  59 310  63
 162 150  30 325 182  70  26  36  31 127  98 137  88  61   6  95  78  51
  21 104 109 304 126 296 136 303 308 313  75  33 158 168  76  12  34  13
  99 311 189 322 102 128 130  29  71 106 169 149 142   0 101 299  23 166
   7 292  94  14 159 288 286  57 139 295 174  73   8 146 318 145  77  22
 321 181  32 307 140 135  69 179 148 306 120  37 124  24 170 317  25  17
 143 291 297 293 185 186 302  15  20 312   4 184 300 192 305  39 177 175
 178  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0197
INFO voc_eval.py: 171: [43 67 77 63 69 46 57 47 29 56 68  5 45 72 38  4 59 22 50 37 25 44 74 17
 73 75 35 66 51 30 23 71  6  2 24 36 82 31 58 76  1 49 54  9 19 12 26 80
 52 28 70 64 13 32 60 65 27 78 48  0 41 61  7  8 62 34 39 40 81  3 18 11
 55 14 10 79 16 42 21 20 33 15 53]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.3077
INFO voc_eval.py: 171: [ 307  919  518 ... 1224 1244 1229]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0197
INFO voc_eval.py: 171: [48 50 43 28 31 39 16 29 17 15 14 33  9 49  3 35 34 38 19 41 44 30 10 18
  7 36 42  1 12 45 23  0 37 25 21  8  4 26 40  2  6  5 20 13 47 46 32 27
 11 24 22]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [1333 1330 1332 ... 2607 2612 1821]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0183
INFO voc_eval.py: 171: [28  8 20 36 13  7 25 17 38 39  3 14 16 21 27  0 71 50 62 49 44 30  6 57
 34 56 31 46 60 18  5  4 29 59 10  1 68 77 22 32 12 72 19 70 53 65 66 47
 54 61  9 58 23  2 43 45 15 78 11 24 26 69 63 67 75 42 74 79 35 64 80 48
 73 52 55 33 40 51 41 76 37]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0438
INFO voc_eval.py: 171: [528 547 692 287 550 552 622 553 554 463 555 462 612 546 557 300 301 602
 302 566 599 598 526 589 574 584 579 452 275 551 387 660 659 260 261 655
 522 638 653 652 262 264 533 266 265 237 527 255 268 537 523 663 644 529
 236 413 416 168 402 401 171 398 408 420 686 450 521 454 456 519 465 473
 394 693 518 516 690 680 688 421 451 315 372 217 219 221 222 286 283 226
 276 229 273 272 271 269 235 242 243 259 247 258 249 252 291 373 215 213
 371 370 365 362 183 355 188 189 190 322 197 198 203 204 205 299 207 297
 209 210 212 214 524 384 253 623 621 616 615 613 610 608 607 606 605 604
 603 601 600 596  65 525 594 592 591 590 624 588 625 628 674 673 672 661
 656 654  15 651 650 649 647  21 646  25 641 639 636  33 632 631 630 627
 587 254 543 108 549 568 570 548 545 563 542 541 576 565 577 106 578 580
 586 531 534 564 540 103 535 538  96 582  74  79  93 635 464 637 361 472
 643 515 517 657 664 677 256 640 556 289 561 359 585 713 336 583 375 329
 328 595 313 723 573 457 572 571 411 611 418 614 295 567 619 441 449 288
 626 406 717 676 184  26 225  27 182 193 218 216  88 185  84  78 200  37
  71 160  63  59 191 172 180  97 194 240  99 238 230 169 246 231 100 116
 741 732 390 593 186 506 179 187 597  62 192 520  56 739 539 581 369  95
 560 559  90 181 558 414 104 379  87 744 415  86 575  82 742 512 496 458
 202 658  16 293 440 439 438 620 618  32 633 220 196 314 224 684 722 721
 683 720 687 244 562  19 648 733 102 223 234 513 166 383 157 239 263 678
 245 536 173 767 662 645 530 629 617 199 419 685 201  80 206 437 208 499
 442  55 634 412 609 569 642 730 460 400 427 696 474 731 139 167 165 409
 471 443 403 495 151  77 532  57 675 727  14 195 353 354 148 360  48 110
 382 122 746 544 391 760 305  38 738 211  31 285 251 694 153  23 146 155
 267 470 468 446 124 250  69 316 740 494 514  58  66  91 507  89  70 669
 105 469 511  43  47 509  67 691  54 682  50 750 681 466 312 708 380 248
 270 156 729 358 330 429  61 710 395 410 393 755 756 282 399  75 303 671
 670 352 467 378 747 158  17 377  73 233 486 386 498 716 366 728 388 503
 121  94 344  60 130 718 126  92 145 290  29 154 338   8  13 381 348 475
 689  28 719 136 150 455 501 368 109 326 762 125 706 134 737 334 425 448
 343 389 117 346 461 679 701 508 232 296 170 493 487 433 228 298 396 749
 113 335  30 502 724  34 227 281 280 703 702 159 340 709  98 407 725 123
 500 444  85 176 699 357 138 241  45 137 768 766 759 178  10 131 142  41
 698 761 127  46 726 257 453 417 294   6 445 274  51  52  40 327 434 144
 751 310  49 385 277 164 129 332 132 331  18 404  22 510 736 323 422 459
 107  20 101  24 114 748 447 497 432 319 758 435 345 175 364 697 311 120
 376 695 397 128 284  39 143 392 133 349 306 765 505 307 704 668 333 424
 753  53 754 757 700 405  11 430 374  36 279 769 325   7  12 292 317 141
 743 162 752 734   2   9 504 119 488 367 481 118 745 324  68  83 321  35
 712  81 490 140 309 431  72  64 735 111  44 177 112   0 426 764 320 356
 342  76 341 351 484 483 308   4 363  42 135 347   1 763 423 147 337 350
 705 174 318 485 478 428 667 476 339 304 707 715 152 711   3 278 436   5
 161 492 149 714 477 163 491 115 479 480 666 489 482 665]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0063
INFO voc_eval.py: 171: [ 62  44  43  23  61  37  42  65  24 159 149  16  38  19 157 158 155 106
  41 164  35  27 112 117  29 166  26 115  63  55  46  17  20 151 153 150
 108  49  47  36  18 154  11 110 116 109  21  31 163  51  94 161  54 114
  28 136  25  52  22 111  34 162 105  91  56  13  59 113  45  90  12  30
 143 160 165  39 119  53  14 122  92 144  95  96  57  40  93  32 156  73
  68  66  58   9 135 152 120 107 139 133  87 142 140 141  88  85 137 123
  78   8 121 126  98  50  76  97  48 138 124 130 125 134  79  64  15 103
 128  60 101 131  71 148  10  33 147  70 129 145  84 104 118  69  81  67
 100  99 132 146   3  74 127  72  83   7  77  80   4 102  89  75  82   1
   6   0  86   5   2]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0124
INFO voc_eval.py: 171: [106 107  40 108  25 109 113 159  30 137  50 153  35 214  49 216  48  46
 217 101  88  87  80  79   2  78   6 233   4  14  15  96  98  18  13  47
  83  85 103  81 100  91 136  89  51 138 145  23 169   8 157   7   5  22
 154 150  41  84  20 195 199 201 254  97  86 236  19   3 190 241  45  39
 250 141 133  21 221 128 226 213 211 206 242 115 155 139 184 223 143 187
  31  53 239  28 147 225 111 247  36  24  17 204 104 156  12 229 122 257
  43   9 203  37  38  57  82 218 209  62 170 102 197  75 181 182 185 116
 135 210 166 192 249 142  93 117  61 246 244 222 129 198  27 130 234 125
  42 193 140 251 188  34 124  69 240 112 231 158  90 219  55  52  60  33
 152 230 220 131  94 164 167 127  11 265 148 126  63 165 215 227 252  56
  58 235 237 228 176 245  77 224   0 194 173 134 202  44  26 232  71 191
 132 200 144  10  59 276 123 149  68 162 114  66 174 180  16 268 212 163
 208 172 205 183 259 264 277 178  70 238 118 151 189 243 186 261  95 253
  32 120 179 260 171   1 258 160 196 263  29  73 255  72 248  92  54 105
 207 267  76 119  99 256 161 168 177 110 146 262  74 175  64 266 121 273
 274  67 269 270 271  65 272 275]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0232
INFO voc_eval.py: 171: [74 49 82 38 77 22 47 45 19 68 56 35 40 91 76 53 87 39 25 42 93 44 37  3
 62 73 88  8 43 29 46 66  4 94 17 92 15 63 81 51 85 83 70 52 54 32 26 50
 71 72 80  1  0 64 16 28 89 21 61 58 41 86 23 69 24 31 95 48  5 14 65  2
 33 13 55 67 78 90  9 84 57 27 20 75 79 34 30 59 18  6 60 36 12 11  7 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.1883
INFO voc_eval.py: 171: [13392  7590 16736 ...  7836  7825  5458]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0479
INFO voc_eval.py: 171: [293 518 516 515 300 398 345 548 396 342  81 545 395 299 399  94  79 566
 105 343 522 391 462 450 401 523 561  80  95 305 303 178 167 533  86  74
  64 166 504 104 502  24 392 514 173 346 347 270 507 530 501  53 148  51
 246 400 578 298 436 440 538 393  98  21 375 569 426  65 390 577 272  47
 509  11 103 344 557 169 268 448  87 276 256 423 294  55  69 454 532 397
 176  67 370 162 452  99 171  90  82  75 119 537 576 574 457 567  83 583
 348 430 526 374  48  96 394 133 341 127 447  84 446  71 560 170  73 257
 371 435 547 438 288 519 493  78 589 100  97  93  92  88 271  77 584  66
  68  70 459 497 460 135  85 321 172 198 184 163 480 277 535 249  89  76
 525  50 291 520  27 555 143 102 151 531 503 581 316 587  19  72 250 101
 466 495 380 442 274 461 490 573  91 468 251 116  60  58  28 125 134 541
 383 513 138 544 512 140 186 120 542 311 326 254 292 255 372 491 483 505
 203 521 381 427 290 149 282 378 319 425  40 215 377  49 324 302 275 258
 146 419 511 456 226 123 131 136 422 333 499 565 147 278 592 373 473 230
 527  54 182 384 124 174 209 350 210 181  20 498 156 417 161 409 320 579
 543 471 568 205 253  44 285 453 351 472 349 464   7 412 139 366 364 553
 168 113 236  38 126 281  41 389 122 329 334 164 273 117   1 318 196 247
 376 510 336   0 158 130 175  15 212 500  52 259 154 262 261 128 223 338
 243 558 386 160 152 416 335  43 580 295 591 322 539 308 590 443 307 420
 586   6 467 582 237 267  33 264 588 465 506 332 231 368 296  39   2 421
 379 238 142 144 429  59 339 571 235 194 245 572 312 482 137 118 475 445
 129 200 239 108 540  45 155 418 455 382 528  26 218 265 189 428  23 193
 195 214 187 402 190 260  42 496  36 197 369 191 165 153 479 424 114 115
 183 575 323 242 340 217 524 494 109 145 508 159 352 328 549 224  46   3
  10 559 585  18 309 317 408 157 150 310 337 222 486 410 228 248 413 206
 286 244 252 488 213 180  25 411 550 449 484 106 406 207 297 536 204 517
 359 284 388   5 403  30 564 111 266 327 201 489 227 385 225 570 234 141
 554 407 283 387 325 202 301 362 289   8 110 229 405   4 492  32 211 552
 444 404 470 185  29 563 358 188 107 433  61 192 112  35 121 355 414 315
  22 199 529  34  62 263 269  56 481 132 331 356   9 474 221 432 220 279
 208 306 469 287 451 415 463 216  13 304 177 477 280 330 179  14 241  63
  12 313 219 476  57 439 314 487 240 431 437 232 458 562  16 367 233 485
  17  31 478  37 441 434 551 354 556 357 534 546 363 353 361 360 365]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0909
INFO voc_eval.py: 171: [ 63  54 115  71  76  68  58  55  84  83  75  91  86  11  57  77 140  56
  66  16  50  72 185 103 122 145 151 125  97  49  32  51  92 193 142 126
 148 130 112  33 134  80 138  24 176 114 110 101  65  81  31 178  52  35
 186  27  29 175 141  18  89 180 188  74  61  70 163  96  53 106  22  62
  60 190  38  28  82  26  95  10  37 150 128   9 184   1  36  98 160  69
 159 109  15  59  99  73  64  78 154 156  34 158 136 143  23 111 132 107
 121 183 146  20 167  88 133  30 149 127 120   0 174  39  45 135 172 191
 105 166 192 164 117  94 113 177 170 165  19 102  48 137  90  12 144 118
 161  46  85 116 173 123   5 171 179 100  42  43  93 131  14  67   2 124
  40 153 139 129 152  79 155  13 119 104 147  87 108  21   7  17 189 187
  41  47  25   3 182  44 181   4 169 157 162   6 168   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0385
INFO voc_eval.py: 171: [106  94  76  98  33  93  80  34 103  66  32  16  41   0  72   1  43  30
 135 138  57  88 165  11   7  73 157  68  36 134 133  97 101  59  60   6
  77  70  10  42  12   8  15  95   9 118 130  89  14  86 139 126  67 147
  56  17 117  62 154  85 141 142  51  38  48  26  47 162 155  81  22  39
 131  31  40  24  27   4 151  50 120 166 144  53 137 100  49 161 156 167
   2  44  21  13  90 110 127  99 129 160 146 148  18   3 136  35  23  84
 102 132  91  61  45  65  52  79  75  46  19 158  69  92   5 143 153 107
 164 109  37  83 159  20  63  64 123 119 152 125 128 121  96  78 111 124
  71 149 150  55 145 122  58 140  82  28 105  87  54 104 112  74 116 163
  25  29 113 114 108 115]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0245
INFO voc_eval.py: 171: [123 331 329  96  34 323 157 159 170 196  25 200 111 252 109 326 105 328
 101 256 100 230 248 330 241 335 336  75  99 245 127 306 198 197 203 273
 178 172 204 169 168 167 166 165 161 337 160 207 259 262 338 247  27  30
 354  35 349 345 258  95 304 305 181 102 173 333 327  10 316 271 199  31
 263 128 312 254 318 339  50 362  19 348 164 289 261 347 119  39 231 103
  12  15  57 251  49  94 270 260 211 268  80 334 221 115  26 195  32 122
 193 118 130 132 163  18 108 238 250  92 219 202 284 340 136 244 242  73
 212 227 110  58 341 351 175 294 135 205 249 162  38  71  72  97 209  43
 185  48 295  81  85  56 229  21 344  42  98 357 324 356 253 208 278 239
 220 187 369 325  93 137  24  53 126  63 236 322 177 308 279 355   9  64
  60 257  90 332  44  66  54 309 131 194 366 300  13 281 232 228  78 191
 363  65 297  88 224  68 158  14 280  36 190  69 307  62  37  51  47 269
 124 234 192 291 218 235 290  74  91 117  33  22 120 114 292  46 243  41
  83 129 116 214 133   1 298 358 139 346   5   8 288 184  70 310 223 125
 106 138 285 222  20 365 213 206 186 233 104   4 275 156 293  59   3 176
  55 274 302 217 246 264 342 112 174  16  67 189  17 154 142  11 360 216
 201 147 182 283 313 352 353  79 141 319 143 113 148 320  29 255  52 180
 171 361 272 215 301  61 367 303  23 107  45 277 314  28 282 287 188 359
 240 144 286 155 315 121 145 226 183 152 265 368 311 343 179 153 266 225
  76 237 267 150   2 317 134 321   0  77  40 364   6 350   7 210  84 149
 296 151 276 299 140  86 146  82  89  87]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0548
INFO voc_eval.py: 171: [244 173 176 191 197 203  92  91 112 235 299 296 249 283 277 274 254 236
  20 266 117   9 128 383   5 121 133 119 118 365 137 131 136 251 146 246
 245 138 238 145  77 114  82  85 208 125 199 161 256 186 113 129 188 139
   1 301  24 366  22 386 295 200 319 156 162 320  18 172  19 325 228 265
 338 360 111 195 192 352  25 206 230  80   6 264 260 257 255 272 286 247
 293  67 354 187 240  72 148 384 306  78 151   7 150   3  14 389 155 375
  54 391 259 271 278  68 294  73 300 231 220  88  84 324 100 198 392 315
 323 232 166 385 371 207 227 289 340 141 373 355 190 219   0 124  17  96
  81  32 167 108 225  71  66 285 303 196 314 126 322 328 132  36 171 110
 287  64 241 356 363  99 381  10 135 169  52 168 362 317 152 115  50 261
 250 158 122 116 281  12 177 223 269 344 209 157 263 292 307 143 107 310
  15  97 233 234 336 204 160  34 342 185 368  61 390  31 164  55  57  76
  29 348 382 201 372  11 350  21 380 262 212 189 376 140 351 337 194 237
 175 276  45 288 102  90 224  94 214 353 105 318 273 304 213  13  37 316
 321 183 343 379  16  86 239   2 308  23 258 327 349 359 377  49 120 142
 311  58 329 275 364 280 184 345  53 253 361 341   4 205  46 216 242 388
  93  51 284 333  42  33 248 290 330  79 211 326 291 378 335 331 193 109
 312 222 370 106 134   8 357  65  30  47  75 346 347 159 127  95 218  87
  98  59  56 302 163 334 202 387  26 123  40 309  35  44 358 170 367 210
 305 154  62 103  27 104 332 181 229 369 282  60 298  83  39 149  28 174
 374  89 267 279 217 268 226  70 130  74 339 101 252 144 297 153  38  41
 147 270 165 313  69  43 179 243 215  63  48 221 180 178 182]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0269
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0637
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.132
INFO cross_voc_dataset_evaluator.py: 134: 0.061
INFO cross_voc_dataset_evaluator.py: 134: 0.009
INFO cross_voc_dataset_evaluator.py: 134: 0.103
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.308
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.018
INFO cross_voc_dataset_evaluator.py: 134: 0.044
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.012
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.188
INFO cross_voc_dataset_evaluator.py: 134: 0.048
INFO cross_voc_dataset_evaluator.py: 134: 0.091
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.025
INFO cross_voc_dataset_evaluator.py: 134: 0.055
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 135: 0.064
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.844s + 0.004s (eta: 0:01:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.471s + 0.005s (eta: 0:00:54)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.482s + 0.005s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.515s + 0.006s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.513s + 0.006s (eta: 0:00:43)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.489s + 0.005s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.475s + 0.005s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.460s + 0.005s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.449s + 0.005s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.444s + 0.005s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.441s + 0.005s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.437s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.434s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.522s + 0.004s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.519s + 0.008s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.544s + 0.007s (eta: 0:00:57)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.533s + 0.008s (eta: 0:00:50)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.520s + 0.007s (eta: 0:00:44)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.501s + 0.006s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.492s + 0.006s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.482s + 0.006s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.470s + 0.006s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.464s + 0.006s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.454s + 0.006s (eta: 0:00:11)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.449s + 0.005s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.440s + 0.005s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.626s + 0.014s (eta: 0:01:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.523s + 0.005s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.505s + 0.005s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.521s + 0.005s (eta: 0:00:49)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.505s + 0.005s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.482s + 0.005s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.466s + 0.005s (eta: 0:00:30)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.457s + 0.005s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.446s + 0.004s (eta: 0:00:19)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.439s + 0.005s (eta: 0:00:15)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.432s + 0.004s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.429s + 0.004s (eta: 0:00:06)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.427s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5499.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.674s + 0.003s (eta: 0:01:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.556s + 0.004s (eta: 0:01:03)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.513s + 0.004s (eta: 0:00:53)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.493s + 0.005s (eta: 0:00:46)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.477s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.452s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.442s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.432s + 0.004s (eta: 0:00:23)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.426s + 0.004s (eta: 0:00:18)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.424s + 0.004s (eta: 0:00:14)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.417s + 0.004s (eta: 0:00:10)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.415s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.412s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 67.773s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [126  25 107  46  45  42  31  27  23  48  41  40  81  16 124  18  26  98
  21  10 106  32   9  11  13 108   7  12  38  29  84 104  15  79  22  44
  85  47  87  99  35  28 102 120  92 101 114  24  69  53  37  95 121  96
 116 111  14  68  52  30  57  34  19  75 122 103 115 119  36 113  50  66
  64  80 110  20  62  91  97  43  54  72 123  56 117  73  33 100  77  90
   2  17  89 105  71  60  58 118  88  70  49  65  55  82 125  76  78  59
   6  83  74   4   3  86   1  51   8  67 109   0 112  61  39  94  63  93
   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0862
INFO voc_eval.py: 171: [163  24 142 145 150  56  20  55  70  71 138 103 134 133 149  25 164 154
  98 157  57  84  53  62 141 140  26  69  43  22  21  60  19  97  66   5
  72  58 160  18  59 116  99 158  83 139  86  64  68  65  63  76 124 119
   3 143 146  78  87 120 105 101 123  94 115 148 136  51 135 147 159 114
  33 153 106 152 161  95 156  23  34 137 162  48 151 155  47  75  12  73
 104 127  36 144   6   1 102  54  89 128 113 110  67  79  49  31   9  93
  50  82  74 109  52 131  17  42 100  38 122 125   4  41  32 117 126  77
  35 130 121  39  29  85 107  88  44  13  37 132   2   0  80   8  45  27
 129  81 111 112  92  96  61 108 118  90  16  10  28  40  46  14  15  30
  91   7  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0671
INFO voc_eval.py: 171: [437 840 859 ... 975 592 719]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0163
INFO voc_eval.py: 171: [454 555  83 380 630  88 552 513  82 401 400  81 542 541 455 124 461 122
 453 520 352 303 615 528 199 633 457 413 298 462 350 304 205 634 427 546
 428 384 371 389 120 522 326 270 336 261 308  10 474 257 129 419 307 511
  25 232 361  18 492 107 151 550 516 314 420 431 131 334 346 562 564 231
 245 394 248  11 223 370 305 373 505 335 135 616 417 344  14 343 460 342
 260 471 339 600 358 521 200 410 230 286 149 363 367 381  31 582 264 407
 421 190 191 332 388 196 347 329 414 242 125 395 302 525 526  30 109  63
 111 468 565 359 524 316 296 322  26 276 221 625 406   9  23 201 553 397
 374  44  69 556 561  48   6 269 436  94   4 349 348 621 493  12 319 567
 573 345 391 320 224 387 463 237 235 597 365 614  52 136 423 476 156 543
 517 547 169 309 357 153  86 382 376 378 137 297 306 229  28 464 227 628
 626   5 577 422 265 312  37 568 234  38 282 398 355 266  76 243 333 589
  56 202 433 584 477 126  29 263 594 104 456 415 592 206  35 157 195  53
  39 284  85 549 165 399 188 340 495 581 255  58  16 318   7  87  24  32
 441 288 218  41 354 240 458 411 222 114 440 160 512 212 588  42 127 258
 192 578  51 635  17 220 585 583 113 467 132 430 238 609 591 503 445 150
 429 161 418 379 217 351 424 416 409 393 590 362 434   0 116  89  49 412
 438 133 372 118 497  77 554 575 278  73 619 294 283 281  15 489 146 323
 272 523 106  36 259 247 485  57 631  99  22 605 172  46 622 569 119 593
   2 175 148 337 105 121 450  13 608 595 587 383 377  45 330 465 301 484
 386 168 560 390 185 310  33 173  54 338 572 481 163 325 244 280 617  80
 629 627 291 187 138 194 392 518 117 219 501  67 579 596 162 176 451  70
 300 315 289 193 141 601 155 385   1  79 182 364  61  62 100 437 466 563
  59 499 632 189 295  50 178 134 253 494  21  20 519 327 480 470 103 446
 444 580 267 250 154 604  68 292 515 256 290 510 529 571 213 170 500 108
 186 241 328 239 459   8  66 313 123 618 402 181  96 204 478 360 183 403
  27 599 527 271  43 324 488 273 225 551 498 548 611 184  75 504 598 610
  84  97 145 432 425 545 369 469 404 447 179 496 576 559 620 299 530  19
 174 115 215 197 214 624 613 356 586 506 249 216 211 331 557 482 171   3
 159 228 353 209  93 368 396 443 275 472 566 140 491 612 233 164 144 210
 435  71 623 502 251  91 483 110 167  34 207 486  55 606 405 311 262 112
 102  60 475 570 101 226  40 538 408 279 449 158 514 366 208 531 607 603
 426 558 534 317 375 268 509  74  72 287 602 152 128 448 507  47 341 143
 203 321 198  90 540  65 442 236 539 277 139 246 254 487  78 535 252 490
  98 473 532 508 293 439 452 479 544 274 574  64 536 533 537 285 166 177
  92 142  95 180 130 147]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1042
INFO voc_eval.py: 171: [276 169 278 154 265 243 238 209 163 165 211 214 215 261  61 274  86 242
 166 173  85 230 244 250 257 109 224 253 226 229 264 281 268 241 202 259
 260 217 262  44  41 316 258 223  54 270  87  95  91 256 269 249 155  88
 114 213 254  47 205 158 111 200 198 192 267  67 279  10  53  50 239 246
  55 236 277 206  28 210 233 157 174 300   5  48 321 222  98 317 245  93
  46  45  83 291 234 232 208  73 231   3  11 167 216 153 140 117 116 327
  27  75  59  90 286 273 162 255  92  81 303  38  42 225 199 203  84 311
  66 124 121 115 112 125 266 272 207 252  63 197 212 318 193 296  82 201
 118  68 228 159 189  80 218 285 219 287 247 119 101 135 133 289 149  43
  69  49  56  57 204 104 237 240 248 156 326 271 263  18   1 292 185  16
 182  65   2 175  35  97 275 196 221 220 322 127 108 235 283 227  19 120
 190 146 113 251 136 195 280 134 123 284 178 282 106 143   9 131 312  60
  64 152 164  30 129  71  99  26  31 139  36 184  89  21   6  51  96  62
  79 105 128 110 306 138 305 299  33 315  94 310  76 170  34  13  77 160
 191  72 100 132 325 313  12 324 130  29 103 171 102 301 144 107 151   0
 168  23  14   7 294 290 297 288 161 141  58  52 176  74 320 148   8 147
  78  22 323 309 183  32 142 137 150 308  70 181 122  37 126  24 319  17
  25 172 145 293 298 295  15 188 187 304  20 314 186   4 302 194 307  39
 179 177 180  40]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0194
INFO voc_eval.py: 171: [63 43 68 79 71 46 57 58 47 29 70 45  5 38  4 74 22 59 44 26 37 50 75 77
 35 17 65 67 51 30 23 73  2  6 76 25 31 36 84 78 24  1 49 54 19  9 12 27
 52 82 72 55 64 60 13 32 61 66 28 80 69 48  0 41  7  8 62 40 39 34  3 83
 18 11 56 14 10 81 16 42 21 20 33 15 53]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.2154
INFO voc_eval.py: 171: [ 623  516  318 ... 1221 1242 1226]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0190
INFO voc_eval.py: 171: [46 48 41 26 37 29 27 15 16 14 13  8 31 47  3 33 32 18 36 39 42 28 17 34
 40  6  7  2 10 43 22  0 35 20 38 12  1 24  4  5 11 19 44 45 30 25  9 23
 21]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [1338  247  254 ... 2615 2620 1825]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0163
INFO voc_eval.py: 171: [28  8 19 36 13  7 25 17 39 38  3 14 16 21 27  0 71 50 62 49 44 30 57  6
 34 56 31 60 45 18  5  4 29 59 10  1 68 22 77 32 12 20 72 70 53 65 66 47
 54  9 61 23 58  2 43 46 15 11 78 24 26 69 63 67 75 42 74 64 79 35 48 80
 73 52 55 33 40 51 41 76 37]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0434
INFO voc_eval.py: 171: [607 256 562 251 645 382 233 553 257 552 615 550 649 549 548 547 653 232
 546 570 260 594 595 638 634 296 297 640 258 298 585 598 580 264 262 261
 574 271 654 647 282 458 545 532 526 525 524 523 522 521 459 444 686 446
 518 542 657 541 272 199 604 198 178 183 596 597 265 267 268 269 184 197
 185 192 193 201 221 254 220 222 603 602 215 214 225 278 601 213 211 203
 231 207 206 600 205 238 239 243 245 248 249 250 208 599 679 592 397 402
 407 411 415 416 544 543 540 538 537 536 535 396 448 450 468 530 529 528
 520 685 683 519 517 515 514 512 534 393 389 379 287 590 293 588 295 587
 586 584 311 583 317 605 578 576 575 573 572 566 564 350 357 561 361 560
 365 366 367 368 559 281 582 255 625 666 641 610 636 617 616 105  20 648
 611  88 672 650  91 103  94 667  14 665 633 618 635 627 623 622 166  63
  24  72 646 680 644 608 655 628 642 619 632  77  32 163 631 658 626 443
 283 284 591 101  36 112 567 291 710 369 356 354 557  61 568 716 569  76
 332  58 325 324  82 579 581 551  86 400 405 309 413 445  95  97  98 706
 651 436 252 219 218 668 210 209  26 155 676 513 194 226 511 188 186 164
 180 179 609 177 175 167 606 189 227  69 620 456 242 235 460 236  25 621
 467 451 502 715 629 508 571 187 364 577  80 435 191 554 614 181 385 434
 555 714  60  31 713 563 556 533 374 452 176 174 182 433 196 677 737 102
 735 288 613 681 589 217  15 652 734 212 593 725  90 492 410 408  84 240
 310  54 675 732 516 656 531 558  18 378 454 669  53 406 437 431 495 760
 414 168 152  78 195 643 230 202 234 241 678 200 624 509 639 100 726 259
 637 723 565 630 612 216 161 190 144 739 355 724  46  13 146 689 162 422
 671 438 107 539 118 432  87 455 398 403 135 491 527 469 377 386 349 395
  55 348 720  75 160 466 263 120 441  30 204 687 150 490 247 246 465 463
  22 142 148 510  37 503 302  67  64 733 280  68  56 731 312 464 505 266
 375 461 722 353  52 507 244  48  65  45 674 673  41 151 684 701 753 308
 748  73 347 740 390 743 277 424 703 662 388 299 663 664 394 326 404 462
 494 499 153 709  16 749  71 372 373 380 383  59 481  85 229 721 117  89
  92 360 122 711  28 126 141 339 132 449 149 470 285 376  12  57 712 343
   7 334 442 145 682 420 699 106 730 497 130 322  27 330 363 121 384 338
 755 113 504 341 427 457 670 489 224 228 482 294 498 717  29  33 292 391
 276 165 742 110 694 695 223 331 154  96 335 275 696 702 401 119 439 496
 718 171  43 352  83 134 133 173 123 761 127  40 719 138   9  44 237 253
 290 752 691 754   5 759 412 140 440 447 428  50  49  38 692  47 323 270
 744 125 429 381 327 306 273 286 328 128  17 159 399  21 729 506 319  19
 453  99 417 111 493 104  23 315 741 751 170 340 430 307 359 116 371 392
 279 690  39 124 501 129 344 139 387 688 758 419 329  51 303 301 747 697
 661  10 425 750  93  35 274 409 321 746 693 370  11 736 762   6 313 289
 137 500 727   2 738 157   8 114 476 115 483  66 705 362 745  34 318 426
 320  81 485 136  70 108 728 109  79 305  62 172   0  42 421 316 351  74
 336 337 478 479 756 346 304 358   4 131 342   1 757 418 143 333 345 169
 480 473 314 423 698 471 660 708 700 300 147 704   3 156 487 707 472 158
 488 486 474 475 484 659 477]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0063
INFO voc_eval.py: 171: [ 65  62  45  44  24  66  43 163  38  25 161 162  39  20  17 153  28  42
 159  30 120 109 168 115 118  36 170 155  18  47 111  27  21  63 157  56
 154  48  50  19  37 119 158 113  12 112  22 117 167  97  55  52  53  26
 139  32 108  35 165  29  23 114 166  60  14  94 116  57  46  13  93 147
 164  41  31 169  15  54 122  95 125  33  58  96  98 148  72 160  76  40
  99  59 138  67  10 142 156 123 110 144 136  90 146 171 145  91  88  51
 126 141  49 129   9  81 101 100 124  79 127 143 134 128  16 152  82 137
  61  74 133  64 106 104 151 131  11  71  34  87  69 132 149 107 121  68
  84 135   4 103 102 150  73 130  85  77 140   8  70  80  75  83   5 105
  92  78  86   7   2   1  89   6   3   0]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0124
INFO voc_eval.py: 171: [108  41 109 110 111  26 161 115 220  31 155  51 217  37  50 219  49  47
 103 139  80  89   2  19  82  81  90   4   6  13  98  15 100 235  12  48
 102 140  85  52  83 138 105  87  91  93  23  42 147 152   5 156   7   8
  24 171 159 238  20 208 204 192 256 201 197  86  88   3  99  21 143 130
  40 216 252 135  57  22 224  46 117 229 214 244 243 228 232 157 141 145
 186 226 106 189  36  39  32  54  29 241  44 221 158  25 206 149 205 113
  64  16 249  11 259 212 183  77 137 124 184 104  58 251 118 187 172  95
 199  38  84 144 131  69 119 246  61 213 248 194 225 168  17  28 200 132
 254 127 142 236  43 195  34 190 222  65 114 126 234  92  53  55 160  71
 242  35  96  10 169 233 154 223 166 267 133 150 129 167 218 128 230 253
  56 237 231 178 239  59  79 247 196 227   0  63 136 175  45 203  27  73
  67 193 146 202 134   9  60 277 125 151  66 164 116  70  68 176 182 270
 215  14 165 210 174 207 261 185 266 278 240  72 153 120 191 188 245 263
  97  18  33 255 173 181 122   1 262 162 198 180 260 257 265  30  74 250
  75  94 209 107 269  78 121 258 101 163 211 170 112  62 179 148 264  76
 177 268 123 275 276 271 272 273 274]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0241
INFO voc_eval.py: 171: [73 49 82 38 76 21 45 47 67 56 28 35 40 91 75 53 87 24 42 39 44 93 37 62
 72 88  3 43  8 29 46 65 22 94  4 16 92 51 63 14 81 83 85 52 69 54 32 25
 50 70 71 80  1  0 15 27 89 61 20 58 41 86 68 23 31 95 48 18  5 13 64  2
 33 55 66 77 90  9 84 57 26 19 74 78 79 34 30 59 17  6 60 36 12 11  7 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.1983
INFO voc_eval.py: 171: [16723 12789 13382 ...  5405  5458  7830]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.0510
INFO voc_eval.py: 171: [295 516 347 518 302 400 515 548 398 344 545 397  82 301 452 462 106 566
 393 401 307 345  80  95 522 181 403 170  96  81 523 533  87 394 306 561
 502  65 501 169  24 504 514 176 349  75 105 348 578 428 300 507 274 150
 272 438 248 377  54  53 402 530  99 278  21  11 577 392 395  47 509 569
 442 538  66 270 258 425 172  88 456 454 165 450 104 346 296 557 532  70
  76  61 174 100 372  68 574 376  91 179  83 399 567 576 396 537 526 343
 583 120 440 173  97 432  48 458  84 350 128 373 547  85  72 493 449 448
 290  74 437  94 519 259 560 134  86  51 323 273  98  79  93 101 589  89
  67 584 497  71 460 459 525  78  69 136 520 279 293 555  27 166 535 187
 175 201  49 480  90  77 251 531 145 318 495 503  73 581 103 102 587 153
  19 252 467  50 444 254  57 461  92 276 382 573 117  28 139 490 469 513
 126 135 541 542 385 141 512 544 313 505 121 189 491 374 294 483 257 256
 328 429 383 292 521 206 321 151 304 427 284  58  40 380 326 124 148 260
 223 379 277 466 565 421 511 499 474 335 375 280 424 498 592 149 125 232
 137 177 132 213 386 127 352 159  20 212 419 568 527 184 185 353 322 351
 411 164 208 543 472 579 255  44 287  41 414 366   7 473 455 553 198 391
 368 238 171 464 331 283 114 118 123 275  39 140 336 249 167 510   1 378
 320   0 178 131 338 216 161 215 263 500  15 157 339 261 558 225 129 245
 264 142  56 324 580 163 297 337 591  43 388 418 154   6 590 309 582 468
 422 445 310 465 539 266 586 588 269 239 334 370 423 381  33 240 506 233
 146 144  38   2 298 431 571 197 247 341 237 572 482 119  52 138 314  59
 540 476 447 242 130  45 109 158 203 528 457 420 221 384 430  26 192 262
 199 196 193 190  23 404 267 496  42 168 371 155  35 200 116 479 426 194
 115 575 186 524 217 244 325 342 147 494  63 508 110 354  55  46 559 330
  10 585 549 227 162  18 319   3 410 160 312 340 219 311 486 152 224 415
 209 288 250 253 412 230 246  25 550 488 413 183 210 207 451 536 408 299
 517 484 107 390 361 564 405  30 286   5 268 204 329 229 489 228 112 387
 143 236 570 409 285 554 389 291 303 364 205   8  64 111 552 492 327 407
   4 214 231  32 188 446  29 406 359 191 563 108  60 195 471 435  36 113
 122 357 416 202 317  22  34 529 156  62 265 271 481 133 358 333 475   9
 434 222 211 281 308 470 289 453 417 463  13 305 180 478 282 332 182  14
 243 315  12 220 477 226 218 316 441 487 241 433 234 439 562  16 369 235
  17 485  31  37 443 436 356 551 556 360 534 546 365 355 363 362 367]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0900
INFO voc_eval.py: 171: [ 65  54 117  77  60  72  58  55  83  90  85  86  11  67  57  78  56  69
 141  16 186  51  74 124  70 102 146 127 152  49  94  32  91  50 143 129
 194 149 114  33 135 139  81  24 107 177 112 116  68  82 179  63  53  31
 187  35  27 176  29 142  18 189  73  76 181  62 164  52  95 108  96  22
  61  64  28  38 191  93  26  10 151  37 185   9 130   1  97  36 161 111
 160 100  15  59  66  75  98 157 155  34 159 144 137  23 113 133 123 109
 104 184 147  21 168 134  88  30   0 150 122 128  45  39 175 136 192 173
 105 193 167 165  92 119 178  19 115 171 106 166  48 101  80 138  89  12
 145 120  84  46 162  87 174 118 126 172 180   5  99  43  42 132   2  71
  14 125  40 154 140 131 153  79 156  13 121 103 110 148  20   7  17 190
 188  41  47  25   3  44 183 182   4 170 158 163   6 169   8]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0384
INFO voc_eval.py: 171: [ 94  77 105  97  33  34  68  82 101  32  93  74  41  43  16   0   1  30
  89  11  58 136 133   7 164  90  96 100  61  62  70  36 156 131 155 132
  72  78   6   8  42  10  12  15  81   9  91  95 128 117  14 137  69  53
 145  57 116  64  17  87 123 152  88 139 140  50  39  26  47  40 154 161
 129  83  31  22  38  24   4  27 149  49  92 141  54 165  99 135 119 160
  48 166   2 153  13  20  44  52 127 109  98 159 147 144 125  18   3 134
  35  86  23 130 102  63  45  67  51  80  46  76 157  19  71   5 142 151
 163 108 106  37 158  85  65  21  59 121 118 150 124  66 126 120 110 122
  79  73 146 148  56 143 138  60  84  28 104  55 103 111 115  75 162  25
  29 112 107 113 114]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0242
INFO voc_eval.py: 171: [124 333  97 331  34 325 158 160 171 197  25 201 247 254 328 112 110 330
 106 232 102 101 100 250 332 243 337 338 339 258 204 128 307 199 198 205
 275 179 173 264 170 169 168 167 166 162  76 161 208 261 308 340 249  35
 351  30 356  27 306  50 320 256 329  96 273 335 314 200 260  10 182 347
  31 265  19 165  57 129 364 341 318 350 103 291 174  95 104  39  12 272
 263 270  81 262 120  26 233 212 116 253  49 222  15 336 349 203 196 119
  32 220 123 195 131 133  93 286 252 240  18 164 109 137 111 353 342 245
 206 343  74  58 229 213 296 176  72 186 251  48 136  98 163  43 210  38
  73 231  86  82  42 297  56 358 346 326  21 359  99 138 327 209  94  63
 187 221 280 255 241 371 178 238  53  24 126 324  60   9  64 357 281 310
 244  91  44 334  71 132 194 311  66  54 259 368 283 192 230 302  13 234
  65 299  79 365  68  89 159 225 282  14  36 271  37  47  51  69  61 309
 191 219 237 293 236 193  92 292 118  75  46 294  33  22 121 115 130 117
  84  41 246   1 300 215   8 134 140   5 360  70 348 312 290 185 125 224
 107  20 139 223 188 367 287 157 207 214 277   4  59 235 105   3 276 295
 177  55 304 248 218 266  16 113 344  67 155 175  17 190  11 143 217 202
 362 183 148 315 285 355 142 354 321  80 149 144 114 322  29 181 257  52
 274 172 363 216 303  62 305 369  23 108 279 316  45  28 189 289 361 284
 242 145 288 317 156 122 146 228 184 153 268 370 313 345 180 154 267 226
 239 269  77 151 135   2 323 319   0  78  40 352 366   6 211 127   7 227
 150  85 298 278 152 141 301  87 147  83  90  88]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0523
INFO voc_eval.py: 171: [ 93 132 134 237 284 278 275 268  20 174 255 192 250 198 204 245  94 178
 298 138   9 113 114 384   5 366 118 301 119 236 120 122 129 146  79 208
 116 200 137 126  87 139 189 187 140 162 147  84 131 240 257  23 297 303
  26 253 367  25 387 246   1 248 385 361 355 353 339 326 322 321 308 295
 112 229 274 231 209 241 201 247 193 256 188 288 258 172 163 265 157 152
 149 267 261 196 393  19  67  80  74   6   7  55  82  18  86 324 325  91
  14 199 392 374 167 156 220 376 151  69 232  75   3 302 260  33 390 100
 296 273 279 221   0 323 191 207 228 317 233  17 291 356 142 341  83 386
 372 125  98 173 305 316 168 127 109 111 197 226  37 364 287  73  68 133
 329 242 170 136 289 357  10  65 115 117  53 382 319  51 108 251 123 282
 262 363 153 169 159  22 294 210 144 224 271 158  12 177 345 264 312 309
 165 102  99 205 391 337  15 235 161  35 234  62  32  30 383 369  58 186
 343 349  11  56  78 202  21 373 351 238 263 381 195 141  45 190 338 213
 377 225 290 352 354 176 320 277  92  96 215 106 306 184  38 214 318  16
  24  89 379 103   2  13 344 143 239 378 310 121  50 360 313 259 328  59
 350 276 330  54 281 342 365 346 254 185 362  47   4  95 206 217 286 243
 389  52  43  34 334 249 331  81 336 212 327 292 293 332 314 194 380 110
 371 223   8 107  66 135 358 348 347  48  31  77 160  97 128 219  88  60
 203 101 304 335 164  57 388  27  41 124 311  46  36 171 368 359 211 307
 104 155  63  28 105 333 182 230  61 370 283  85 300  40  29  90 175 150
 375 280 269  72 218 270 130  76 227 340 252 145  39 299 154  42 148 315
 272 166  44  71 180 244 285  64 266 216  49 181 222  70 179 183]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0273
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0579
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.086
INFO cross_voc_dataset_evaluator.py: 134: 0.067
INFO cross_voc_dataset_evaluator.py: 134: 0.016
INFO cross_voc_dataset_evaluator.py: 134: 0.104
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.215
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.016
INFO cross_voc_dataset_evaluator.py: 134: 0.043
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.012
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.090
INFO cross_voc_dataset_evaluator.py: 134: 0.038
INFO cross_voc_dataset_evaluator.py: 134: 0.024
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.027
INFO cross_voc_dataset_evaluator.py: 135: 0.058
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='configs/dt/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det.yaml', dataset='clipart', load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=True, num_classes=None, output_dir=None, range=None, set_cfgs=[], vis=False)
INFO test_net.py:  83: Automatically set output directory to Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO subprocess.py:  59: 0,1,2,3
INFO subprocess.py:  67: gpu_inds: [0, 1, 2, 3]
INFO subprocess.py:  89: detection range command 0: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 0 125 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 1: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 125 250 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 2: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 250 375 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py:  89: detection range command 3: python /home/ubuntu/Detectron.pytorch/tools/test_net.py --range 375 500 --cfg Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml --set TEST.DATASETS '("cross_clipart_test",)' --output_dir Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test --load_ckpt Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 0 with range [1, 125]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[0, 125], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 1/125 0.525s + 0.003s (eta: 0:01:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 11/125 0.393s + 0.004s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 21/125 0.397s + 0.005s (eta: 0:00:41)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 31/125 0.397s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 41/125 0.404s + 0.005s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 51/125 0.398s + 0.005s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 61/125 0.393s + 0.004s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 71/125 0.395s + 0.004s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 81/125 0.393s + 0.004s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 91/125 0.391s + 0.004s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 101/125 0.392s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 111/125 0.392s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [1, 125] of 500: 121/125 0.394s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_0_125.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 1 with range [126, 250]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[125, 250], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 126/250 0.587s + 0.004s (eta: 0:01:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 136/250 0.393s + 0.004s (eta: 0:00:45)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 146/250 0.399s + 0.005s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 156/250 0.392s + 0.005s (eta: 0:00:37)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 166/250 0.389s + 0.004s (eta: 0:00:33)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 176/250 0.391s + 0.004s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 186/250 0.402s + 0.004s (eta: 0:00:26)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 196/250 0.400s + 0.004s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 206/250 0.399s + 0.004s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 216/250 0.404s + 0.004s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 226/250 0.401s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 236/250 0.402s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [126, 250] of 500: 246/250 0.396s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_125_250.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 2 with range [251, 375]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[250, 375], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 251/375 0.384s + 0.003s (eta: 0:00:47)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 261/375 0.417s + 0.004s (eta: 0:00:48)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 271/375 0.386s + 0.004s (eta: 0:00:40)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 281/375 0.405s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 291/375 0.402s + 0.004s (eta: 0:00:34)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 301/375 0.400s + 0.004s (eta: 0:00:29)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 311/375 0.396s + 0.004s (eta: 0:00:25)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 321/375 0.395s + 0.004s (eta: 0:00:21)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 331/375 0.394s + 0.004s (eta: 0:00:17)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 341/375 0.393s + 0.004s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 351/375 0.392s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 361/375 0.391s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [251, 375] of 500: 371/375 0.392s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_250_375.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO subprocess.py: 129: # ---------------------------------------------------------------------------- #
INFO subprocess.py: 131: stdout of subprocess 3 with range [376, 500]
INFO subprocess.py: 133: # ---------------------------------------------------------------------------- #
INFO test_net.py:  72: Called with args:
INFO test_net.py:  73: Namespace(cfg_file='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_config.yaml', dataset=None, load_ckpt='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth', load_detectron=None, multi_gpu_testing=False, num_classes=None, output_dir='Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test', range=[375, 500], set_cfgs=['TEST.DATASETS', '("cross_clipart_test",)'], vis=False)
INFO test_net.py: 125: Testing with config:
INFO test_net.py: 126: {'BBOX_XFORM_CLIP': 4.135166556742356,
 'CROP_RESIZE_WITH_MAX_POOL': True,
 'CUDA': False,
 'DATA_DIR': '/home/ubuntu/Detectron.pytorch/data',
 'DATA_LOADER': {'NUM_THREADS': 4},
 'DEBUG': False,
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXPECTED_RESULTS': [],
 'EXPECTED_RESULTS_ATOL': 0.005,
 'EXPECTED_RESULTS_EMAIL': '',
 'EXPECTED_RESULTS_RTOL': 0.1,
 'FAST_RCNN': {'CONV_HEAD_DIM': 256,
               'MLP_HEAD_DIM': 1024,
               'NUM_STACKED_CONVS': 4,
               'ROI_BOX_HEAD': 'fast_rcnn_heads.roi_2mlp_head',
               'ROI_XFORM_METHOD': 'RoIAlign',
               'ROI_XFORM_RESOLUTION': 7,
               'ROI_XFORM_SAMPLING_RATIO': 2},
 'FPN': {'COARSEST_STRIDE': 32,
         'DIM': 256,
         'EXTRA_CONV_LEVELS': False,
         'FPN_ON': True,
         'MULTILEVEL_ROIS': True,
         'MULTILEVEL_RPN': True,
         'ROI_CANONICAL_LEVEL': 4,
         'ROI_CANONICAL_SCALE': 224,
         'ROI_MAX_LEVEL': 5,
         'ROI_MIN_LEVEL': 2,
         'RPN_ANCHOR_START_SIZE': 32,
         'RPN_ASPECT_RATIOS': (0.5, 1, 2),
         'RPN_COLLECT_SCALE': 1,
         'RPN_MAX_LEVEL': 6,
         'RPN_MIN_LEVEL': 2,
         'USE_GN': False,
         'ZERO_INIT_LATERAL': False},
 'GROUP_NORM': {'DIM_PER_GP': -1, 'EPSILON': 1e-05, 'NUM_GROUPS': 32},
 'KRCNN': {'CONV_HEAD_DIM': 256,
           'CONV_HEAD_KERNEL': 3,
           'CONV_INIT': 'GaussianFill',
           'DECONV_DIM': 256,
           'DECONV_KERNEL': 4,
           'DILATION': 1,
           'HEATMAP_SIZE': -1,
           'INFERENCE_MIN_SIZE': 0,
           'KEYPOINT_CONFIDENCE': 'bbox',
           'LOSS_WEIGHT': 1.0,
           'MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH': 20,
           'NMS_OKS': False,
           'NORMALIZE_BY_VISIBLE_KEYPOINTS': True,
           'NUM_KEYPOINTS': -1,
           'NUM_STACKED_CONVS': 8,
           'ROI_KEYPOINTS_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'UP_SCALE': -1,
           'USE_DECONV': False,
           'USE_DECONV_OUTPUT': False},
 'MATLAB': 'matlab',
 'MODEL': {'BBOX_REG_WEIGHTS': (10.0, 10.0, 5.0, 5.0),
           'CLS_AGNOSTIC_BBOX_REG': False,
           'CONV_BODY': 'FPN.fpn_ResNet101_conv5_body',
           'FASTER_RCNN': True,
           'KEYPOINTS_ON': False,
           'LOAD_IMAGENET_PRETRAINED_WEIGHTS': True,
           'MASK_ON': False,
           'NUM_CLASSES': 21,
           'RPN_ONLY': False,
           'SHARE_RES5': False,
           'TYPE': 'generalized_rcnn',
           'UNSUPERVISED_POSE': False},
 'MRCNN': {'CLS_SPECIFIC_MASK': True,
           'CONV_INIT': 'GaussianFill',
           'DILATION': 2,
           'DIM_REDUCED': 256,
           'MEMORY_EFFICIENT_LOSS': True,
           'RESOLUTION': 14,
           'ROI_MASK_HEAD': '',
           'ROI_XFORM_METHOD': 'RoIAlign',
           'ROI_XFORM_RESOLUTION': 7,
           'ROI_XFORM_SAMPLING_RATIO': 0,
           'THRESH_BINARIZE': 0.5,
           'UPSAMPLE_RATIO': 1,
           'USE_FC_OUTPUT': False,
           'WEIGHT_LOSS_MASK': 1.0},
 'NUM_GPUS': 4,
 'OUTPUT_DIR': 'Outputs',
 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),
 'POOLING_MODE': 'crop',
 'POOLING_SIZE': 7,
 'PYTORCH_VERSION_LESS_THAN_040': False,
 'RESNETS': {'FREEZE_AT': 2,
             'IMAGENET_PRETRAINED_WEIGHTS': 'data/pretrained_model/resnet101_caffe.pth',
             'NUM_GROUPS': 1,
             'RES5_DILATION': 1,
             'SHORTCUT_FUNC': 'basic_bn_shortcut',
             'STEM_FUNC': 'basic_bn_stem',
             'STRIDE_1X1': True,
             'TRANS_FUNC': 'bottleneck_transformation',
             'USE_GN': False,
             'WIDTH_PER_GROUP': 64},
 'RETINANET': {'ANCHOR_SCALE': 4,
               'ASPECT_RATIOS': (0.5, 1.0, 2.0),
               'BBOX_REG_BETA': 0.11,
               'BBOX_REG_WEIGHT': 1.0,
               'CLASS_SPECIFIC_BBOX': False,
               'INFERENCE_TH': 0.05,
               'LOSS_ALPHA': 0.25,
               'LOSS_GAMMA': 2.0,
               'NEGATIVE_OVERLAP': 0.4,
               'NUM_CONVS': 4,
               'POSITIVE_OVERLAP': 0.5,
               'PRE_NMS_TOP_N': 1000,
               'PRIOR_PROB': 0.01,
               'RETINANET_ON': False,
               'SCALES_PER_OCTAVE': 3,
               'SHARE_CLS_BBOX_TOWER': False,
               'SOFTMAX': False},
 'RFCN': {'PS_GRID_SIZE': 3},
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/ubuntu/Detectron.pytorch',
 'RPN': {'ASPECT_RATIOS': (0.5, 1, 2),
         'CLS_ACTIVATION': 'sigmoid',
         'OUT_DIM': 512,
         'OUT_DIM_AS_IN_DIM': True,
         'RPN_ON': True,
         'SIZES': (64, 128, 256, 512),
         'STRIDE': 16},
 'SOLVER': {'BASE_LR': 0.02,
            'BIAS_DOUBLE_LR': True,
            'BIAS_WEIGHT_DECAY': False,
            'GAMMA': 0.1,
            'LOG_LR_CHANGE_THRESHOLD': 1.1,
            'LRS': [],
            'LR_POLICY': 'steps_with_decay',
            'MAX_ITER': 6000,
            'MOMENTUM': 0.9,
            'SCALE_MOMENTUM': True,
            'SCALE_MOMENTUM_THRESHOLD': 1.1,
            'STEPS': [0, 2000, 4000],
            'STEP_SIZE': 30000,
            'TYPE': 'SGD',
            'WARM_UP_FACTOR': 0.3333333333333333,
            'WARM_UP_ITERS': 500,
            'WARM_UP_METHOD': 'linear',
            'WEIGHT_DECAY': 0.0001,
            'WEIGHT_DECAY_GN': 0.0},
 'TEST': {'BBOX_AUG': {'AREA_TH_HI': 32400,
                       'AREA_TH_LO': 2500,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'COORD_HEUR': 'UNION',
                       'ENABLED': False,
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False,
                       'SCORE_HEUR': 'UNION'},
          'BBOX_REG': True,
          'BBOX_VOTE': {'ENABLED': False,
                        'SCORING_METHOD': 'ID',
                        'SCORING_METHOD_BETA': 1.0,
                        'VOTE_TH': 0.8},
          'COMPETITION_MODE': True,
          'DATASETS': ('cross_clipart_test',),
          'DETECTIONS_PER_IM': 100,
          'FORCE_JSON_DATASET_EVAL': False,
          'KPS_AUG': {'AREA_TH': 32400,
                      'ASPECT_RATIOS': (),
                      'ASPECT_RATIO_H_FLIP': False,
                      'ENABLED': False,
                      'HEUR': 'HM_AVG',
                      'H_FLIP': False,
                      'MAX_SIZE': 4000,
                      'SCALES': (),
                      'SCALE_H_FLIP': False,
                      'SCALE_SIZE_DEP': False},
          'MASK_AUG': {'AREA_TH': 32400,
                       'ASPECT_RATIOS': (),
                       'ASPECT_RATIO_H_FLIP': False,
                       'ENABLED': False,
                       'HEUR': 'SOFT_AVG',
                       'H_FLIP': False,
                       'MAX_SIZE': 4000,
                       'SCALES': (),
                       'SCALE_H_FLIP': False,
                       'SCALE_SIZE_DEP': False},
          'MAX_SIZE': 1000,
          'NMS': 0.5,
          'PRECOMPUTED_PROPOSALS': False,
          'PROPOSAL_FILES': (),
          'PROPOSAL_LIMIT': 2000,
          'RPN_MIN_SIZE': 0,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 1000,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALE': 600,
          'SCORE_THRESH': 0.05,
          'SOFT_NMS': {'ENABLED': False, 'METHOD': 'linear', 'SIGMA': 0.5}},
 'TRAIN': {'ASPECT_CROPPING': False,
           'ASPECT_GROUPING': True,
           'ASPECT_HI': 2,
           'ASPECT_LO': 0.5,
           'BATCH_SIZE_PER_IM': 512,
           'BBOX_INSIDE_WEIGHTS': (1.0, 1.0, 1.0, 1.0),
           'BBOX_NORMALIZE_MEANS': (0.0, 0.0, 0.0, 0.0),
           'BBOX_NORMALIZE_STDS': (0.1, 0.1, 0.2, 0.2),
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'COPY_CLS_TO_DET': False,
           'CROWD_FILTER_THRESH': 0.7,
           'DATASETS': (),
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'FREEZE_CONV_BODY': False,
           'GT_MIN_AREA': -1,
           'IMS_PER_BATCH': 4,
           'MAX_SIZE': 1000,
           'PROPOSAL_FILES': (),
           'RPN_BATCH_SIZE_PER_IM': 256,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 0,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 2000,
           'RPN_STRADDLE_THRESH': 0,
           'SCALES': (600,),
           'SNAPSHOT_ITERS': 2000,
           'SPATIAL_REG': True,
           'USE_FLIPPED': True,
           'WEAK_SUPERVISE': True,
           'WEAK_SUPERVISE_WITH_PRETRAIN': True},
 'VIS': False,
 'VIS_TH': 0.9}
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
INFO test_engine.py: 330: loading checkpoint Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/ckpt/model_step5999.pth
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 376/500 0.488s + 0.002s (eta: 0:01:00)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 386/500 0.366s + 0.004s (eta: 0:00:42)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 396/500 0.371s + 0.004s (eta: 0:00:38)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 406/500 0.383s + 0.004s (eta: 0:00:36)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 416/500 0.377s + 0.004s (eta: 0:00:31)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 426/500 0.379s + 0.004s (eta: 0:00:28)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 436/500 0.384s + 0.004s (eta: 0:00:24)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 446/500 0.383s + 0.004s (eta: 0:00:20)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 456/500 0.382s + 0.004s (eta: 0:00:16)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 466/500 0.383s + 0.004s (eta: 0:00:13)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 476/500 0.383s + 0.004s (eta: 0:00:09)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 486/500 0.382s + 0.004s (eta: 0:00:05)
INFO test_engine.py: 281: im_detect: range [376, 500] of 500: 496/500 0.384s + 0.004s (eta: 0:00:01)
INFO test_engine.py: 314: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detection_range_375_500.pkl
/home/ubuntu/anaconda3/envs/pytorch41/lib/python3.7/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")

INFO test_engine.py: 211: Wrote detections to: /home/ubuntu/Detectron.pytorch/Outputs/e2e_faster_rcnn_R-101-FPN_clipart_weakly_with-spatial-reg_random-init-det/Oct27-18-34-49_ip-172-31-8-158_step/test/detections.pkl
INFO test_engine.py: 161: Total inference time: 58.338s
INFO task_evaluation.py:  77: Evaluating detections
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: aeroplane
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bicycle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bird
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: boat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bottle
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: bus
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: car
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cat
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: chair
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: cow
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: diningtable
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: dog
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: horse
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: motorbike
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: person
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: pottedplant
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sheep
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: sofa
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: train
INFO cross_voc_dataset_evaluator.py:  74: Writing VOC results for: tvmonitor
INFO cross_voc_dataset_evaluator.py: 115: VOC07 metric? Yes
INFO voc_eval.py: 171: [125  25  23 107  48  46  45  27  31  42  81  16  41  21  26  40  98 123
  18  13  11  10   9 106  32  12  38   7  22  84 104  15  79  30  35  85
  87  47  44  99  28 119  24 102  92 113 101  69  53  37  95 120 115  96
 110  14  52  68  29  34  57  19  75 103 121 114 118  36 112  66  50  80
  64 109  20  97  91  62  43  54  72 116 122  73  56  33 100  77  90   2
  17  89  71 105  60 124  58  88 117  65  70  49  82  55  78  76  59   6
  83  74   4   1   3  86   8  51  67   0 108 111  61  39  94  63  93   5]
INFO cross_voc_dataset_evaluator.py: 127: AP for aeroplane = 0.0965
INFO voc_eval.py: 171: [162 149  24  55 141  56  20 145 163  25  57 148 156 153 137  71  98 103
 132 133  70  66  62  84  60  97 140 139  53  43  69  26  21  19  22  58
  59  18 159  72   5  64  83  86  99 116 157  68 138  76 142 119   3 124
  65  63  87 105  78 144 120  94 115 146 123 147  51 135 134 101 158  33
 106 152  95 151 114 160 155  23  34 136 161 150  48  75 154  47  73  12
 104   6 143  36 102 127   1  54 110 113 128  89  67  49  31   9  79  93
  50  82 109  52  74 131  17  42 100  38   4 122  41 125 117  32 126  77
 130  35  29  85  39 121 107  88  44  13  37   2  80   0   8  45  27 129
 111  96 112  91  61 118 108  90  10  16  28  40  81  46  14  15  92  30
   7  11]
INFO cross_voc_dataset_evaluator.py: 127: AP for bicycle = 0.0516
INFO voc_eval.py: 171: [180 842 150 ... 594 723 722]
INFO cross_voc_dataset_evaluator.py: 127: AP for bird = 0.0153
INFO voc_eval.py: 171: [456  83 383 558 403 402  82 556  81  88 634 515 123 464 544 545 125 455
 457 458 200 301 306 430 415 355 460 618 354 637 522 530 524 476 421 429
 121 638 433 549 130 310 328 273 234 340 263 259  10 250 248  18 307  25
 364 513 387 374 206 373  11 262 350 463 493 132 553 396 308 422 233 106
 376 565 567 335 225 311 317 152 518 338 346 343 473 339 507 384 347 348
 351 412 419 361 232 288 150 366 370 201 191 136  31 523 585  14 619 603
 568 423 416 408 192 391 397 331  64 334  30 244 628 266 197 126 305 470
 528 527 110 107 409 399 278 377 526 223 362  26 324 299  23 559 352 624
   9 564 353 271  48   4  44   6 202 555 312 494 239 237 438 226  69  94
 546 599 315 478 393 367 229 360 349 632 617  12 390 519 465  86 425 322
 550 321 170 137 570 576  52 157 381 379 300 466 580  28 629 385 231 138
 245 154   5 309 424 267 587 268 400  76 592 435 571  39 236 401 337 358
 459 127 316  38  56 284 166 203  37  35 479 597  85 595 207 290 552 196
 221 104 344 265 189  53 584 158 417 286 257  29   7 260 128  16  41 496
 357 242 443  24  58  32  87 320 193 413  51 591 514  42 214 162 113 581
 461 442 469 224 639 220  17 588 586 112 133 431 432 365 612 594 505 447
 151 161 420 395 382 240 436 426 499 557 418 411 593 375 440   0  89  77
  49 134 115 119  15 622 325 414 578 498 297  73 525 280 261 173 490 274
  36 147 486 283 285  57 608 116 249 176 635  99  46  22   2 572 596 120
 452 341 598 149 625 611 122 386 467  45  13 590 392 105  33 174 186 380
 575 389 563 304 164 313 169 332 485 620 342 293 333 633 327 118 246 282
 631  54  80 195 394 139 163 503 520 188 336 600 453 117 194 222  67 582
 177 291 318 156 368  70  79 142 604 303  60 388 183 468 501 636   1  61
 439 100 255  62 190 180 566 482 135  50 495 298  21 521 329 448  20 472
 446 103 583 607 155 269 292  68 252 512 258 295 241 531 517 502 215 574
 171 462 187 109   8 330 124 404 621  66 182 602  27  43 363 480 405 529
 272 184  96 205 489 275 326 506 185 227 554  75 551 614 601 500  84 146
  97 613 434 548 471 449 372 427 406 179 497 302 623 562 579 532 114  19
 218 175 627 216 616 198 359 589 508 172 560 219 483 213 251   3 160 230
 356 210  93 398 474 277 445 371 217 569 165 492 235 615 145 141 212  71
 437 626 253  91 108 484 504 168 487  55  34 609 264 314 208 111 407  59
 102 477 573 101 228 540  40 410 281 209 516 451 610 533 159 536 561 630
 369 606 428 378 243 319  74 270 511  72 605 153 289 450 129 509  47 144
 323 345 204 444 542 199 238  90  65 541 279 140 247 256 537 488 254  78
 491  98 294 475 441 534 510 454 296 481 547 276 577  63 538 535 539 287
 543 211 167 178  92 143  95 181 131 148]
INFO cross_voc_dataset_evaluator.py: 127: AP for boat = 0.1052
INFO voc_eval.py: 171: [274 166 275 152 264 272 207 208 242 161 160 237 261  59  85 241 163 171
 213 212 249 243  84 107 229 268 252 263 200 259 278 267 228 224 222 240
  90 257  42  40 260 215 258 221 256 255  53 203 269 153  87 109 314 198
  94 247  86 112 196 156 245 211 235 253  67 190  46 266 238 204  54 155
  52 319   9 276 277 219  50 244 172  43 298  47  28 209  44   4 232 230
  82  72 289  73   3  97  10 315  92 231 205 233 214 115 284 114 113  27
 164 325 309 138 201 151  89  58 122 159 301 121  66 223 110 195 197  62
 265  79 118 254  91 191  37 271  83 199 206 316 294 210 251 116 283  65
  81  41  48 285 133  78 217 227 186 216 287 202  55  56 100 246 128 131
 236 147 154 239 324 248 262 270 103   1 173  45  17 273 183  64 290   2
 281  96  15  34 180 220 218 234 320 194 225 326 117 188 106 124 226 279
 144  18 168 250 134 193 111 176 280 282 132 141 120 105   8 129 310  60
 162 150  29  63  30  35  26  70 126 182 137  98  88  49  20  95  80  61
   5 104 304 108 125 297 136  68 303 308  93  32 313  74  76 157 167 189
 311  33  12 322  75 323 127  99 130  11  71   0 102 149 142 169 101 299
 165  22 292   6  13 288 286 139 158 295  57 174  51 318   7 146 145  77
  21 321  31 307 181 140 135 148 306  69 179 119  36  25 317 123 170  23
  24 143  16 291 296 293  14 187 185 302  19 312 184 300 192 305  38 177
 175 178  39]
INFO cross_voc_dataset_evaluator.py: 127: AP for bottle = 0.0203
INFO voc_eval.py: 171: [64 80 44 69 72 47 59 58 48 71 27  5 46 39  4 75 22 60 25 45 51 78 38 66
 17 36 76 68 52 26 28 33 74  2  6 23 77 37 31 85 79 50  1 19  9 55 12 24
 53 73 56 83 65 61 13 32 62 67 30 81 70  0 49 42  7 29  8 63 41 40 35  3
 84 18 11 57 14 10 82 16 43 21 20 34 15 54]
INFO cross_voc_dataset_evaluator.py: 127: AP for bus = 0.1703
INFO voc_eval.py: 171: [ 595  510  318 ...   30 1248 1232]
INFO cross_voc_dataset_evaluator.py: 127: AP for car = 0.0208
INFO voc_eval.py: 171: [44 46 36 39 28 25 14 26 15 13 12 30  7 45 31 32  3 17 35 38 40 27 16 33
  2  6  5 21 41  9  1 34 19 37 11  0  4 10 18 23 42 43 29 24  8 22 20]
INFO cross_voc_dataset_evaluator.py: 127: AP for cat = 0.0455
INFO voc_eval.py: 171: [2353 1162 1490 ... 1391 2618 1825]
INFO cross_voc_dataset_evaluator.py: 127: AP for chair = 0.0185
INFO voc_eval.py: 171: [29  8 19 14 38  7 26 17 41 40  3 15 16 28 22  0 53 74 65 52 47 31 35  6
 32 60 59 48 63  4  5 30 10 62  1 71 23 20 79 11 33 73 75 56 68 69 57 50
  9 64 25 18 61  2 21 45 49 12 80 24 13 27 72 66 70 44 78 77 67 36 51 81
 82 55 76 58 34 42 43 54 39 37 46]
INFO cross_voc_dataset_evaluator.py: 127: AP for cow = 0.0391
INFO voc_eval.py: 171: [630 514 296 517 518 519 520 251 521 246 297 298 527 241 454 522 653 680
 591 455 271 270 443 603 441 590 611 262 261 259 594 258 282 264 650 239
 636 641 643 569 566 380 231 645 558 581 184 634 649 576 548 537 539 222
 549 542 541 544 545 546 543 638 637 640 632 281 651 287 644 661 660 317
 295 646 311 642 662 293 272 217 216 214 213 210 209 221 208 206 205 203
 201 200 199 207 278 225 235 269 268 267 265 257 256 666 631 250 249 248
 244 243 236 252 254 392 629 533 532 531 530 526 525 524 534 516 513 511
 510 508 588 592 593 515 595 536 540 584 583 582 580 579 578 574 538 572
 570 568 562 560 557 556 555 571 350 596 598 401 396 395 194 388 622 623
 406 378 367 366 365 364 360 628 357 624 597 619 618 599 600 601 604 606
 464 447 411 445 612 613 614 615 616 416 415 607 193 586 679  87  90 163
 178  75  93  23  72 677 166 185 183  14  19 673 192 674  31 102 104  62
 605 456  74 602 463  57  60 291 284 283 368 587 507 509 664  69 654 647
 448 100 356  97 354 621  96 703  94 399 112 404 699 452 408 332 617  85
 709  81 325 324 432 434 440 442 309 413  36 626 164 211 212 179 553 167
 240 177 670 233 218 232 220 175 565 563 564 227 180 186 226 189 196  25
 155 575  24 577 188 547 253 373 363 101 610  15 609 174  83 706 625 573
  89 384 707 176 187 198 410 215 407 431 708 181 182 567 529 191 289 504
 498 671 727 589  53 550 488 433  59 551 585 552  30 728 512 669 725 648
  79 237 559 310 449 718 675 730  52  99 719 753 528  45 716 430 492 620
 405 554 414 450  76  17 561 435 505 608 377 197 652 219 168 152 204 639
 202 161 635 238 633 672 663 230 260 627 195 146 713  86 717  13 385 162
 190  73 107 160 465 487 135 376 355 118 462 402 499 665 683 348 397 349
 143 394 732 451  54 535 523 421  47 460 724  37 263 461  21  63 247  67
  66 105 150 486  29 681 280 142 506  55 148 245 302 726 120 438  44 715
 242  51 266  40 678 353 151 374 459 694 457 308 668 746 667 312 502 501
  64 229 741 403 387 389 736 659 696 347 393 477 658 458 299 326 423 657
 733 277 382  71 153 490 372 117 714 702 495 742  16  84  58 379  27 359
 704  56 491 371 126 141 122 132  91  88   7 375 466  12  26 149 446 334
 339 343 285 705 439 113 106 692 493 121 322 676 145 723 419 748 478 362
 383 130 341 338 330 390 453 228 294 426 485 500 224  32 688  77 735 331
 292 494 110  28 165 710 695  35 154 276 275  95 335 223 689 134  68 119
 711 171 400 436 690 352  43  42 133 712 747 755   9 234  39 138  82 127
 123 255   5 685   8  49 444 290 412 437 140 427 752  38 744  48  46 273
 323 686 737 327 428 306 381 124 328 128 170 503 286 398 722  20 319 159
 111 489  98  22  18 103 315 734 429 745 340 358 116 307 129 125 497 370
 391 684 279 386 751 344 418 139  50 682 300 329 303 740  92 424  10  34
 321 743 656 274 691 409  11 739   6 687 729 313 369 114 754 157 496 731
 472 136 288   2 479 172 698  65 720 115 318  33 481 137 108 721  70 738
 109  80 361 425 173  78 305  61   0 420 320  41 316 336 474 351 337 475
 346 749   4 342 304 131   1 333 750 144 169 345 476 469 422 314 467 693
 701 417 147 655 301 697   3 156 483 700 468 158 484 482 470 471 480 473]
INFO cross_voc_dataset_evaluator.py: 127: AP for diningtable = 0.0062
INFO voc_eval.py: 171: [ 69  27  28  50  66  48  47  70  21 114 169 168  43  41 126  19 159 170
 123 117 174 166  38  46  51 121  31 125 161  20 176  22  30  67  60  52
 164 160  36 122  54  42 173  14 165  58 119 101 113 118 147  24  35  34
  57 172  29  64  26 171  16  37  33  98 120  25  56  49 124  15 178  32
 153  61  97  45  17  40 175  59 128  62 131  99  23  63 145 149 100 102
 167  77  44  72  12 163 154 103 162 115 129 177 143 158  71  94  55 152
  53 151  95 136  92 132 148  11  65  18 105 104 133  85  79 139  39 130
 150 134 116 140 144  87  76  68  83  74  91 111  13 109 137 112 138 142
 155 127   5  73  78  88 108  89 135 107 141 146 157 156  10  75  86  80
  84   6 110  81 106  96  82   9  93   1  90   8   2   3   4   0   7]
INFO cross_voc_dataset_evaluator.py: 127: AP for dog = 0.0128
INFO voc_eval.py: 171: [107 106  40 109 108  49  25  98  30  96  88  87  36  80  79  78  50  46
 113 233 101  48 159  19   6 217   4 153  12  13   2  15 215 137 216  81
  52  83 136 169  89  91 154 100 103 150 145  85 157 138  41   5  17  20
  47  84  86 241  97 255  23  22 236  21 115 230  45 133 141 225 220   3
 214 212 206 190 195 199  39 128 202 251 248  53 104 187  28 239 228 139
  31 147 143  38 155   7  35 224 184 222  11 218 204  62   8 156 111  16
 258 247  43  24 203  56 197 185  93  75 182 250 102 122 181  37 117 170
  82 211 135 166  59 192 142 244 242 246  67 116 130 193 198  27 129 234
 253 140  42 219 112 232 124  33  63  90 188 125 240  69 158  54 231  51
 152  94 167  10 221  34 148 131 266 210 164 127 165  55 126 252 226 237
 235  57 227 176   1  77 245 223 194  61 134 173  44 201  71  26 191 144
  58 132  65   9 200 277 162 149 114 229 123  64  68 174 180  66 213  14
 269 208 172 163 205 265 183 260 238 151 276  70 118 189 186 262  95 243
  18  32 171 179 120 254 160 261 196 256   0 264 178 259  92  29 249 207
  72  73 268  76 105 257  99 119 161 209 168 110  60 177 146 263 267 175
  74 121 274 275 270 271 272 273]
INFO cross_voc_dataset_evaluator.py: 127: AP for horse = 0.0231
INFO voc_eval.py: 171: [73 49 82 38 76 47 21 45 56 67 28 40 35 53 75 91 87 42 39 23 93 44 37 88
 72  3 62 43  8 46 29 65 22 94  4 92 16 51 14 63 81 83 85 52 69 54 50 32
 25 70 71 80  1  0 15 61 27 89 20 57 41 86 68 24 31 48 95 18  5 13 64  2
 33 55 66 77 90  9 84 26 19 74 79 78 30 34 59 58 17  6 60 36 12 11  7 10]
INFO cross_voc_dataset_evaluator.py: 127: AP for motorbike = 0.1983
INFO voc_eval.py: 171: [16698 13379 14716 ...  5451  7858  5398]
INFO cross_voc_dataset_evaluator.py: 127: AP for person = 0.1301
INFO voc_eval.py: 171: [292 514 398 516 345 513 396 300 342 546 543 395 299 107 564 401 399 458
 520 450 106  95 343  81 304 391 105  96  87  82 501 169 531 303 180 175
 559 392 521  66 512  76 498 400  22 168 500 347 346 298 272 436 374 426
 505 270 246 151 528  54  53 576  11 567 507 575  19  47  99 536 390 440
 555 276 393  67 268 530 104 293 344 256 173  88 449 423 164 171 454  71
  77 452 100 581  62 572 178 373 430  91 369  83  69 397 524 456 394 172
 574 257 341 438 348 565 446 121  84 535  85 517  48  97 435  75 448 545
 370 491  94 287 135  73 587 129 558 582  80  86  89 101 271  51 495  93
 460  98 322  79 277 478 533  72 174 523 137  70  68 457  78 165 103 249
  90 518 199 186  26  49 290 553  50 493 579 465 529 146 503 102 585 316
  74 442 250 379  17  58  92 488  27 459 571 539 511 118 252 274 467 140
 127 136 382 311 540 502 542 141 122 510 489 189 254 371 291 255 326 481
  40 289 380 319 282 377 427 205 519 221 275 302 425 152  59 258 149 509
 324 125 376 372 590 419 134 422 497 472 563 464  18 176 184 126 333 278
 230 210 211 150 525 496 138 417 350 470 128 566 183 157 383 577 541 320
 207 162 253 409 351  44 349 284  39  41 142 236 471   6 365 364 453 412
 329   1 388 115 119 124 197 170 462 273 281 551 318 508 334 375   0 247
 213 177 166 133 499 336 214  14 223 193 261 156 262 337 259 556 143 243
  57 131 416 335 161 294 589 385 307 578 321 130 537  43 443 154 588 308
 420 584   5 466 463 586 504 237 580 264 267  33 378 238   2 367  38 421
 332 231 148 160 429 145 295 245 196 569 235 339 570 312 480  60  52 139
 120 445 474 538  45 110 201 418 132 158 240 219 381 192 455 526  25 265
  21 428 190 402 260 494 194  42 198  35 167 155 163 195 368 424 116 573
 117 477 185 215 492 522  64 242 340 147 111 323 352 225 328 506  46 547
 557   3 583 408   9  10  55 217 309 317 159 310 305 338 202 222 413 248
 484 153 410 285 208 228 548 251 244 486 182  24 411 534 406 387 359 482
 297 108 447 209 206 562 515 403  29   4 327 226 203 266 113 487 227 384
 568 144 407 234 552 386 283 362  65 325 204 301 288   7 490 405 112  32
 550 212 229 188  28 389 444 404 191 357 561  36 469 109 433  61 114  23
 414 355 123  20 200 315  34 527 263  63 269 187 479 331 356 220 296 432
   8 473 279 468 306 451 286 415 461 179 476 280 330 181  13 241 218  12
 313 216 475  56 224 314 485 439 239 431 437 232 560 366  15  16  31 483
 233  30  37 441 434 354 549 554 358 532 544 363 353 361 360]
INFO cross_voc_dataset_evaluator.py: 127: AP for pottedplant = 0.0913
INFO voc_eval.py: 171: [ 63  56  47  74 115  53  68  57  81  88  64  82  84  11  55  75  54  67
 139  15  59 184  71 122 100 144 125  48 150  93  31  89  50 141 129 192
 147 112 133  32  23 137  78 109 175 103 114  66  79 177  52  30  49 185
  34  26 174 140  17 187  28 179  73  70  61 162  94  51  21 104  37  62
  60  80 189  27  92  25  10   8 149 183 128   1  36  95  35 159  69  58
 108 158  14  65  96  72  76 155 153 142  22  33 135 157 110 105 131 121
 101 182  20 132 145  86 167 148   0  29  38 120 126  43 190 134 173 191
 102 171 165  91 117 176 113 163  18 169  98  46 164 136  87   9 143 118
  44 160  83 116 172 124 170   5 178  97  42 130  41  13  90   2  39 123
 152 138 127 151  77 154  12  99 119 107 146  85  19 106   7 111  16 188
 186  40  24   3  45 181   4 180 168 156 161   6 166]
INFO cross_voc_dataset_evaluator.py: 127: AP for sheep = 0.0410
INFO voc_eval.py: 171: [107  99  96  79  84  95  70  34  33  32 108 104  76  43  41  30   0   1
  16   7  11  91 137 139  92  58 167  72  63  98 102 159  36 158 135 136
  64  12  80  15   8  42  10   6  74   9  83  93  97 120 132  14  53 141
 127  57 148  71  60  66 156  90  89 119 144  17  50 142  39  26  47  40
 164  38  85 153  22  31 133   4  24  27  94  49 143 101 140 122 169  54
 152 116   2  48 163 168  13  20  44 157 100  52 131 129 147 162 151  18
 138   3  35  88  23 134 103  65  69  45  51 160  78  82  46  73  19 145
   5 155 114 166 109  87  37 161  67  21  59 125 154  62 121  68 128 130
 123 112 126 149  56 150  75  81 146  61 124  28  86  55 106 105 118  77
  25 165  29 111 113 115 110 117]
INFO cross_voc_dataset_evaluator.py: 127: AP for sofa = 0.0217
INFO voc_eval.py: 171: [124  25 331  97 170 329 196  34 157 323 159 197 273 178  76 336 335 172
 338 198 169 200 203 330 167 204 128 306 305 248 160 161 112 168 110 106
 328 102 101 100 165 166 326 207 337 256  35  27 349 259  30 230 252 241
 354 262 245 247 173 333 289 316 312 258 348 254 304  19 129  96 164 104
 327  10 362  31 263 318 271  57 181 199 339 345  50  15  39 120 261 231
 260  95 103  49 268 270 211  26 116 347 251 221  81 334  12 238 119 250
 202 163  18 195  93 194 131 219 284 133  32 123 341 205  58 340 111 109
 227 352 137  74 243 295 162 209 185 174 249  43 136  72  73  38  98  48
 212  82  86 294 229 344  42 324  99  56 356 357 138  21 208  94  63 369
 187 278 220 239 325 253 177 127 236 322  24  53 355  60 308   9  64 242
 279  54 257  66  44 193  71 309 132  91 332 232 300 228 297  65 191 363
 366 281  13  14 224  68  79 307 280 189  89 158  37  69  36  51  47  62
 269 192 125 291 234 290 218 118 235  92  46  75 121 292  33 115  22 130
  41  84 117 206   8 140 244 214   1 298 134 346 358  70 288 310 184   5
 126 223 186 107  20 139 285 222 365 156 275 105 213 233   4   3  59 293
 274 176 302  55 217 246  67 264 342 113 154  16  17 175 190  11 143 216
 201 182 360 147 313 142 353 351 283  80 320 319 148 180 144 114  29 255
  52 171 272 215 361  61 301 303 367 314  23 108 277 188  45  28 287 145
 282 359 240 155 286 315 122 146 183 226 152 265 368 311 343 179 153 266
 267 237 150 225   2 321 135 317  77  78   0  40 350 210   6 364   7 149
  85 296 276 151 141 299  87  83  90  88]
INFO cross_voc_dataset_evaluator.py: 127: AP for train = 0.0510
INFO voc_eval.py: 171: [236 275 267 253 249 244  93 235 203 197 191 277 177 137 134 132 129 122
 120 119 118 114 113 173 283  94 300 297   9  20   5 383 365  26 131  79
 206 199 139 187  25 185 141 146 147 138 126 239 162  23   1 386 296 116
 302  87 252 247  84 245 366 256 152 354 384 360 112 149 186 158 287 294
 273 266 264 260 257 255 246 240 307 230 228 208 319 202 320 192 325 338
 352 171 164 153 196 392  74  67   6  82  75   7  19  18  55  80 167 391
 198 301 295 151  69  86 278 373 207 375 324 323 259 100 389  14 219 231
   3  33 157 220  91 272 316 190 322 340  68  17 142 315   0 371 290 355
 133  98 232 125 385  83 382 227  37 328 172 109 225  73 111 286 127 363
 195 356 304  53 288  51 169 241  65 163 115 117 160 136  10 250 261 108
 281 123 362  22 344 293 168 159 317 154  15 209  12 233 311 175 263 144
 336 223 189 270 308 204  62 390 102 234  99  35  32  30 381 342 368 348
  78  58 372  21 200  11  56 237 350 194 337 262 380 376 140 224 351  92
  45 276 188 212 289 353 305 318  96 106 176 214 321 213 183  38  24  16
 103 378   2  89 343 121  13 238 377 349 143  50 359 327 309  59 312 258
  54 274 329 280 184 364 345  47   4 254 341 361  95 205 216  52 285 242
  43 333 330 248  34 388 335  81 326 211 292 291 193 110 379 370 331 313
 357  66 222 135 107   8  48 346 347  77  31 161 128  97 218  88 201 334
 165  57  60 101 303 387  41 310  27 124 170  46  36 367 358 210 306  64
 156 104  28 332 105 181  61 229 369 282  85 299  90  29  40 279 174 374
  76 150 268  72 269 217 339 130 226 145  39 251 155 298  42 148  44 314
 271 166  71 179 243  63 284 265  49 215 180  70 221 178 182]
INFO cross_voc_dataset_evaluator.py: 127: AP for tvmonitor = 0.0276
INFO cross_voc_dataset_evaluator.py: 130: Mean AP = 0.0593
INFO cross_voc_dataset_evaluator.py: 131: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 132: Results:
INFO cross_voc_dataset_evaluator.py: 134: 0.096
INFO cross_voc_dataset_evaluator.py: 134: 0.052
INFO cross_voc_dataset_evaluator.py: 134: 0.015
INFO cross_voc_dataset_evaluator.py: 134: 0.105
INFO cross_voc_dataset_evaluator.py: 134: 0.020
INFO cross_voc_dataset_evaluator.py: 134: 0.170
INFO cross_voc_dataset_evaluator.py: 134: 0.021
INFO cross_voc_dataset_evaluator.py: 134: 0.045
INFO cross_voc_dataset_evaluator.py: 134: 0.019
INFO cross_voc_dataset_evaluator.py: 134: 0.039
INFO cross_voc_dataset_evaluator.py: 134: 0.006
INFO cross_voc_dataset_evaluator.py: 134: 0.013
INFO cross_voc_dataset_evaluator.py: 134: 0.023
INFO cross_voc_dataset_evaluator.py: 134: 0.198
INFO cross_voc_dataset_evaluator.py: 134: 0.130
INFO cross_voc_dataset_evaluator.py: 134: 0.091
INFO cross_voc_dataset_evaluator.py: 134: 0.041
INFO cross_voc_dataset_evaluator.py: 134: 0.022
INFO cross_voc_dataset_evaluator.py: 134: 0.051
INFO cross_voc_dataset_evaluator.py: 134: 0.028
INFO cross_voc_dataset_evaluator.py: 135: 0.059
INFO cross_voc_dataset_evaluator.py: 136: ~~~~~~~~
INFO cross_voc_dataset_evaluator.py: 137: 
INFO cross_voc_dataset_evaluator.py: 138: ----------------------------------------------------------
INFO cross_voc_dataset_evaluator.py: 139: Results computed with the **unofficial** Python eval code.
INFO cross_voc_dataset_evaluator.py: 140: Results should be very close to the official MATLAB code.
INFO cross_voc_dataset_evaluator.py: 141: Use `./tools/reval.py --matlab ...` for your paper.
INFO cross_voc_dataset_evaluator.py: 142: -- Thanks, The Management
INFO cross_voc_dataset_evaluator.py: 143: ----------------------------------------------------------
INFO task_evaluation.py:  63: Evaluating bounding boxes is done!
INFO task_evaluation.py: 189: copypaste: Dataset: cross_clipart_test
INFO task_evaluation.py: 191: copypaste: Task: box
INFO task_evaluation.py: 194: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 195: copypaste: -1.0000,-1.0000,-1.0000,-1.0000,-1.0000,-1.0000
